{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    },
    "cell_id": "5b5f00a7-a133-44aa-9bfa-4ebf923ac3b1",
    "deepnote_cell_type": "code"
   },
   "source": "import sys\nsys.path.append('..')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "cell_id": "00001-74c37d14-d584-4e03-a578-0f8a6f44d59d",
    "deepnote_cell_type": "code"
   },
   "source": "import numpy as np\nimport pandas as pd\nimport pickle\nimport random\n\nimport neptune.new as neptune\nfrom neptune.new.integrations.tensorflow_keras import NeptuneCallback\n\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nimport tensorflow\nfrom tensorflow import keras\nfrom keras.models import Sequential\nfrom keras.layers import LSTM, Dense, Dropout, TimeDistributed, Conv1D, MaxPooling1D, Flatten, Bidirectional, Input, Flatten, Activation, Reshape, RepeatVector, Concatenate\nfrom keras.optimizers import RMSprop, Adam, Adamax, Adagrad\n\nimport keras_tuner\n\nfrom lib.read_data import read_and_join_output_file\n#from lib.create_pipeline import create_transformation_pipeline\nfrom lib.deeplearning import create_transformation_pipelines\nfrom lib.transform_impute import convert_back_df\nfrom lib.split_data import train_test_group_time_split",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "cell_id": "00002-9f567753-19b8-4da2-84ad-e5bbcfb96462",
    "deepnote_cell_type": "code"
   },
   "source": "RANDOM_SEED = 31\nrandom.seed(RANDOM_SEED)\nnp.random.seed(RANDOM_SEED)\ntensorflow.random.set_seed(RANDOM_SEED)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "cell_id": "00003-b66afc3d-92a0-45e7-8703-0c83c6cb390e",
    "deepnote_cell_type": "code"
   },
   "source": "print(\"Num GPUs Available: \", len(tensorflow.config.list_physical_devices('GPU')))",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Num GPUs Available:  0\n"
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "cell_id": "00004-6cc38d12-9fc5-4541-b460-f93b0d75b7f6",
    "deepnote_cell_type": "code"
   },
   "source": "# During experiment we can try to use neptune.ai to log all the Tensorflow experiments results\nneptune_key = pickle.load(open(\"./neptune.pkl\", \"rb\"))\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Preparing the Dataset\nThe train and test sets are split by Township-Ranges, i.e. some Township-Ranges data are either fully in the train or test set.\nThe target value is the value of that variable for 2021\nThus train/test sets are of shape (number of Township-Ranges, 7 years (2014-2020), the number of features).\nThe input of 1 data point in the model is of shape (7x81\n",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "cell_id": "00005-09406f11-f47c-43e4-8770-926fa87e9c23",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "cell_id": "00006-2a2328e0-01e3-4132-b589-dcd548553014",
    "deepnote_cell_type": "code"
   },
   "source": "test_size=0.15\n# Load the data from the ETL output files\nX = read_and_join_output_file()\n#X[\"WELL_COUNT\"] = X[\"WELL_COUNT_PUBLIC\"] + X[\"WELL_COUNT_AGRICULTURE\"] + X[\"WELL_COUNT_DOMESTIC\"] + X[\"WELL_COUNT_INDUSTRIAL\"]\n#X.drop(columns=[\"WELL_COUNT_PUBLIC\", \"WELL_COUNT_AGRICULTURE\", \"WELL_COUNT_DOMESTIC\", \"WELL_COUNT_INDUSTRIAL\"], inplace=True)\n# Split the data into a training and a test set\nX_train_df, X_test_df, y_train_df, y_test_df = train_test_group_time_split(X, index=[\"TOWNSHIP_RANGE\", \"YEAR\"], group=\"TOWNSHIP_RANGE\", test_size=test_size, random_seed=RANDOM_SEED)\n# Create, fit and apply the data imputation pipeline to the training and test sets\nimpute_pipeline, columns = create_transformation_pipelines(X_train_df)\nX_train_impute = impute_pipeline.fit_transform(X_train_df)\nX_test_impute = impute_pipeline.transform(X_test_df)\n# Convert the X_train and X_test back to dataframes\nX_train_impute_df = pd.DataFrame(X_train_impute, index=X_train_df.index, columns=columns)\nX_test_impute_df = pd.DataFrame(X_test_impute, index=X_test_df.index, columns=columns)\nX_train_impute_df[\"GSE_GWE\"] = np.sqrt(X_train_impute_df[\"GSE_GWE\"])\nX_test_impute_df[\"GSE_GWE\"] = np.sqrt(X_test_impute_df[\"GSE_GWE\"])\n# Keep only the GSE_GWE variable as the outcome variable\nscaler = MinMaxScaler()\ny_train = scaler.fit_transform(y_train_df[[\"GSE_GWE\"]])\ny_train = np.sqrt(y_train)\ny_test = scaler.transform(y_test_df[[\"GSE_GWE\"]])\ny_train_3d = y_train[..., np.newaxis]\nX_train_impute_df",
   "outputs": [
    {
     "data": {
      "text/plain": "                     TOTALDRILLDEPTH_AVG  WELLYIELD_AVG  STATICWATERLEVEL_AVG  \\\nTOWNSHIP_RANGE YEAR                                                             \nT01N R02E      2014             0.000000       0.000489              0.020868   \n               2015             0.000000       0.000000              0.000000   \n               2016             0.000000       0.003259              0.036728   \n               2017             0.066667       0.006410              0.025876   \n               2018             0.053651       0.000652              0.063022   \n...                                  ...            ...                   ...   \nT32S R30E      2016             0.000000       0.000000              0.000000   \n               2017             0.000000       0.000000              0.000000   \n               2018             0.000000       0.000000              0.000000   \n               2019             0.000000       0.000000              0.000000   \n               2020             0.000000       0.000000              0.000000   \n\n                     TOPOFPERFORATEDINTERVAL_AVG  \\\nTOWNSHIP_RANGE YEAR                                \nT01N R02E      2014                     0.052288   \n               2015                     0.000000   \n               2016                     0.084967   \n               2017                     0.082789   \n               2018                     0.077124   \n...                                          ...   \nT32S R30E      2016                     0.000000   \n               2017                     0.000000   \n               2018                     0.000000   \n               2019                     0.000000   \n               2020                     0.000000   \n\n                     BOTTOMOFPERFORATEDINTERVAL_AVG  TOTALCOMPLETEDDEPTH_AVG  \\\nTOWNSHIP_RANGE YEAR                                                            \nT01N R02E      2014                        0.076699                 0.127841   \n               2015                        0.000000                 0.000000   \n               2016                        0.058252                 0.056818   \n               2017                        0.064725                 0.074495   \n               2018                        0.053592                 0.064015   \n...                                             ...                      ...   \nT32S R30E      2016                        0.000000                 0.000000   \n               2017                        0.000000                 0.000000   \n               2018                        0.000000                 0.000000   \n               2019                        0.000000                 0.000000   \n               2020                        0.000000                 0.000000   \n\n                     VEGETATION_BLUE_OAK-GRAY_PINE  \\\nTOWNSHIP_RANGE YEAR                                  \nT01N R02E      2014                       0.010798   \n               2015                       0.010798   \n               2016                       0.010798   \n               2017                       0.010798   \n               2018                       0.010798   \n...                                            ...   \nT32S R30E      2016                       0.033178   \n               2017                       0.033178   \n               2018                       0.033178   \n               2019                       0.033178   \n               2020                       0.033178   \n\n                     VEGETATION_CALIFORNIA_COAST_LIVE_OAK  \\\nTOWNSHIP_RANGE YEAR                                         \nT01N R02E      2014                              0.002749   \n               2015                              0.002749   \n               2016                              0.002749   \n               2017                              0.002749   \n               2018                              0.002749   \n...                                                   ...   \nT32S R30E      2016                              0.000000   \n               2017                              0.000000   \n               2018                              0.000000   \n               2019                              0.000000   \n               2020                              0.000000   \n\n                     VEGETATION_CANYON_LIVE_OAK  VEGETATION_HARD_CHAPARRAL  \\\nTOWNSHIP_RANGE YEAR                                                          \nT01N R02E      2014                    0.000000                   0.000633   \n               2015                    0.000000                   0.000633   \n               2016                    0.000000                   0.000633   \n               2017                    0.000000                   0.000633   \n               2018                    0.000000                   0.000633   \n...                                         ...                        ...   \nT32S R30E      2016                    0.002023                   0.003535   \n               2017                    0.002023                   0.003535   \n               2018                    0.002023                   0.003535   \n               2019                    0.002023                   0.003535   \n               2020                    0.002023                   0.003535   \n\n                     ...  POPULATION_DENSITY  PCT_OF_CAPACITY  \\\nTOWNSHIP_RANGE YEAR  ...                                        \nT01N R02E      2014  ...            0.391791         0.776467   \n               2015  ...            0.394044         0.776467   \n               2016  ...            0.395968         0.776467   \n               2017  ...            0.406050         0.776467   \n               2018  ...            0.405447         0.776467   \n...                  ...                 ...              ...   \nT32S R30E      2016  ...            0.004489         0.496289   \n               2017  ...            0.004477         0.496289   \n               2018  ...            0.004494         0.496289   \n               2019  ...            0.004511         0.580893   \n               2020  ...            0.004533         0.499980   \n\n                     GROUNDSURFACEELEVATION_AVG  AVERAGE_YEARLY_PRECIPITATION  \\\nTOWNSHIP_RANGE YEAR                                                             \nT01N R02E      2014                    0.043092                      0.286941   \n               2015                    0.037376                      0.301232   \n               2016                    0.016622                      0.357881   \n               2017                    0.031660                      0.689154   \n               2018                    0.051869                      0.252603   \n...                                         ...                           ...   \nT32S R30E      2016                    0.058099                      0.118655   \n               2017                    0.058099                      0.180043   \n               2018                    0.058099                      0.084816   \n               2019                    0.058099                      0.168764   \n               2020                    0.058099                      0.166161   \n\n                     SHORTAGE_COUNT   GSE_GWE  WELL_COUNT_AGRICULTURE  \\\nTOWNSHIP_RANGE YEAR                                                     \nT01N R02E      2014             0.0  0.288740                0.021277   \n               2015             0.0  0.286127                0.000000   \n               2016             0.0  0.266940                0.000000   \n               2017             0.0  0.264658                0.000000   \n               2018             0.0  0.258964                0.021277   \n...                             ...       ...                     ...   \nT32S R30E      2016             0.0  0.816752                0.000000   \n               2017             0.0  0.752786                0.000000   \n               2018             0.0  0.772691                0.000000   \n               2019             0.0  0.780003                0.000000   \n               2020             0.0  0.771880                0.000000   \n\n                     WELL_COUNT_DOMESTIC  WELL_COUNT_INDUSTRIAL  \\\nTOWNSHIP_RANGE YEAR                                               \nT01N R02E      2014             0.013889                    0.0   \n               2015             0.000000                    0.0   \n               2016             0.013889                    0.0   \n               2017             0.041667                    0.0   \n               2018             0.013889                    0.0   \n...                                  ...                    ...   \nT32S R30E      2016             0.000000                    0.0   \n               2017             0.000000                    0.0   \n               2018             0.000000                    0.0   \n               2019             0.000000                    0.0   \n               2020             0.000000                    0.0   \n\n                     WELL_COUNT_PUBLIC  \nTOWNSHIP_RANGE YEAR                     \nT01N R02E      2014                0.0  \n               2015                0.0  \n               2016                0.0  \n               2017                0.0  \n               2018                0.0  \n...                                ...  \nT32S R30E      2016                0.0  \n               2017                0.0  \n               2018                0.0  \n               2019                0.0  \n               2020                0.0  \n\n[2842 rows x 81 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th>TOTALDRILLDEPTH_AVG</th>\n      <th>WELLYIELD_AVG</th>\n      <th>STATICWATERLEVEL_AVG</th>\n      <th>TOPOFPERFORATEDINTERVAL_AVG</th>\n      <th>BOTTOMOFPERFORATEDINTERVAL_AVG</th>\n      <th>TOTALCOMPLETEDDEPTH_AVG</th>\n      <th>VEGETATION_BLUE_OAK-GRAY_PINE</th>\n      <th>VEGETATION_CALIFORNIA_COAST_LIVE_OAK</th>\n      <th>VEGETATION_CANYON_LIVE_OAK</th>\n      <th>VEGETATION_HARD_CHAPARRAL</th>\n      <th>...</th>\n      <th>POPULATION_DENSITY</th>\n      <th>PCT_OF_CAPACITY</th>\n      <th>GROUNDSURFACEELEVATION_AVG</th>\n      <th>AVERAGE_YEARLY_PRECIPITATION</th>\n      <th>SHORTAGE_COUNT</th>\n      <th>GSE_GWE</th>\n      <th>WELL_COUNT_AGRICULTURE</th>\n      <th>WELL_COUNT_DOMESTIC</th>\n      <th>WELL_COUNT_INDUSTRIAL</th>\n      <th>WELL_COUNT_PUBLIC</th>\n    </tr>\n    <tr>\n      <th>TOWNSHIP_RANGE</th>\n      <th>YEAR</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">T01N R02E</th>\n      <th>2014</th>\n      <td>0.000000</td>\n      <td>0.000489</td>\n      <td>0.020868</td>\n      <td>0.052288</td>\n      <td>0.076699</td>\n      <td>0.127841</td>\n      <td>0.010798</td>\n      <td>0.002749</td>\n      <td>0.000000</td>\n      <td>0.000633</td>\n      <td>...</td>\n      <td>0.391791</td>\n      <td>0.776467</td>\n      <td>0.043092</td>\n      <td>0.286941</td>\n      <td>0.0</td>\n      <td>0.288740</td>\n      <td>0.021277</td>\n      <td>0.013889</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2015</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.010798</td>\n      <td>0.002749</td>\n      <td>0.000000</td>\n      <td>0.000633</td>\n      <td>...</td>\n      <td>0.394044</td>\n      <td>0.776467</td>\n      <td>0.037376</td>\n      <td>0.301232</td>\n      <td>0.0</td>\n      <td>0.286127</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2016</th>\n      <td>0.000000</td>\n      <td>0.003259</td>\n      <td>0.036728</td>\n      <td>0.084967</td>\n      <td>0.058252</td>\n      <td>0.056818</td>\n      <td>0.010798</td>\n      <td>0.002749</td>\n      <td>0.000000</td>\n      <td>0.000633</td>\n      <td>...</td>\n      <td>0.395968</td>\n      <td>0.776467</td>\n      <td>0.016622</td>\n      <td>0.357881</td>\n      <td>0.0</td>\n      <td>0.266940</td>\n      <td>0.000000</td>\n      <td>0.013889</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2017</th>\n      <td>0.066667</td>\n      <td>0.006410</td>\n      <td>0.025876</td>\n      <td>0.082789</td>\n      <td>0.064725</td>\n      <td>0.074495</td>\n      <td>0.010798</td>\n      <td>0.002749</td>\n      <td>0.000000</td>\n      <td>0.000633</td>\n      <td>...</td>\n      <td>0.406050</td>\n      <td>0.776467</td>\n      <td>0.031660</td>\n      <td>0.689154</td>\n      <td>0.0</td>\n      <td>0.264658</td>\n      <td>0.000000</td>\n      <td>0.041667</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2018</th>\n      <td>0.053651</td>\n      <td>0.000652</td>\n      <td>0.063022</td>\n      <td>0.077124</td>\n      <td>0.053592</td>\n      <td>0.064015</td>\n      <td>0.010798</td>\n      <td>0.002749</td>\n      <td>0.000000</td>\n      <td>0.000633</td>\n      <td>...</td>\n      <td>0.405447</td>\n      <td>0.776467</td>\n      <td>0.051869</td>\n      <td>0.252603</td>\n      <td>0.0</td>\n      <td>0.258964</td>\n      <td>0.021277</td>\n      <td>0.013889</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">T32S R30E</th>\n      <th>2016</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.033178</td>\n      <td>0.000000</td>\n      <td>0.002023</td>\n      <td>0.003535</td>\n      <td>...</td>\n      <td>0.004489</td>\n      <td>0.496289</td>\n      <td>0.058099</td>\n      <td>0.118655</td>\n      <td>0.0</td>\n      <td>0.816752</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2017</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.033178</td>\n      <td>0.000000</td>\n      <td>0.002023</td>\n      <td>0.003535</td>\n      <td>...</td>\n      <td>0.004477</td>\n      <td>0.496289</td>\n      <td>0.058099</td>\n      <td>0.180043</td>\n      <td>0.0</td>\n      <td>0.752786</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2018</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.033178</td>\n      <td>0.000000</td>\n      <td>0.002023</td>\n      <td>0.003535</td>\n      <td>...</td>\n      <td>0.004494</td>\n      <td>0.496289</td>\n      <td>0.058099</td>\n      <td>0.084816</td>\n      <td>0.0</td>\n      <td>0.772691</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2019</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.033178</td>\n      <td>0.000000</td>\n      <td>0.002023</td>\n      <td>0.003535</td>\n      <td>...</td>\n      <td>0.004511</td>\n      <td>0.580893</td>\n      <td>0.058099</td>\n      <td>0.168764</td>\n      <td>0.0</td>\n      <td>0.780003</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2020</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.033178</td>\n      <td>0.000000</td>\n      <td>0.002023</td>\n      <td>0.003535</td>\n      <td>...</td>\n      <td>0.004533</td>\n      <td>0.499980</td>\n      <td>0.058099</td>\n      <td>0.166161</td>\n      <td>0.0</td>\n      <td>0.771880</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>2842 rows × 81 columns</p>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "cell_id": "00007-fdce7bf4-bd1c-49b1-9de2-bd6f42bf31be",
    "deepnote_cell_type": "code"
   },
   "source": "# Change the shape of the input array to (number of Township-Ranges, 7 years (2014-2020), the number of features)\nX_train = X_train_impute_df.values.reshape(len(X_train_impute_df.index.get_level_values(0).unique()), len(X_train_impute_df.index.get_level_values(1).unique()), X_train_impute_df.shape[1])\nX_test = X_test_impute_df.values.reshape(len(X_test_impute_df.index.get_level_values(0).unique()), len(X_test_impute_df.index.get_level_values(1).unique()), X_test_impute_df.shape[1])",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "cell_id": "00008-fde2bed5-926e-4134-bc9a-5247d0940076",
    "deepnote_cell_type": "code"
   },
   "source": "print(\"=\"*100)\nprint(\"Checking the train, validation and test input (X) datasets sizes:\")\nprint(f\"Size of the X_train dataset: {X_train.shape}\")\n#print(f\"Size of the X_val dataset: {X_val.shape}\")\nprint(f\"Size of the X_test dataset: {X_test.shape}\")\nprint(\"=\"*100)\nprint(\"Checking the train, validation and test output (y) datasets sizes:\")\nprint(f\"Size of the y_train dataset: {y_train.shape}\")\n#print(f\"Size of the y_val dataset: {y_val_df.shape}\")\nprint(f\"Size of the y_test dataset: {y_test.shape}\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "====================================================================================================\nChecking the train, validation and test input (X) datasets sizes:\nSize of the X_train dataset: (406, 7, 81)\nSize of the X_test dataset: (72, 7, 81)\n====================================================================================================\nChecking the train, validation and test output (y) datasets sizes:\nSize of the y_train dataset: (406, 1)\nSize of the y_test dataset: (72, 1)\n"
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "cell_id": "00009-7827c9ff-561c-40d1-80e1-3b169678a1c1",
    "deepnote_cell_type": "code"
   },
   "source": "def evaluate_forecast(y_test_inverse, yhat_inverse):\n    mse_ = keras.metrics.MeanSquaredError()\n    mae_ = keras.metrics.MeanAbsoluteError()\n    rmse_ = keras.metrics.RootMeanSquaredError()\n    mae = mae_(y_test_inverse,yhat_inverse)\n    print('mae:', mae)\n    mse = mse_(y_test_inverse,yhat_inverse)\n    print('mse:', mse)\n    rmse = rmse_(y_test_inverse,yhat_inverse)\n    print('rmse:', rmse)\n    return mae, mse, rmse",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "cell_id": "00010-83f829b0-a405-49c0-8cd4-f00733f19c20",
    "deepnote_cell_type": "code"
   },
   "source": "nb_features = len(X_train_impute_df.columns)\noutput_activation = \"linear\"",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Simple Model Hyper-parameter Tuning\nThis model is just made of a single LSTM layer",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "cell_id": "00011-2da2fd60-0261-4ffd-9bdd-a800bf563fff",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "cell_id": "00012-e00e6aa5-16ef-41a2-a4eb-3fdb48420882",
    "deepnote_cell_type": "code"
   },
   "source": "class Model1(keras_tuner.HyperModel):\n    def build(self, hp):\n        model = Sequential()\n        hp_units = hp.Int(\"units\", min_value=20, max_value=300, step=20)\n        hp_activ = hp.Choice(\"activation\", values=[\"relu\", \"tanh\", \"sigmoid\"])\n        model.add(LSTM(units=hp_units, activation=hp_activ, input_shape=(7, nb_features)))\n        model.add(Dense(1, activation=output_activation))\n\n        hp_learning_rate = hp.Choice(\"learning_rate\", values=[1e-2, 1e-3, 1e-4])\n        model.compile(loss=\"mse\", optimizer=Adam(learning_rate=hp_learning_rate), metrics=[keras.metrics.RootMeanSquaredError()])\n\n        return model\n\n    def fit(self, hp, model, *args, **kwargs):\n        return model.fit(\n            *args,\n            validation_split=hp.Float(\"validation_split\", min_value=0.05, max_value=0.2, step=0.05),\n            batch_size=hp.Int(\"batch_size\", min_value=32, max_value=192, step=32),\n            #epochs=hp.Int(\"epochs\", min_value=50, max_value=400, step=50),\n            **kwargs,\n        )",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "cell_id": "00013-57666280-5842-4af7-8256-343697748c6a",
    "deepnote_cell_type": "code"
   },
   "source": "stop_early = tensorflow.keras.callbacks.EarlyStopping(monitor='val_root_mean_squared_error', patience=7, verbose=1)\ntuner = keras_tuner.Hyperband(Model1(),\n                              objective=keras_tuner.Objective(\"val_root_mean_squared_error\", direction=\"min\"),\n                              max_epochs=100000,\n                              factor=1000,\n                              overwrite=True,\n                              directory=\"keras_tuner\",\n                              project_name=\"keras_tuner\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    },
    "cell_id": "00014-d9bd72ec-cfee-4049-85cc-2aff46deb9ed",
    "deepnote_cell_type": "code"
   },
   "source": "tuner.search(X_train, y_train, epochs=100, callbacks=[stop_early])\n\n# Get the optimal hyperparameters\nbest_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n\nprint(f\"\"\"\nThe hyperparameter search is complete.\nlstm_units: {best_hps.get('units')}\nlstm_activation: {best_hps.get('activation')}\nlearning_rate: {best_hps.get('learning_rate')}\nbatch_size: {best_hps.get('batch_size')}\nvalidation_split: {best_hps.get('validation_split')}\n\"\"\")",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Trial 902 Complete [00h 00m 04s]\nval_root_mean_squared_error: 0.12750886380672455\n\nBest val_root_mean_squared_error So Far: 0.0701412484049797\nTotal elapsed time: 02h 37m 23s\n\nSearch: Running Trial #903\n\nValue             |Best Value So Far |Hyperparameter\n200               |80                |units\nrelu              |sigmoid           |activation\n0.001             |0.01              |learning_rate\n0.1               |0.05              |validation_split\n128               |32                |batch_size\n100               |100               |tuner/epochs\n0                 |0                 |tuner/initial_epoch\n1                 |1                 |tuner/bracket\n0                 |0                 |tuner/round\n\nEpoch 1/100\n3/3 [==============================] - 2s 281ms/step - loss: 0.1836 - root_mean_squared_error: 0.4285 - val_loss: 0.0876 - val_root_mean_squared_error: 0.2960\nEpoch 2/100\n3/3 [==============================] - 0s 142ms/step - loss: 0.0541 - root_mean_squared_error: 0.2326 - val_loss: 0.0179 - val_root_mean_squared_error: 0.1339\nEpoch 3/100\n3/3 [==============================] - 0s 63ms/step - loss: 0.0498 - root_mean_squared_error: 0.2233 - val_loss: 0.0317 - val_root_mean_squared_error: 0.1779\nEpoch 4/100\n3/3 [==============================] - ETA: 0s - loss: 0.0373 - root_mean_squared_error: 0.1930"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Simple Model Evaluation\nThe results of the evaluation of the tuned model compared to the test set are storred in Neptune",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "cell_id": "00015-59d62173-28d8-40c0-8094-ae4108cd9f6d",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "cell_id": "00016-26747e78-6c51-4fa9-b9bb-3f22a08f347c",
    "deepnote_cell_type": "code"
   },
   "source": "m1_hyper_parameters = {\n    \"random_seed\": RANDOM_SEED,\n    \"test_size\": test_size,\n    \"validation_split\": 0.1,\n    \"learning_rate\": 0.01,\n    \"batch_size\": 32,\n    \"epochs\": 200,\n    \"lstm_units\": 40,\n    \"lstm_activation\": \"sigmoid\",\n    \"output_activation\": output_activation,\n    \"nb_features\": nb_features,\n    \"optimizer\": \"Adam\"\n}\n\nm1_optimizer = {\n    \"RMSprop\": RMSprop(learning_rate=m1_hyper_parameters[\"learning_rate\"]),\n    \"Adam\": Adam(learning_rate=m1_hyper_parameters[\"learning_rate\"]),\n    \"Adamax\": Adamax(learning_rate=m1_hyper_parameters[\"learning_rate\"]),\n    \"Adagrad\": Adagrad(learning_rate=m1_hyper_parameters[\"learning_rate\"])\n}",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "cell_id": "00017-f66e94c2-d2a0-434b-a1a9-9873477f8e7c",
    "deepnote_cell_type": "code"
   },
   "source": "model1 = Sequential()\nmodel1.add(LSTM(m1_hyper_parameters[\"lstm_units\"], activation=m1_hyper_parameters[\"lstm_activation\"], input_shape=(7, nb_features)))\nmodel1.add(Dense(1, activation=m1_hyper_parameters[\"output_activation\"]))\nmodel1.summary()",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Model: \"sequential_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n lstm_1 (LSTM)               (None, 70)                42560     \n                                                                 \n dense_2 (Dense)             (None, 1)                 71        \n                                                                 \n=================================================================\nTotal params: 42,631\nTrainable params: 42,631\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "cell_id": "00018-f1b9f28f-c5b9-48db-8942-250404477f96",
    "deepnote_cell_type": "code"
   },
   "source": "# Start experiment\nrun = neptune.init(\n    project=\"milestone2-california-water-shortage/deeplearning-lstm\",\n    api_token=neptune_key,\n    name=\"Basic Model\",\n    tags=[\"WithDetailedWellCounts\", \"UnidirectionalLSTM\"]\n)\nneptune_cbk = NeptuneCallback(run=run, base_namespace='metrics')\nrun['hyper-parameters'] = m1_hyper_parameters\n\nmodel1.compile(loss=\"mse\", optimizer=m1_optimizer[m1_hyper_parameters[\"optimizer\"]], metrics=[keras.metrics.RootMeanSquaredError()])\nmodel1.fit(X_train, y_train,\n                     validation_split=m1_hyper_parameters[\"validation_split\"],\n                     batch_size=m1_hyper_parameters[\"batch_size\"],\n                     epochs=m1_hyper_parameters[\"epochs\"],\n                     shuffle=True,\n                     callbacks=[neptune_cbk])\nyhat = model1.predict(X_test, verbose=0)\nyhat_inverse = scaler.inverse_transform(yhat)\ny_test_inverse = scaler.inverse_transform(y_test)\nmae, mse, rmse = evaluate_forecast(y_test_inverse, yhat_inverse)\nrun[\"eval/mae\"] = mae\nrun[\"eval/mse\"] = mse\nrun[\"eval/rmse\"] = rmse\nrun.stop()",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "https://app.neptune.ai/milestone2-california-water-shortage/deeplearning-lstm/e/DEEPLSTM-87\nRemember to stop your run once you’ve finished logging your metadata (https://docs.neptune.ai/api-reference/run#.stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.\nEpoch 1/200\n12/12 [==============================] - 1s 27ms/step - loss: 0.2526 - val_loss: 0.0368\nEpoch 2/200\n12/12 [==============================] - 0s 9ms/step - loss: 0.0431 - val_loss: 0.0152\nEpoch 3/200\n12/12 [==============================] - 0s 8ms/step - loss: 0.0233 - val_loss: 0.0116\nEpoch 4/200\n12/12 [==============================] - 0s 9ms/step - loss: 0.0175 - val_loss: 0.0121\nEpoch 5/200\n12/12 [==============================] - 0s 8ms/step - loss: 0.0148 - val_loss: 0.0084\nEpoch 6/200\n12/12 [==============================] - 0s 9ms/step - loss: 0.0129 - val_loss: 0.0082\nEpoch 7/200\n12/12 [==============================] - 0s 9ms/step - loss: 0.0112 - val_loss: 0.0062\nEpoch 8/200\n12/12 [==============================] - 0s 9ms/step - loss: 0.0099 - val_loss: 0.0050\nEpoch 9/200\n12/12 [==============================] - 0s 9ms/step - loss: 0.0086 - val_loss: 0.0053\nEpoch 10/200\n12/12 [==============================] - 0s 8ms/step - loss: 0.0076 - val_loss: 0.0041\nEpoch 11/200\n12/12 [==============================] - 0s 8ms/step - loss: 0.0067 - val_loss: 0.0040\nEpoch 12/200\n12/12 [==============================] - 0s 10ms/step - loss: 0.0059 - val_loss: 0.0039\nEpoch 13/200\n12/12 [==============================] - 0s 10ms/step - loss: 0.0053 - val_loss: 0.0084\nEpoch 14/200\n12/12 [==============================] - 0s 8ms/step - loss: 0.0051 - val_loss: 0.0051\nEpoch 15/200\n12/12 [==============================] - 0s 10ms/step - loss: 0.0042 - val_loss: 0.0044\nEpoch 16/200\n12/12 [==============================] - 0s 9ms/step - loss: 0.0037 - val_loss: 0.0041\nEpoch 17/200\n12/12 [==============================] - 0s 11ms/step - loss: 0.0035 - val_loss: 0.0043\nEpoch 18/200\n12/12 [==============================] - 0s 10ms/step - loss: 0.0032 - val_loss: 0.0035\nEpoch 19/200\n12/12 [==============================] - 0s 8ms/step - loss: 0.0030 - val_loss: 0.0044\nEpoch 20/200\n12/12 [==============================] - 0s 9ms/step - loss: 0.0029 - val_loss: 0.0045\nEpoch 21/200\n12/12 [==============================] - 0s 10ms/step - loss: 0.0029 - val_loss: 0.0077\nEpoch 22/200\n12/12 [==============================] - 0s 10ms/step - loss: 0.0028 - val_loss: 0.0059\nEpoch 23/200\n12/12 [==============================] - 0s 8ms/step - loss: 0.0028 - val_loss: 0.0074\nEpoch 24/200\n12/12 [==============================] - 0s 8ms/step - loss: 0.0029 - val_loss: 0.0029\nEpoch 25/200\n12/12 [==============================] - 0s 10ms/step - loss: 0.0024 - val_loss: 0.0048\nEpoch 26/200\n12/12 [==============================] - 0s 15ms/step - loss: 0.0025 - val_loss: 0.0034\nEpoch 27/200\n12/12 [==============================] - 0s 10ms/step - loss: 0.0024 - val_loss: 0.0021\nEpoch 28/200\n12/12 [==============================] - 0s 9ms/step - loss: 0.0022 - val_loss: 0.0028\nEpoch 29/200\n12/12 [==============================] - 0s 9ms/step - loss: 0.0022 - val_loss: 0.0026\nEpoch 30/200\n12/12 [==============================] - 0s 11ms/step - loss: 0.0022 - val_loss: 0.0022\nEpoch 31/200\n12/12 [==============================] - 0s 10ms/step - loss: 0.0021 - val_loss: 0.0015\nEpoch 32/200\n12/12 [==============================] - 0s 10ms/step - loss: 0.0022 - val_loss: 0.0035\nEpoch 33/200\n12/12 [==============================] - 0s 9ms/step - loss: 0.0021 - val_loss: 0.0028\nEpoch 34/200\n12/12 [==============================] - 0s 9ms/step - loss: 0.0021 - val_loss: 0.0014\nEpoch 35/200\n12/12 [==============================] - 0s 11ms/step - loss: 0.0021 - val_loss: 0.0031\nEpoch 36/200\n12/12 [==============================] - 0s 8ms/step - loss: 0.0023 - val_loss: 0.0011\nEpoch 37/200\n12/12 [==============================] - 0s 9ms/step - loss: 0.0023 - val_loss: 0.0035\nEpoch 38/200\n12/12 [==============================] - 0s 8ms/step - loss: 0.0020 - val_loss: 0.0029\nEpoch 39/200\n12/12 [==============================] - 0s 9ms/step - loss: 0.0021 - val_loss: 9.7630e-04\nEpoch 40/200\n12/12 [==============================] - 0s 9ms/step - loss: 0.0021 - val_loss: 0.0023\nEpoch 41/200\n12/12 [==============================] - 0s 11ms/step - loss: 0.0022 - val_loss: 9.6225e-04\nEpoch 42/200\n12/12 [==============================] - 0s 11ms/step - loss: 0.0023 - val_loss: 0.0034\nEpoch 43/200\n12/12 [==============================] - 0s 10ms/step - loss: 0.0020 - val_loss: 0.0015\nEpoch 44/200\n12/12 [==============================] - 0s 9ms/step - loss: 0.0019 - val_loss: 0.0011\nEpoch 45/200\n12/12 [==============================] - 0s 10ms/step - loss: 0.0025 - val_loss: 0.0020\nEpoch 46/200\n12/12 [==============================] - 0s 11ms/step - loss: 0.0020 - val_loss: 0.0010\nEpoch 47/200\n12/12 [==============================] - 0s 10ms/step - loss: 0.0024 - val_loss: 0.0030\nEpoch 48/200\n12/12 [==============================] - 0s 8ms/step - loss: 0.0021 - val_loss: 0.0013\nEpoch 49/200\n12/12 [==============================] - 0s 10ms/step - loss: 0.0021 - val_loss: 0.0028\nEpoch 50/200\n12/12 [==============================] - 0s 9ms/step - loss: 0.0021 - val_loss: 0.0014\nEpoch 51/200\n12/12 [==============================] - 0s 8ms/step - loss: 0.0019 - val_loss: 0.0014\nEpoch 52/200\n12/12 [==============================] - 0s 8ms/step - loss: 0.0019 - val_loss: 0.0011\nEpoch 53/200\n12/12 [==============================] - 0s 8ms/step - loss: 0.0021 - val_loss: 0.0059\nEpoch 54/200\n12/12 [==============================] - 0s 8ms/step - loss: 0.0032 - val_loss: 0.0040\nEpoch 55/200\n12/12 [==============================] - 0s 8ms/step - loss: 0.0034 - val_loss: 0.0023\nEpoch 56/200\n12/12 [==============================] - 0s 8ms/step - loss: 0.0022 - val_loss: 0.0022\nEpoch 57/200\n12/12 [==============================] - 0s 8ms/step - loss: 0.0022 - val_loss: 0.0017\nEpoch 58/200\n12/12 [==============================] - 0s 8ms/step - loss: 0.0021 - val_loss: 0.0012\nEpoch 59/200\n12/12 [==============================] - 0s 8ms/step - loss: 0.0023 - val_loss: 0.0024\nEpoch 60/200\n12/12 [==============================] - 0s 8ms/step - loss: 0.0018 - val_loss: 0.0013\nEpoch 61/200\n12/12 [==============================] - 0s 8ms/step - loss: 0.0018 - val_loss: 0.0028\nEpoch 62/200\n12/12 [==============================] - 0s 8ms/step - loss: 0.0019 - val_loss: 0.0013\nEpoch 63/200\n12/12 [==============================] - 0s 9ms/step - loss: 0.0018 - val_loss: 0.0018\nEpoch 64/200\n12/12 [==============================] - 0s 8ms/step - loss: 0.0018 - val_loss: 0.0035\nEpoch 65/200\n12/12 [==============================] - 0s 8ms/step - loss: 0.0021 - val_loss: 0.0012\nEpoch 66/200\n12/12 [==============================] - 0s 8ms/step - loss: 0.0020 - val_loss: 0.0011\nEpoch 67/200\n12/12 [==============================] - 0s 8ms/step - loss: 0.0018 - val_loss: 0.0031\nEpoch 68/200\n12/12 [==============================] - 0s 8ms/step - loss: 0.0019 - val_loss: 9.9339e-04\nEpoch 69/200\n12/12 [==============================] - 0s 8ms/step - loss: 0.0020 - val_loss: 0.0016\nEpoch 70/200\n12/12 [==============================] - 0s 9ms/step - loss: 0.0018 - val_loss: 0.0017\nEpoch 71/200\n12/12 [==============================] - 0s 9ms/step - loss: 0.0018 - val_loss: 0.0034\nEpoch 72/200\n12/12 [==============================] - 0s 8ms/step - loss: 0.0018 - val_loss: 0.0016\nEpoch 73/200\n12/12 [==============================] - 0s 8ms/step - loss: 0.0019 - val_loss: 0.0039\nEpoch 74/200\n12/12 [==============================] - 0s 8ms/step - loss: 0.0020 - val_loss: 0.0011\nEpoch 75/200\n12/12 [==============================] - 0s 8ms/step - loss: 0.0023 - val_loss: 0.0020\nEpoch 76/200\n12/12 [==============================] - 0s 8ms/step - loss: 0.0018 - val_loss: 0.0012\nEpoch 77/200\n12/12 [==============================] - 0s 8ms/step - loss: 0.0018 - val_loss: 0.0011\nEpoch 78/200\n12/12 [==============================] - 0s 8ms/step - loss: 0.0019 - val_loss: 0.0059\nEpoch 79/200\n12/12 [==============================] - 0s 8ms/step - loss: 0.0027 - val_loss: 0.0017\nEpoch 80/200\n12/12 [==============================] - 0s 8ms/step - loss: 0.0020 - val_loss: 0.0015\nEpoch 81/200\n12/12 [==============================] - 0s 8ms/step - loss: 0.0026 - val_loss: 0.0025\nEpoch 82/200\n12/12 [==============================] - 0s 8ms/step - loss: 0.0019 - val_loss: 0.0027\nEpoch 83/200\n12/12 [==============================] - 0s 9ms/step - loss: 0.0020 - val_loss: 0.0019\nEpoch 84/200\n12/12 [==============================] - 0s 8ms/step - loss: 0.0018 - val_loss: 0.0016\nEpoch 85/200\n12/12 [==============================] - 0s 8ms/step - loss: 0.0018 - val_loss: 0.0041\nEpoch 86/200\n12/12 [==============================] - 0s 8ms/step - loss: 0.0021 - val_loss: 0.0012\nEpoch 87/200\n12/12 [==============================] - 0s 8ms/step - loss: 0.0026 - val_loss: 0.0011\nEpoch 88/200\n12/12 [==============================] - 0s 8ms/step - loss: 0.0018 - val_loss: 0.0023\nEpoch 89/200\n12/12 [==============================] - 0s 8ms/step - loss: 0.0018 - val_loss: 0.0023\nEpoch 90/200\n12/12 [==============================] - 0s 9ms/step - loss: 0.0018 - val_loss: 0.0017\nEpoch 91/200\n12/12 [==============================] - 0s 8ms/step - loss: 0.0017 - val_loss: 0.0013\nEpoch 92/200\n12/12 [==============================] - 0s 8ms/step - loss: 0.0020 - val_loss: 0.0022\nEpoch 93/200\n12/12 [==============================] - 0s 8ms/step - loss: 0.0020 - val_loss: 0.0020\nEpoch 94/200\n12/12 [==============================] - 0s 8ms/step - loss: 0.0019 - val_loss: 0.0014\nEpoch 95/200\n12/12 [==============================] - 0s 8ms/step - loss: 0.0019 - val_loss: 0.0036\nEpoch 96/200\n12/12 [==============================] - 0s 9ms/step - loss: 0.0019 - val_loss: 0.0014\nEpoch 97/200\n12/12 [==============================] - 0s 8ms/step - loss: 0.0019 - val_loss: 0.0022\nEpoch 98/200\n12/12 [==============================] - 0s 10ms/step - loss: 0.0016 - val_loss: 0.0014\nEpoch 99/200\n12/12 [==============================] - 0s 8ms/step - loss: 0.0020 - val_loss: 0.0013\nEpoch 100/200\n12/12 [==============================] - 0s 8ms/step - loss: 0.0019 - val_loss: 0.0044\nEpoch 101/200\n12/12 [==============================] - 0s 8ms/step - loss: 0.0022 - val_loss: 0.0034\nEpoch 102/200\n12/12 [==============================] - 0s 9ms/step - loss: 0.0019 - val_loss: 0.0013\nEpoch 103/200\n12/12 [==============================] - 0s 8ms/step - loss: 0.0019 - val_loss: 0.0016\nEpoch 104/200\n12/12 [==============================] - 0s 8ms/step - loss: 0.0017 - val_loss: 0.0018\nEpoch 105/200\n12/12 [==============================] - 0s 8ms/step - loss: 0.0018 - val_loss: 0.0050\nEpoch 106/200\n12/12 [==============================] - 0s 9ms/step - loss: 0.0018 - val_loss: 0.0022\nEpoch 107/200\n12/12 [==============================] - 0s 8ms/step - loss: 0.0021 - val_loss: 0.0013\nEpoch 108/200\n12/12 [==============================] - 0s 8ms/step - loss: 0.0022 - val_loss: 0.0016\nEpoch 109/200\n12/12 [==============================] - 0s 8ms/step - loss: 0.0019 - val_loss: 0.0018\nEpoch 110/200\n12/12 [==============================] - 0s 10ms/step - loss: 0.0018 - val_loss: 0.0028\nEpoch 111/200\n12/12 [==============================] - 0s 9ms/step - loss: 0.0019 - val_loss: 0.0011\nEpoch 112/200\n12/12 [==============================] - 0s 9ms/step - loss: 0.0021 - val_loss: 0.0015\nEpoch 113/200\n12/12 [==============================] - 0s 9ms/step - loss: 0.0018 - val_loss: 0.0019\nEpoch 114/200\n12/12 [==============================] - 0s 8ms/step - loss: 0.0019 - val_loss: 0.0017\nEpoch 115/200\n12/12 [==============================] - 0s 9ms/step - loss: 0.0017 - val_loss: 0.0031\nEpoch 116/200\n12/12 [==============================] - 0s 10ms/step - loss: 0.0018 - val_loss: 0.0020\nEpoch 117/200\n12/12 [==============================] - 0s 10ms/step - loss: 0.0019 - val_loss: 0.0014\nEpoch 118/200\n12/12 [==============================] - 0s 9ms/step - loss: 0.0018 - val_loss: 0.0014\nEpoch 119/200\n12/12 [==============================] - 0s 10ms/step - loss: 0.0020 - val_loss: 0.0014\nEpoch 120/200\n12/12 [==============================] - 0s 10ms/step - loss: 0.0019 - val_loss: 0.0014\nEpoch 121/200\n12/12 [==============================] - 0s 9ms/step - loss: 0.0020 - val_loss: 0.0024\nEpoch 122/200\n12/12 [==============================] - 0s 9ms/step - loss: 0.0018 - val_loss: 0.0022\nEpoch 123/200\n12/12 [==============================] - 0s 9ms/step - loss: 0.0016 - val_loss: 0.0026\nEpoch 124/200\n12/12 [==============================] - 0s 11ms/step - loss: 0.0016 - val_loss: 0.0017\nEpoch 125/200\n12/12 [==============================] - 0s 9ms/step - loss: 0.0016 - val_loss: 0.0023\nEpoch 126/200\n12/12 [==============================] - 0s 9ms/step - loss: 0.0017 - val_loss: 0.0028\nEpoch 127/200\n12/12 [==============================] - 0s 9ms/step - loss: 0.0021 - val_loss: 0.0013\nEpoch 128/200\n12/12 [==============================] - 0s 9ms/step - loss: 0.0026 - val_loss: 0.0014\nEpoch 129/200\n12/12 [==============================] - 0s 9ms/step - loss: 0.0019 - val_loss: 0.0031\nEpoch 130/200\n12/12 [==============================] - 0s 9ms/step - loss: 0.0017 - val_loss: 0.0023\nEpoch 131/200\n12/12 [==============================] - 0s 9ms/step - loss: 0.0016 - val_loss: 0.0013\nEpoch 132/200\n12/12 [==============================] - 0s 8ms/step - loss: 0.0016 - val_loss: 0.0021\nEpoch 133/200\n12/12 [==============================] - 0s 9ms/step - loss: 0.0017 - val_loss: 0.0037\nEpoch 134/200\n12/12 [==============================] - 0s 9ms/step - loss: 0.0018 - val_loss: 0.0029\nEpoch 135/200\n12/12 [==============================] - 0s 9ms/step - loss: 0.0016 - val_loss: 0.0023\nEpoch 136/200\n12/12 [==============================] - 0s 8ms/step - loss: 0.0015 - val_loss: 0.0019\nEpoch 137/200\n12/12 [==============================] - 0s 10ms/step - loss: 0.0017 - val_loss: 0.0021\nEpoch 138/200\n12/12 [==============================] - 0s 12ms/step - loss: 0.0019 - val_loss: 0.0018\nEpoch 139/200\n12/12 [==============================] - 0s 8ms/step - loss: 0.0015 - val_loss: 0.0015\nEpoch 140/200\n12/12 [==============================] - 0s 8ms/step - loss: 0.0015 - val_loss: 0.0025\nEpoch 141/200\n12/12 [==============================] - 0s 8ms/step - loss: 0.0017 - val_loss: 0.0015\nEpoch 142/200\n12/12 [==============================] - 0s 9ms/step - loss: 0.0017 - val_loss: 0.0016\nEpoch 143/200\n12/12 [==============================] - 0s 9ms/step - loss: 0.0023 - val_loss: 0.0015\nEpoch 144/200\n12/12 [==============================] - 0s 10ms/step - loss: 0.0017 - val_loss: 0.0039\nEpoch 145/200\n12/12 [==============================] - 0s 8ms/step - loss: 0.0017 - val_loss: 0.0019\nEpoch 146/200\n12/12 [==============================] - 0s 10ms/step - loss: 0.0017 - val_loss: 0.0025\nEpoch 147/200\n12/12 [==============================] - 0s 10ms/step - loss: 0.0016 - val_loss: 0.0014\nEpoch 148/200\n12/12 [==============================] - 0s 9ms/step - loss: 0.0017 - val_loss: 0.0015\nEpoch 149/200\n12/12 [==============================] - 0s 9ms/step - loss: 0.0016 - val_loss: 0.0028\nEpoch 150/200\n12/12 [==============================] - 0s 9ms/step - loss: 0.0015 - val_loss: 0.0021\nEpoch 151/200\n12/12 [==============================] - 0s 10ms/step - loss: 0.0016 - val_loss: 0.0022\nEpoch 152/200\n12/12 [==============================] - 0s 10ms/step - loss: 0.0015 - val_loss: 0.0026\nEpoch 153/200\n12/12 [==============================] - 0s 9ms/step - loss: 0.0017 - val_loss: 0.0044\nEpoch 154/200\n12/12 [==============================] - 0s 8ms/step - loss: 0.0018 - val_loss: 0.0020\nEpoch 155/200\n12/12 [==============================] - 0s 8ms/step - loss: 0.0016 - val_loss: 0.0025\nEpoch 156/200\n12/12 [==============================] - 0s 8ms/step - loss: 0.0019 - val_loss: 0.0014\nEpoch 157/200\n12/12 [==============================] - 0s 8ms/step - loss: 0.0016 - val_loss: 0.0012\nEpoch 158/200\n12/12 [==============================] - 0s 8ms/step - loss: 0.0017 - val_loss: 0.0019\nEpoch 159/200\n12/12 [==============================] - 0s 8ms/step - loss: 0.0019 - val_loss: 0.0015\nEpoch 160/200\n12/12 [==============================] - 0s 8ms/step - loss: 0.0015 - val_loss: 0.0014\nEpoch 161/200\n12/12 [==============================] - 0s 8ms/step - loss: 0.0014 - val_loss: 0.0024\nEpoch 162/200\n12/12 [==============================] - 0s 8ms/step - loss: 0.0014 - val_loss: 0.0041\nEpoch 163/200\n12/12 [==============================] - 0s 8ms/step - loss: 0.0017 - val_loss: 0.0020\nEpoch 164/200\n12/12 [==============================] - 0s 8ms/step - loss: 0.0014 - val_loss: 0.0020\nEpoch 165/200\n12/12 [==============================] - 0s 9ms/step - loss: 0.0015 - val_loss: 0.0019\nEpoch 166/200\n12/12 [==============================] - 0s 8ms/step - loss: 0.0014 - val_loss: 0.0015\nEpoch 167/200\n12/12 [==============================] - 0s 8ms/step - loss: 0.0016 - val_loss: 0.0013\nEpoch 168/200\n12/12 [==============================] - 0s 8ms/step - loss: 0.0015 - val_loss: 0.0015\nEpoch 169/200\n12/12 [==============================] - 0s 8ms/step - loss: 0.0015 - val_loss: 0.0020\nEpoch 170/200\n12/12 [==============================] - 0s 8ms/step - loss: 0.0013 - val_loss: 0.0015\nEpoch 171/200\n12/12 [==============================] - 0s 8ms/step - loss: 0.0015 - val_loss: 0.0014\nEpoch 172/200\n12/12 [==============================] - 0s 8ms/step - loss: 0.0014 - val_loss: 0.0018\nEpoch 173/200\n12/12 [==============================] - 0s 8ms/step - loss: 0.0013 - val_loss: 0.0018\nEpoch 174/200\n12/12 [==============================] - 0s 8ms/step - loss: 0.0016 - val_loss: 0.0023\nEpoch 175/200\n12/12 [==============================] - 0s 8ms/step - loss: 0.0016 - val_loss: 0.0016\nEpoch 176/200\n12/12 [==============================] - 0s 8ms/step - loss: 0.0018 - val_loss: 0.0016\nEpoch 177/200\n12/12 [==============================] - 0s 8ms/step - loss: 0.0015 - val_loss: 0.0018\nEpoch 178/200\n12/12 [==============================] - 0s 8ms/step - loss: 0.0014 - val_loss: 0.0014\nEpoch 179/200\n12/12 [==============================] - 0s 9ms/step - loss: 0.0013 - val_loss: 0.0021\nEpoch 180/200\n12/12 [==============================] - 0s 8ms/step - loss: 0.0017 - val_loss: 0.0018\nEpoch 181/200\n12/12 [==============================] - 0s 8ms/step - loss: 0.0016 - val_loss: 0.0019\nEpoch 182/200\n12/12 [==============================] - 0s 8ms/step - loss: 0.0016 - val_loss: 0.0018\nEpoch 183/200\n12/12 [==============================] - 0s 8ms/step - loss: 0.0013 - val_loss: 0.0039\nEpoch 184/200\n12/12 [==============================] - 0s 9ms/step - loss: 0.0015 - val_loss: 0.0038\nEpoch 185/200\n12/12 [==============================] - 0s 9ms/step - loss: 0.0017 - val_loss: 0.0020\nEpoch 186/200\n12/12 [==============================] - 0s 10ms/step - loss: 0.0016 - val_loss: 0.0027\nEpoch 187/200\n12/12 [==============================] - 0s 9ms/step - loss: 0.0015 - val_loss: 0.0023\nEpoch 188/200\n12/12 [==============================] - 0s 8ms/step - loss: 0.0012 - val_loss: 0.0021\nEpoch 189/200\n12/12 [==============================] - 0s 8ms/step - loss: 0.0014 - val_loss: 0.0015\nEpoch 190/200\n12/12 [==============================] - 0s 8ms/step - loss: 0.0015 - val_loss: 0.0015\nEpoch 191/200\n12/12 [==============================] - 0s 8ms/step - loss: 0.0019 - val_loss: 0.0014\nEpoch 192/200\n12/12 [==============================] - 0s 8ms/step - loss: 0.0022 - val_loss: 0.0019\nEpoch 193/200\n12/12 [==============================] - 0s 8ms/step - loss: 0.0017 - val_loss: 0.0016\nEpoch 194/200\n12/12 [==============================] - 0s 9ms/step - loss: 0.0021 - val_loss: 0.0023\nEpoch 195/200\n12/12 [==============================] - 0s 8ms/step - loss: 0.0024 - val_loss: 0.0044\nEpoch 196/200\n12/12 [==============================] - 0s 8ms/step - loss: 0.0020 - val_loss: 0.0046\nEpoch 197/200\n12/12 [==============================] - 0s 8ms/step - loss: 0.0014 - val_loss: 0.0043\nEpoch 198/200\n12/12 [==============================] - 0s 8ms/step - loss: 0.0013 - val_loss: 0.0016\nEpoch 199/200\n12/12 [==============================] - 0s 8ms/step - loss: 0.0012 - val_loss: 0.0026\nEpoch 200/200\n12/12 [==============================] - 0s 8ms/step - loss: 0.0014 - val_loss: 0.0016\nmae: tf.Tensor(31.096588, shape=(), dtype=float32)\nmse: tf.Tensor(2664.7837, shape=(), dtype=float32)\nrmse: tf.Tensor(51.621544, shape=(), dtype=float32)\nShutting down background jobs, please wait a moment...\nDone!\nWaiting for the remaining 193 operations to synchronize with Neptune. Do not kill this process.\nAll 193 operations synced, thanks for waiting!\nExplore the metadata in the Neptune app:\nhttps://app.neptune.ai/milestone2-california-water-shortage/deeplearning-lstm/e/DEEPLSTM-87\n"
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Model2 Hyper-parameter tuning\nThis model is made of\n* a simple or bidirectional LSTM layer\n* a Dense unit\n* a Dropout Unit",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "cell_id": "00019-d3296a05-c66a-4717-96bb-48c28172363c",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "cell_id": "00020-4069c9f1-07e1-4bd8-9493-dcfda776b795",
    "deepnote_cell_type": "code"
   },
   "source": "class Model2(keras_tuner.HyperModel):\n    def build(self, hp):\n        model = Sequential()\n        lstm_units = hp.Int(\"lstm_units\", min_value=20, max_value=300, step=20)\n        lstm_activ = hp.Choice(\"lstm_activation\", values=[\"relu\", \"tanh\", \"sigmoid\"])\n        model.add(LSTM(units=lstm_units, activation=lstm_activ, input_shape=(7, nb_features)))\n        dense_units = hp.Int(\"dense_units\", min_value=11, max_value=101, step=10)\n        dense_activation = hp.Choice(\"dense_activation\", values=[\"relu\", \"tanh\", \"sigmoid\"])\n        model.add(Dense(dense_units, activation=dense_activation))\n        hp_dropout = hp.Float(\"dropout_rate\", min_value=0.05, max_value=0.25, step=0.05)\n        model.add(Dropout(hp_dropout))\n        model.add(Dense(1, activation=output_activation))\n\n        hp_learning_rate = hp.Choice(\"learning_rate\", values=[1e-2, 1e-3, 1e-4])\n        model.compile(loss=\"mse\", optimizer=Adam(learning_rate=hp_learning_rate), metrics=[keras.metrics.RootMeanSquaredError()])\n\n        return model\n\n    def fit(self, hp, model, *args, **kwargs):\n        return model.fit(\n            *args,\n            validation_split=hp.Float(\"validation_split\", min_value=0.05, max_value=0.2, step=0.05),\n            batch_size=hp.Int(\"batch_size\", min_value=32, max_value=192, step=32),\n            #epochs=hp.Int(\"epochs\", min_value=50, max_value=400, step=50),\n            **kwargs,\n        )",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "cell_id": "00021-ecb4e598-b51e-4fb4-9776-6aabe253666e",
    "deepnote_cell_type": "code"
   },
   "source": "stop_early = tensorflow.keras.callbacks.EarlyStopping(monitor=\"val_root_mean_squared_error\", patience=7, verbose=1)\ntuner = keras_tuner.Hyperband(Model2(),\n                              objective=keras_tuner.Objective(\"val_root_mean_squared_error\", direction=\"min\"),\n                              max_epochs=200000,\n                              factor=2000,\n                              overwrite=True,\n                              directory=\"keras_tuner\",\n                              project_name=\"keras_tuner\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    },
    "cell_id": "00022-eb58a883-00f2-4a4f-80ff-bee86b893570",
    "deepnote_cell_type": "code"
   },
   "source": "tuner.search(X_train, y_train, epochs=200, callbacks=[stop_early])\n\n# Get the optimal hyperparameters\nbest_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n\nprint(f\"\"\"\nThe hyperparameter search is complete.\nlstm_units: {best_hps.get('lstm_units')}\nlstm_activation: {best_hps.get('lstm_activation')}\ndense_units: {best_hps.get('dense_units')}\ndense_activation: {best_hps.get('dense_activation')}\ndropout_rate: {best_hps.get('dropout_rate')}\nlearning_rate: {best_hps.get('learning_rate')}\nbatch_size: {best_hps.get('batch_size')}\nvalidation_split: {best_hps.get('validation_split')}\n\"\"\")",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Trial 2220 Complete [00h 00m 14s]\nval_root_mean_squared_error: 0.15608908236026764\n\nBest val_root_mean_squared_error So Far: 0.07610634714365005\nTotal elapsed time: 14h 26m 03s\n\nSearch: Running Trial #2221\n\nValue             |Best Value So Far |Hyperparameter\n240               |200               |lstm_units\nrelu              |relu              |lstm_activation\n11                |31                |dense_units\ntanh              |sigmoid           |dense_activation\n0.2               |0.2               |dropout_rate\n0.001             |0.001             |learning_rate\n0.15              |0.1               |validation_split\n128               |96                |batch_size\n100               |100               |tuner/epochs\n0                 |0                 |tuner/initial_epoch\n1                 |1                 |tuner/bracket\n0                 |0                 |tuner/round\n\nEpoch 1/100\n3/3 [==============================] - 3s 473ms/step - loss: 0.0659 - root_mean_squared_error: 0.2568 - val_loss: 0.0202 - val_root_mean_squared_error: 0.1420\nEpoch 2/100\n3/3 [==============================] - 1s 391ms/step - loss: 0.0440 - root_mean_squared_error: 0.2097 - val_loss: 0.0207 - val_root_mean_squared_error: 0.1439\nEpoch 3/100\n3/3 [==============================] - 1s 436ms/step - loss: 0.0226 - root_mean_squared_error: 0.1504 - val_loss: 0.0283 - val_root_mean_squared_error: 0.1682\nEpoch 4/100\n3/3 [==============================] - 1s 277ms/step - loss: 0.0218 - root_mean_squared_error: 0.1478 - val_loss: 0.0263 - val_root_mean_squared_error: 0.1621\nEpoch 5/100\n3/3 [==============================] - 1s 460ms/step - loss: 0.0183 - root_mean_squared_error: 0.1354 - val_loss: 0.0199 - val_root_mean_squared_error: 0.1410\nEpoch 6/100\n3/3 [==============================] - 1s 200ms/step - loss: 0.0155 - root_mean_squared_error: 0.1245 - val_loss: 0.0203 - val_root_mean_squared_error: 0.1426\nEpoch 7/100\n3/3 [==============================] - 1s 265ms/step - loss: 0.0129 - root_mean_squared_error: 0.1137 - val_loss: 0.0156 - val_root_mean_squared_error: 0.1249\nEpoch 8/100\n3/3 [==============================] - 1s 275ms/step - loss: 0.0093 - root_mean_squared_error: 0.0966 - val_loss: 0.0130 - val_root_mean_squared_error: 0.1142\nEpoch 9/100\n3/3 [==============================] - 1s 293ms/step - loss: 0.0094 - root_mean_squared_error: 0.0969 - val_loss: 0.0119 - val_root_mean_squared_error: 0.1092\nEpoch 10/100\n3/3 [==============================] - 1s 305ms/step - loss: 0.0081 - root_mean_squared_error: 0.0898 - val_loss: 0.0108 - val_root_mean_squared_error: 0.1041\nEpoch 11/100\n3/3 [==============================] - 1s 304ms/step - loss: 0.0090 - root_mean_squared_error: 0.0946 - val_loss: 0.0103 - val_root_mean_squared_error: 0.1013\nEpoch 12/100\n3/3 [==============================] - 1s 300ms/step - loss: 0.0071 - root_mean_squared_error: 0.0841 - val_loss: 0.0099 - val_root_mean_squared_error: 0.0995\nEpoch 13/100\n3/3 [==============================] - 1s 295ms/step - loss: 0.0065 - root_mean_squared_error: 0.0804 - val_loss: 0.0098 - val_root_mean_squared_error: 0.0991\nEpoch 14/100\n3/3 [==============================] - 1s 185ms/step - loss: 0.0069 - root_mean_squared_error: 0.0829 - val_loss: 0.0102 - val_root_mean_squared_error: 0.1010\nEpoch 15/100\n3/3 [==============================] - 1s 180ms/step - loss: 0.0068 - root_mean_squared_error: 0.0823 - val_loss: 0.0100 - val_root_mean_squared_error: 0.1002\nEpoch 16/100\n"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Model 2 Evaluation",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "cell_id": "00023-73e2e37d-4a13-4d3c-8468-faac1cd2493a",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "cell_id": "00024-06ac849b-07d4-456d-9163-9f998594a97b",
    "deepnote_cell_type": "code"
   },
   "source": "m2_hyper_parameters = {\n    \"random_seed\": RANDOM_SEED,\n    \"test_size\": test_size,\n    \"validation_split\": 0.15,\n    \"learning_rate\": 0.001,\n    \"batch_size\": 32,\n    \"epochs\": 200,\n    \"lstm_units\": 120,\n    \"lstm_activation\": \"sigmoid\",\n    \"dense_units\": 11,\n    \"dense_activation\": \"tanh\",\n    \"dropout\": 0.25,\n    \"output_activation\": \"linear\",\n    \"nb_features\": nb_features,\n    \"optimizer\": \"Adam\"\n}\n\nm2_optimizer = {\n    \"RMSprop\": RMSprop(learning_rate=m2_hyper_parameters[\"learning_rate\"]),\n    \"Adam\": Adam(learning_rate=m2_hyper_parameters[\"learning_rate\"]),\n    \"Adamax\": Adamax(learning_rate=m2_hyper_parameters[\"learning_rate\"]),\n    \"Adagrad\": Adagrad(learning_rate=m2_hyper_parameters[\"learning_rate\"])\n}\n\nmodel2 = Sequential()\nmodel2.add(LSTM(m2_hyper_parameters[\"lstm_units\"], activation=m2_hyper_parameters[\"lstm_activation\"], input_shape=(7, nb_features)))\nmodel2.add(Dense(m2_hyper_parameters[\"dense_units\"], activation=m2_hyper_parameters[\"dense_activation\"]))\nmodel2.add(Dropout(m2_hyper_parameters[\"dropout\"]))\nmodel2.add(Dense(1, activation=m2_hyper_parameters[\"output_activation\"]))\nmodel2.summary()",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Model: \"sequential_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n bidirectional (Bidirectiona  (None, 240)              193920    \n l)                                                              \n                                                                 \n dense_2 (Dense)             (None, 11)                2651      \n                                                                 \n dropout_1 (Dropout)         (None, 11)                0         \n                                                                 \n dense_3 (Dense)             (None, 1)                 12        \n                                                                 \n=================================================================\nTotal params: 196,583\nTrainable params: 196,583\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "cell_id": "00025-85f1ff95-62bd-4a25-815c-4a6db9409adf",
    "deepnote_cell_type": "code"
   },
   "source": "run = neptune.init(\n    project=\"milestone2-california-water-shortage/deeplearning-lstm\",\n    api_token=neptune_key,\n    name=\"Advanced Model 1\",\n    tags=[\"WithDetailedWellCounts\", \"BidirectionalLSTM\"]\n)\nneptune_cbk = NeptuneCallback(run=run, base_namespace='metrics')\nrun['hyper-parameters'] = m2_hyper_parameters\n\nmodel2.compile(loss=\"mse\", optimizer=m2_optimizer[m2_hyper_parameters[\"optimizer\"]], metrics=[keras.metrics.RootMeanSquaredError()])\nmodel2.fit(X_train, y_train,\n                     validation_split=m2_hyper_parameters[\"validation_split\"],\n                     batch_size=m2_hyper_parameters[\"batch_size\"],\n                     epochs=m2_hyper_parameters[\"epochs\"],\n                     shuffle=True,\n                     callbacks=[neptune_cbk])\nyhat = model2.predict(X_test, verbose=0)\nyhat_inverse = scaler.inverse_transform(yhat)\ny_test_inverse = scaler.inverse_transform(y_test)\nevaluate_forecast(y_test_inverse, yhat_inverse)\nmae, mse, rmse = evaluate_forecast(y_test_inverse, yhat_inverse)\nrun[\"eval/mae\"] = mae\nrun[\"eval/mse\"] = mse\nrun[\"eval/rmse\"] = rmse\nrun.stop()",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "https://app.neptune.ai/milestone2-california-water-shortage/deeplearning-lstm/e/DEEPLSTM-141\nRemember to stop your run once you’ve finished logging your metadata (https://docs.neptune.ai/api-reference/run#.stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.\nEpoch 1/200\n11/11 [==============================] - 2s 58ms/step - loss: 0.1887 - root_mean_squared_error: 0.4344 - val_loss: 0.0370 - val_root_mean_squared_error: 0.1923\nEpoch 2/200\n11/11 [==============================] - 0s 28ms/step - loss: 0.0777 - root_mean_squared_error: 0.2787 - val_loss: 0.0387 - val_root_mean_squared_error: 0.1968\nEpoch 3/200\n11/11 [==============================] - 0s 28ms/step - loss: 0.0552 - root_mean_squared_error: 0.2349 - val_loss: 0.0387 - val_root_mean_squared_error: 0.1968\nEpoch 4/200\n11/11 [==============================] - 0s 27ms/step - loss: 0.0354 - root_mean_squared_error: 0.1882 - val_loss: 0.0357 - val_root_mean_squared_error: 0.1888\nEpoch 5/200\n11/11 [==============================] - 0s 28ms/step - loss: 0.0290 - root_mean_squared_error: 0.1702 - val_loss: 0.0353 - val_root_mean_squared_error: 0.1879\nEpoch 6/200\n11/11 [==============================] - 0s 28ms/step - loss: 0.0269 - root_mean_squared_error: 0.1640 - val_loss: 0.0341 - val_root_mean_squared_error: 0.1848\nEpoch 7/200\n11/11 [==============================] - 0s 27ms/step - loss: 0.0250 - root_mean_squared_error: 0.1580 - val_loss: 0.0310 - val_root_mean_squared_error: 0.1760\nEpoch 8/200\n11/11 [==============================] - 0s 28ms/step - loss: 0.0215 - root_mean_squared_error: 0.1466 - val_loss: 0.0303 - val_root_mean_squared_error: 0.1741\nEpoch 9/200\n11/11 [==============================] - 0s 30ms/step - loss: 0.0199 - root_mean_squared_error: 0.1409 - val_loss: 0.0284 - val_root_mean_squared_error: 0.1684\nEpoch 10/200\n11/11 [==============================] - 0s 27ms/step - loss: 0.0195 - root_mean_squared_error: 0.1395 - val_loss: 0.0275 - val_root_mean_squared_error: 0.1659\nEpoch 11/200\n11/11 [==============================] - 0s 29ms/step - loss: 0.0205 - root_mean_squared_error: 0.1432 - val_loss: 0.0250 - val_root_mean_squared_error: 0.1581\nEpoch 12/200\n11/11 [==============================] - 0s 27ms/step - loss: 0.0169 - root_mean_squared_error: 0.1300 - val_loss: 0.0245 - val_root_mean_squared_error: 0.1564\nEpoch 13/200\n11/11 [==============================] - 0s 28ms/step - loss: 0.0175 - root_mean_squared_error: 0.1322 - val_loss: 0.0243 - val_root_mean_squared_error: 0.1558\nEpoch 14/200\n11/11 [==============================] - 0s 28ms/step - loss: 0.0185 - root_mean_squared_error: 0.1359 - val_loss: 0.0217 - val_root_mean_squared_error: 0.1474\nEpoch 15/200\n11/11 [==============================] - 0s 28ms/step - loss: 0.0177 - root_mean_squared_error: 0.1329 - val_loss: 0.0221 - val_root_mean_squared_error: 0.1486\nEpoch 16/200\n11/11 [==============================] - 0s 28ms/step - loss: 0.0153 - root_mean_squared_error: 0.1235 - val_loss: 0.0190 - val_root_mean_squared_error: 0.1379\nEpoch 17/200\n11/11 [==============================] - 0s 28ms/step - loss: 0.0143 - root_mean_squared_error: 0.1194 - val_loss: 0.0179 - val_root_mean_squared_error: 0.1338\nEpoch 18/200\n11/11 [==============================] - 0s 28ms/step - loss: 0.0137 - root_mean_squared_error: 0.1169 - val_loss: 0.0172 - val_root_mean_squared_error: 0.1310\nEpoch 19/200\n11/11 [==============================] - 0s 28ms/step - loss: 0.0151 - root_mean_squared_error: 0.1230 - val_loss: 0.0164 - val_root_mean_squared_error: 0.1279\nEpoch 20/200\n11/11 [==============================] - 0s 28ms/step - loss: 0.0160 - root_mean_squared_error: 0.1264 - val_loss: 0.0216 - val_root_mean_squared_error: 0.1469\nEpoch 21/200\n11/11 [==============================] - 0s 29ms/step - loss: 0.0123 - root_mean_squared_error: 0.1110 - val_loss: 0.0187 - val_root_mean_squared_error: 0.1368\nEpoch 22/200\n11/11 [==============================] - 0s 29ms/step - loss: 0.0134 - root_mean_squared_error: 0.1156 - val_loss: 0.0169 - val_root_mean_squared_error: 0.1298\nEpoch 23/200\n11/11 [==============================] - 0s 29ms/step - loss: 0.0140 - root_mean_squared_error: 0.1182 - val_loss: 0.0148 - val_root_mean_squared_error: 0.1215\nEpoch 24/200\n11/11 [==============================] - 0s 30ms/step - loss: 0.0117 - root_mean_squared_error: 0.1083 - val_loss: 0.0182 - val_root_mean_squared_error: 0.1348\nEpoch 25/200\n11/11 [==============================] - 0s 28ms/step - loss: 0.0138 - root_mean_squared_error: 0.1176 - val_loss: 0.0148 - val_root_mean_squared_error: 0.1216\nEpoch 26/200\n11/11 [==============================] - 0s 29ms/step - loss: 0.0109 - root_mean_squared_error: 0.1044 - val_loss: 0.0138 - val_root_mean_squared_error: 0.1174\nEpoch 27/200\n11/11 [==============================] - 0s 29ms/step - loss: 0.0105 - root_mean_squared_error: 0.1024 - val_loss: 0.0136 - val_root_mean_squared_error: 0.1164\nEpoch 28/200\n11/11 [==============================] - 0s 27ms/step - loss: 0.0135 - root_mean_squared_error: 0.1162 - val_loss: 0.0193 - val_root_mean_squared_error: 0.1389\nEpoch 29/200\n11/11 [==============================] - 0s 28ms/step - loss: 0.0128 - root_mean_squared_error: 0.1130 - val_loss: 0.0161 - val_root_mean_squared_error: 0.1270\nEpoch 30/200\n11/11 [==============================] - 0s 28ms/step - loss: 0.0101 - root_mean_squared_error: 0.1003 - val_loss: 0.0148 - val_root_mean_squared_error: 0.1217\nEpoch 31/200\n11/11 [==============================] - 0s 27ms/step - loss: 0.0104 - root_mean_squared_error: 0.1021 - val_loss: 0.0140 - val_root_mean_squared_error: 0.1183\nEpoch 32/200\n11/11 [==============================] - 0s 28ms/step - loss: 0.0091 - root_mean_squared_error: 0.0955 - val_loss: 0.0134 - val_root_mean_squared_error: 0.1158\nEpoch 33/200\n11/11 [==============================] - 0s 28ms/step - loss: 0.0094 - root_mean_squared_error: 0.0968 - val_loss: 0.0132 - val_root_mean_squared_error: 0.1148\nEpoch 34/200\n11/11 [==============================] - 0s 28ms/step - loss: 0.0119 - root_mean_squared_error: 0.1092 - val_loss: 0.0171 - val_root_mean_squared_error: 0.1307\nEpoch 35/200\n11/11 [==============================] - 0s 27ms/step - loss: 0.0109 - root_mean_squared_error: 0.1044 - val_loss: 0.0125 - val_root_mean_squared_error: 0.1118\nEpoch 36/200\n11/11 [==============================] - 0s 28ms/step - loss: 0.0102 - root_mean_squared_error: 0.1011 - val_loss: 0.0135 - val_root_mean_squared_error: 0.1162\nEpoch 37/200\n11/11 [==============================] - 0s 28ms/step - loss: 0.0096 - root_mean_squared_error: 0.0979 - val_loss: 0.0126 - val_root_mean_squared_error: 0.1122\nEpoch 38/200\n11/11 [==============================] - 0s 28ms/step - loss: 0.0093 - root_mean_squared_error: 0.0963 - val_loss: 0.0128 - val_root_mean_squared_error: 0.1130\nEpoch 39/200\n11/11 [==============================] - 0s 27ms/step - loss: 0.0098 - root_mean_squared_error: 0.0992 - val_loss: 0.0133 - val_root_mean_squared_error: 0.1152\nEpoch 40/200\n11/11 [==============================] - 0s 29ms/step - loss: 0.0098 - root_mean_squared_error: 0.0992 - val_loss: 0.0223 - val_root_mean_squared_error: 0.1493\nEpoch 41/200\n11/11 [==============================] - 0s 28ms/step - loss: 0.0091 - root_mean_squared_error: 0.0955 - val_loss: 0.0164 - val_root_mean_squared_error: 0.1282\nEpoch 42/200\n11/11 [==============================] - 0s 31ms/step - loss: 0.0082 - root_mean_squared_error: 0.0908 - val_loss: 0.0135 - val_root_mean_squared_error: 0.1160\nEpoch 43/200\n11/11 [==============================] - 0s 29ms/step - loss: 0.0108 - root_mean_squared_error: 0.1038 - val_loss: 0.0188 - val_root_mean_squared_error: 0.1369\nEpoch 44/200\n11/11 [==============================] - 0s 28ms/step - loss: 0.0129 - root_mean_squared_error: 0.1136 - val_loss: 0.0118 - val_root_mean_squared_error: 0.1088\nEpoch 45/200\n11/11 [==============================] - 0s 29ms/step - loss: 0.0082 - root_mean_squared_error: 0.0906 - val_loss: 0.0183 - val_root_mean_squared_error: 0.1353\nEpoch 46/200\n11/11 [==============================] - 0s 29ms/step - loss: 0.0089 - root_mean_squared_error: 0.0946 - val_loss: 0.0140 - val_root_mean_squared_error: 0.1185\nEpoch 47/200\n11/11 [==============================] - 0s 28ms/step - loss: 0.0123 - root_mean_squared_error: 0.1109 - val_loss: 0.0113 - val_root_mean_squared_error: 0.1065\nEpoch 48/200\n11/11 [==============================] - 0s 28ms/step - loss: 0.0116 - root_mean_squared_error: 0.1078 - val_loss: 0.0166 - val_root_mean_squared_error: 0.1287\nEpoch 49/200\n11/11 [==============================] - 0s 28ms/step - loss: 0.0120 - root_mean_squared_error: 0.1097 - val_loss: 0.0121 - val_root_mean_squared_error: 0.1100\nEpoch 50/200\n11/11 [==============================] - 0s 27ms/step - loss: 0.0073 - root_mean_squared_error: 0.0854 - val_loss: 0.0122 - val_root_mean_squared_error: 0.1105\nEpoch 51/200\n11/11 [==============================] - 0s 28ms/step - loss: 0.0083 - root_mean_squared_error: 0.0910 - val_loss: 0.0116 - val_root_mean_squared_error: 0.1076\nEpoch 52/200\n11/11 [==============================] - 0s 27ms/step - loss: 0.0082 - root_mean_squared_error: 0.0905 - val_loss: 0.0115 - val_root_mean_squared_error: 0.1071\nEpoch 53/200\n11/11 [==============================] - 0s 28ms/step - loss: 0.0107 - root_mean_squared_error: 0.1036 - val_loss: 0.0142 - val_root_mean_squared_error: 0.1191\nEpoch 54/200\n11/11 [==============================] - 0s 28ms/step - loss: 0.0079 - root_mean_squared_error: 0.0892 - val_loss: 0.0133 - val_root_mean_squared_error: 0.1151\nEpoch 55/200\n11/11 [==============================] - 0s 28ms/step - loss: 0.0071 - root_mean_squared_error: 0.0845 - val_loss: 0.0126 - val_root_mean_squared_error: 0.1124\nEpoch 56/200\n11/11 [==============================] - 0s 28ms/step - loss: 0.0087 - root_mean_squared_error: 0.0932 - val_loss: 0.0168 - val_root_mean_squared_error: 0.1295\nEpoch 57/200\n11/11 [==============================] - 0s 28ms/step - loss: 0.0102 - root_mean_squared_error: 0.1011 - val_loss: 0.0129 - val_root_mean_squared_error: 0.1137\nEpoch 58/200\n11/11 [==============================] - 0s 28ms/step - loss: 0.0082 - root_mean_squared_error: 0.0908 - val_loss: 0.0120 - val_root_mean_squared_error: 0.1094\nEpoch 59/200\n11/11 [==============================] - 0s 28ms/step - loss: 0.0077 - root_mean_squared_error: 0.0875 - val_loss: 0.0117 - val_root_mean_squared_error: 0.1082\nEpoch 60/200\n11/11 [==============================] - 0s 28ms/step - loss: 0.0117 - root_mean_squared_error: 0.1083 - val_loss: 0.0120 - val_root_mean_squared_error: 0.1093\nEpoch 61/200\n11/11 [==============================] - 0s 31ms/step - loss: 0.0117 - root_mean_squared_error: 0.1082 - val_loss: 0.0119 - val_root_mean_squared_error: 0.1092\nEpoch 62/200\n11/11 [==============================] - 0s 33ms/step - loss: 0.0104 - root_mean_squared_error: 0.1019 - val_loss: 0.0119 - val_root_mean_squared_error: 0.1089\nEpoch 63/200\n11/11 [==============================] - 0s 37ms/step - loss: 0.0102 - root_mean_squared_error: 0.1010 - val_loss: 0.0120 - val_root_mean_squared_error: 0.1095\nEpoch 64/200\n11/11 [==============================] - 0s 34ms/step - loss: 0.0070 - root_mean_squared_error: 0.0836 - val_loss: 0.0128 - val_root_mean_squared_error: 0.1129\nEpoch 65/200\n11/11 [==============================] - 0s 30ms/step - loss: 0.0079 - root_mean_squared_error: 0.0891 - val_loss: 0.0114 - val_root_mean_squared_error: 0.1066\nEpoch 66/200\n11/11 [==============================] - 0s 30ms/step - loss: 0.0081 - root_mean_squared_error: 0.0900 - val_loss: 0.0112 - val_root_mean_squared_error: 0.1057\nEpoch 67/200\n11/11 [==============================] - 0s 30ms/step - loss: 0.0068 - root_mean_squared_error: 0.0825 - val_loss: 0.0120 - val_root_mean_squared_error: 0.1097\nEpoch 68/200\n11/11 [==============================] - 0s 29ms/step - loss: 0.0090 - root_mean_squared_error: 0.0947 - val_loss: 0.0116 - val_root_mean_squared_error: 0.1075\nEpoch 69/200\n11/11 [==============================] - 0s 30ms/step - loss: 0.0085 - root_mean_squared_error: 0.0923 - val_loss: 0.0162 - val_root_mean_squared_error: 0.1272\nEpoch 70/200\n11/11 [==============================] - 0s 29ms/step - loss: 0.0090 - root_mean_squared_error: 0.0948 - val_loss: 0.0181 - val_root_mean_squared_error: 0.1344\nEpoch 71/200\n11/11 [==============================] - 0s 28ms/step - loss: 0.0081 - root_mean_squared_error: 0.0902 - val_loss: 0.0132 - val_root_mean_squared_error: 0.1150\nEpoch 72/200\n11/11 [==============================] - 0s 27ms/step - loss: 0.0084 - root_mean_squared_error: 0.0919 - val_loss: 0.0228 - val_root_mean_squared_error: 0.1511\nEpoch 73/200\n11/11 [==============================] - 0s 28ms/step - loss: 0.0094 - root_mean_squared_error: 0.0971 - val_loss: 0.0136 - val_root_mean_squared_error: 0.1168\nEpoch 74/200\n11/11 [==============================] - 0s 28ms/step - loss: 0.0072 - root_mean_squared_error: 0.0850 - val_loss: 0.0115 - val_root_mean_squared_error: 0.1070\nEpoch 75/200\n11/11 [==============================] - 0s 28ms/step - loss: 0.0066 - root_mean_squared_error: 0.0813 - val_loss: 0.0113 - val_root_mean_squared_error: 0.1063\nEpoch 76/200\n11/11 [==============================] - 0s 28ms/step - loss: 0.0056 - root_mean_squared_error: 0.0750 - val_loss: 0.0111 - val_root_mean_squared_error: 0.1052\nEpoch 77/200\n11/11 [==============================] - 0s 27ms/step - loss: 0.0063 - root_mean_squared_error: 0.0795 - val_loss: 0.0118 - val_root_mean_squared_error: 0.1084\nEpoch 78/200\n11/11 [==============================] - 0s 27ms/step - loss: 0.0071 - root_mean_squared_error: 0.0842 - val_loss: 0.0137 - val_root_mean_squared_error: 0.1169\nEpoch 79/200\n11/11 [==============================] - 0s 29ms/step - loss: 0.0081 - root_mean_squared_error: 0.0897 - val_loss: 0.0132 - val_root_mean_squared_error: 0.1149\nEpoch 80/200\n11/11 [==============================] - 0s 28ms/step - loss: 0.0070 - root_mean_squared_error: 0.0837 - val_loss: 0.0152 - val_root_mean_squared_error: 0.1231\nEpoch 81/200\n11/11 [==============================] - 0s 27ms/step - loss: 0.0080 - root_mean_squared_error: 0.0893 - val_loss: 0.0123 - val_root_mean_squared_error: 0.1110\nEpoch 82/200\n11/11 [==============================] - 0s 28ms/step - loss: 0.0096 - root_mean_squared_error: 0.0979 - val_loss: 0.0175 - val_root_mean_squared_error: 0.1324\nEpoch 83/200\n11/11 [==============================] - 0s 28ms/step - loss: 0.0085 - root_mean_squared_error: 0.0920 - val_loss: 0.0154 - val_root_mean_squared_error: 0.1243\nEpoch 84/200\n11/11 [==============================] - 0s 32ms/step - loss: 0.0074 - root_mean_squared_error: 0.0862 - val_loss: 0.0127 - val_root_mean_squared_error: 0.1127\nEpoch 85/200\n11/11 [==============================] - 0s 29ms/step - loss: 0.0081 - root_mean_squared_error: 0.0900 - val_loss: 0.0117 - val_root_mean_squared_error: 0.1081\nEpoch 86/200\n11/11 [==============================] - 0s 29ms/step - loss: 0.0074 - root_mean_squared_error: 0.0857 - val_loss: 0.0105 - val_root_mean_squared_error: 0.1024\nEpoch 87/200\n11/11 [==============================] - 0s 29ms/step - loss: 0.0067 - root_mean_squared_error: 0.0816 - val_loss: 0.0112 - val_root_mean_squared_error: 0.1056\nEpoch 88/200\n11/11 [==============================] - 0s 29ms/step - loss: 0.0060 - root_mean_squared_error: 0.0777 - val_loss: 0.0112 - val_root_mean_squared_error: 0.1056\nEpoch 89/200\n11/11 [==============================] - 0s 28ms/step - loss: 0.0072 - root_mean_squared_error: 0.0849 - val_loss: 0.0128 - val_root_mean_squared_error: 0.1133\nEpoch 90/200\n11/11 [==============================] - 0s 28ms/step - loss: 0.0065 - root_mean_squared_error: 0.0808 - val_loss: 0.0108 - val_root_mean_squared_error: 0.1039\nEpoch 91/200\n11/11 [==============================] - 0s 28ms/step - loss: 0.0061 - root_mean_squared_error: 0.0783 - val_loss: 0.0111 - val_root_mean_squared_error: 0.1053\nEpoch 92/200\n11/11 [==============================] - 0s 28ms/step - loss: 0.0056 - root_mean_squared_error: 0.0750 - val_loss: 0.0110 - val_root_mean_squared_error: 0.1046\nEpoch 93/200\n11/11 [==============================] - 0s 28ms/step - loss: 0.0063 - root_mean_squared_error: 0.0791 - val_loss: 0.0112 - val_root_mean_squared_error: 0.1061\nEpoch 94/200\n11/11 [==============================] - 0s 28ms/step - loss: 0.0061 - root_mean_squared_error: 0.0781 - val_loss: 0.0110 - val_root_mean_squared_error: 0.1050\nEpoch 95/200\n11/11 [==============================] - 0s 27ms/step - loss: 0.0072 - root_mean_squared_error: 0.0851 - val_loss: 0.0123 - val_root_mean_squared_error: 0.1108\nEpoch 96/200\n11/11 [==============================] - 0s 29ms/step - loss: 0.0080 - root_mean_squared_error: 0.0893 - val_loss: 0.0112 - val_root_mean_squared_error: 0.1060\nEpoch 97/200\n11/11 [==============================] - 0s 30ms/step - loss: 0.0072 - root_mean_squared_error: 0.0851 - val_loss: 0.0110 - val_root_mean_squared_error: 0.1050\nEpoch 98/200\n11/11 [==============================] - 0s 28ms/step - loss: 0.0067 - root_mean_squared_error: 0.0818 - val_loss: 0.0118 - val_root_mean_squared_error: 0.1086\nEpoch 99/200\n11/11 [==============================] - 0s 27ms/step - loss: 0.0070 - root_mean_squared_error: 0.0835 - val_loss: 0.0114 - val_root_mean_squared_error: 0.1067\nEpoch 100/200\n11/11 [==============================] - 0s 27ms/step - loss: 0.0068 - root_mean_squared_error: 0.0827 - val_loss: 0.0119 - val_root_mean_squared_error: 0.1090\nEpoch 101/200\n11/11 [==============================] - 0s 28ms/step - loss: 0.0058 - root_mean_squared_error: 0.0760 - val_loss: 0.0112 - val_root_mean_squared_error: 0.1056\nEpoch 102/200\n11/11 [==============================] - 0s 27ms/step - loss: 0.0066 - root_mean_squared_error: 0.0812 - val_loss: 0.0105 - val_root_mean_squared_error: 0.1025\nEpoch 103/200\n11/11 [==============================] - 0s 27ms/step - loss: 0.0070 - root_mean_squared_error: 0.0834 - val_loss: 0.0106 - val_root_mean_squared_error: 0.1032\nEpoch 104/200\n11/11 [==============================] - 0s 27ms/step - loss: 0.0072 - root_mean_squared_error: 0.0847 - val_loss: 0.0110 - val_root_mean_squared_error: 0.1050\nEpoch 105/200\n11/11 [==============================] - 0s 27ms/step - loss: 0.0074 - root_mean_squared_error: 0.0859 - val_loss: 0.0126 - val_root_mean_squared_error: 0.1123\nEpoch 106/200\n11/11 [==============================] - 0s 28ms/step - loss: 0.0089 - root_mean_squared_error: 0.0941 - val_loss: 0.0127 - val_root_mean_squared_error: 0.1126\nEpoch 107/200\n11/11 [==============================] - 0s 28ms/step - loss: 0.0102 - root_mean_squared_error: 0.1011 - val_loss: 0.0137 - val_root_mean_squared_error: 0.1171\nEpoch 108/200\n11/11 [==============================] - 0s 28ms/step - loss: 0.0073 - root_mean_squared_error: 0.0853 - val_loss: 0.0130 - val_root_mean_squared_error: 0.1142\nEpoch 109/200\n11/11 [==============================] - 0s 28ms/step - loss: 0.0061 - root_mean_squared_error: 0.0784 - val_loss: 0.0118 - val_root_mean_squared_error: 0.1085\nEpoch 110/200\n11/11 [==============================] - 0s 28ms/step - loss: 0.0073 - root_mean_squared_error: 0.0855 - val_loss: 0.0153 - val_root_mean_squared_error: 0.1236\nEpoch 111/200\n11/11 [==============================] - 0s 29ms/step - loss: 0.0071 - root_mean_squared_error: 0.0843 - val_loss: 0.0117 - val_root_mean_squared_error: 0.1084\nEpoch 112/200\n11/11 [==============================] - 0s 29ms/step - loss: 0.0083 - root_mean_squared_error: 0.0911 - val_loss: 0.0100 - val_root_mean_squared_error: 0.1001\nEpoch 113/200\n11/11 [==============================] - 0s 28ms/step - loss: 0.0081 - root_mean_squared_error: 0.0902 - val_loss: 0.0120 - val_root_mean_squared_error: 0.1097\nEpoch 114/200\n11/11 [==============================] - 0s 28ms/step - loss: 0.0094 - root_mean_squared_error: 0.0969 - val_loss: 0.0149 - val_root_mean_squared_error: 0.1220\nEpoch 115/200\n11/11 [==============================] - 0s 27ms/step - loss: 0.0123 - root_mean_squared_error: 0.1107 - val_loss: 0.0147 - val_root_mean_squared_error: 0.1212\nEpoch 116/200\n11/11 [==============================] - 0s 30ms/step - loss: 0.0098 - root_mean_squared_error: 0.0988 - val_loss: 0.0110 - val_root_mean_squared_error: 0.1051\nEpoch 117/200\n11/11 [==============================] - 0s 27ms/step - loss: 0.0059 - root_mean_squared_error: 0.0767 - val_loss: 0.0107 - val_root_mean_squared_error: 0.1035\nEpoch 118/200\n11/11 [==============================] - 0s 28ms/step - loss: 0.0064 - root_mean_squared_error: 0.0802 - val_loss: 0.0107 - val_root_mean_squared_error: 0.1036\nEpoch 119/200\n11/11 [==============================] - 0s 28ms/step - loss: 0.0077 - root_mean_squared_error: 0.0876 - val_loss: 0.0102 - val_root_mean_squared_error: 0.1009\nEpoch 120/200\n11/11 [==============================] - 0s 27ms/step - loss: 0.0080 - root_mean_squared_error: 0.0896 - val_loss: 0.0117 - val_root_mean_squared_error: 0.1083\nEpoch 121/200\n11/11 [==============================] - 0s 27ms/step - loss: 0.0073 - root_mean_squared_error: 0.0857 - val_loss: 0.0113 - val_root_mean_squared_error: 0.1065\nEpoch 122/200\n11/11 [==============================] - 0s 28ms/step - loss: 0.0073 - root_mean_squared_error: 0.0853 - val_loss: 0.0120 - val_root_mean_squared_error: 0.1095\nEpoch 123/200\n11/11 [==============================] - 0s 27ms/step - loss: 0.0070 - root_mean_squared_error: 0.0836 - val_loss: 0.0111 - val_root_mean_squared_error: 0.1051\nEpoch 124/200\n11/11 [==============================] - 0s 27ms/step - loss: 0.0063 - root_mean_squared_error: 0.0793 - val_loss: 0.0117 - val_root_mean_squared_error: 0.1080\nEpoch 125/200\n11/11 [==============================] - 0s 28ms/step - loss: 0.0063 - root_mean_squared_error: 0.0791 - val_loss: 0.0150 - val_root_mean_squared_error: 0.1224\nEpoch 126/200\n11/11 [==============================] - 0s 29ms/step - loss: 0.0081 - root_mean_squared_error: 0.0898 - val_loss: 0.0115 - val_root_mean_squared_error: 0.1075\nEpoch 127/200\n11/11 [==============================] - 0s 31ms/step - loss: 0.0079 - root_mean_squared_error: 0.0891 - val_loss: 0.0102 - val_root_mean_squared_error: 0.1008\nEpoch 128/200\n11/11 [==============================] - 0s 30ms/step - loss: 0.0090 - root_mean_squared_error: 0.0949 - val_loss: 0.0107 - val_root_mean_squared_error: 0.1032\nEpoch 129/200\n11/11 [==============================] - 0s 29ms/step - loss: 0.0072 - root_mean_squared_error: 0.0848 - val_loss: 0.0099 - val_root_mean_squared_error: 0.0996\nEpoch 130/200\n11/11 [==============================] - 0s 30ms/step - loss: 0.0069 - root_mean_squared_error: 0.0832 - val_loss: 0.0112 - val_root_mean_squared_error: 0.1058\nEpoch 131/200\n11/11 [==============================] - 0s 30ms/step - loss: 0.0060 - root_mean_squared_error: 0.0778 - val_loss: 0.0100 - val_root_mean_squared_error: 0.0998\nEpoch 132/200\n11/11 [==============================] - 0s 30ms/step - loss: 0.0070 - root_mean_squared_error: 0.0836 - val_loss: 0.0107 - val_root_mean_squared_error: 0.1034\nEpoch 133/200\n11/11 [==============================] - 0s 29ms/step - loss: 0.0076 - root_mean_squared_error: 0.0874 - val_loss: 0.0109 - val_root_mean_squared_error: 0.1042\nEpoch 134/200\n11/11 [==============================] - 0s 28ms/step - loss: 0.0071 - root_mean_squared_error: 0.0843 - val_loss: 0.0111 - val_root_mean_squared_error: 0.1053\nEpoch 135/200\n11/11 [==============================] - 0s 29ms/step - loss: 0.0060 - root_mean_squared_error: 0.0777 - val_loss: 0.0113 - val_root_mean_squared_error: 0.1063\nEpoch 136/200\n11/11 [==============================] - 0s 28ms/step - loss: 0.0095 - root_mean_squared_error: 0.0977 - val_loss: 0.0211 - val_root_mean_squared_error: 0.1451\nEpoch 137/200\n11/11 [==============================] - 0s 28ms/step - loss: 0.0118 - root_mean_squared_error: 0.1086 - val_loss: 0.0136 - val_root_mean_squared_error: 0.1166\nEpoch 138/200\n11/11 [==============================] - 0s 28ms/step - loss: 0.0076 - root_mean_squared_error: 0.0872 - val_loss: 0.0104 - val_root_mean_squared_error: 0.1022\nEpoch 139/200\n11/11 [==============================] - 0s 27ms/step - loss: 0.0079 - root_mean_squared_error: 0.0887 - val_loss: 0.0100 - val_root_mean_squared_error: 0.1002\nEpoch 140/200\n11/11 [==============================] - 0s 27ms/step - loss: 0.0058 - root_mean_squared_error: 0.0762 - val_loss: 0.0099 - val_root_mean_squared_error: 0.0993\nEpoch 141/200\n11/11 [==============================] - 0s 27ms/step - loss: 0.0059 - root_mean_squared_error: 0.0766 - val_loss: 0.0115 - val_root_mean_squared_error: 0.1074\nEpoch 142/200\n11/11 [==============================] - 0s 27ms/step - loss: 0.0067 - root_mean_squared_error: 0.0816 - val_loss: 0.0160 - val_root_mean_squared_error: 0.1263\nEpoch 143/200\n11/11 [==============================] - 0s 28ms/step - loss: 0.0091 - root_mean_squared_error: 0.0954 - val_loss: 0.0106 - val_root_mean_squared_error: 0.1028\nEpoch 144/200\n11/11 [==============================] - 0s 27ms/step - loss: 0.0061 - root_mean_squared_error: 0.0783 - val_loss: 0.0102 - val_root_mean_squared_error: 0.1009\nEpoch 145/200\n11/11 [==============================] - 0s 27ms/step - loss: 0.0064 - root_mean_squared_error: 0.0799 - val_loss: 0.0111 - val_root_mean_squared_error: 0.1052\nEpoch 146/200\n11/11 [==============================] - 0s 27ms/step - loss: 0.0057 - root_mean_squared_error: 0.0754 - val_loss: 0.0123 - val_root_mean_squared_error: 0.1108\nEpoch 147/200\n11/11 [==============================] - 0s 27ms/step - loss: 0.0058 - root_mean_squared_error: 0.0764 - val_loss: 0.0109 - val_root_mean_squared_error: 0.1043\nEpoch 148/200\n11/11 [==============================] - 0s 27ms/step - loss: 0.0061 - root_mean_squared_error: 0.0781 - val_loss: 0.0104 - val_root_mean_squared_error: 0.1018\nEpoch 149/200\n11/11 [==============================] - 0s 27ms/step - loss: 0.0047 - root_mean_squared_error: 0.0686 - val_loss: 0.0104 - val_root_mean_squared_error: 0.1021\nEpoch 150/200\n11/11 [==============================] - 0s 28ms/step - loss: 0.0064 - root_mean_squared_error: 0.0803 - val_loss: 0.0101 - val_root_mean_squared_error: 0.1004\nEpoch 151/200\n11/11 [==============================] - 0s 28ms/step - loss: 0.0075 - root_mean_squared_error: 0.0865 - val_loss: 0.0096 - val_root_mean_squared_error: 0.0982\nEpoch 152/200\n11/11 [==============================] - 0s 28ms/step - loss: 0.0061 - root_mean_squared_error: 0.0783 - val_loss: 0.0123 - val_root_mean_squared_error: 0.1107\nEpoch 153/200\n11/11 [==============================] - 0s 28ms/step - loss: 0.0054 - root_mean_squared_error: 0.0736 - val_loss: 0.0097 - val_root_mean_squared_error: 0.0983\nEpoch 154/200\n11/11 [==============================] - 0s 29ms/step - loss: 0.0064 - root_mean_squared_error: 0.0801 - val_loss: 0.0097 - val_root_mean_squared_error: 0.0987\nEpoch 155/200\n11/11 [==============================] - 0s 29ms/step - loss: 0.0052 - root_mean_squared_error: 0.0721 - val_loss: 0.0110 - val_root_mean_squared_error: 0.1051\nEpoch 156/200\n11/11 [==============================] - 0s 28ms/step - loss: 0.0058 - root_mean_squared_error: 0.0760 - val_loss: 0.0103 - val_root_mean_squared_error: 0.1016\nEpoch 157/200\n11/11 [==============================] - 0s 28ms/step - loss: 0.0069 - root_mean_squared_error: 0.0828 - val_loss: 0.0107 - val_root_mean_squared_error: 0.1036\nEpoch 158/200\n11/11 [==============================] - 0s 28ms/step - loss: 0.0046 - root_mean_squared_error: 0.0679 - val_loss: 0.0104 - val_root_mean_squared_error: 0.1019\nEpoch 159/200\n11/11 [==============================] - 0s 28ms/step - loss: 0.0064 - root_mean_squared_error: 0.0802 - val_loss: 0.0127 - val_root_mean_squared_error: 0.1129\nEpoch 160/200\n11/11 [==============================] - 0s 28ms/step - loss: 0.0061 - root_mean_squared_error: 0.0779 - val_loss: 0.0107 - val_root_mean_squared_error: 0.1034\nEpoch 161/200\n11/11 [==============================] - 0s 27ms/step - loss: 0.0074 - root_mean_squared_error: 0.0863 - val_loss: 0.0101 - val_root_mean_squared_error: 0.1004\nEpoch 162/200\n11/11 [==============================] - 0s 28ms/step - loss: 0.0057 - root_mean_squared_error: 0.0757 - val_loss: 0.0102 - val_root_mean_squared_error: 0.1011\nEpoch 163/200\n11/11 [==============================] - 0s 27ms/step - loss: 0.0054 - root_mean_squared_error: 0.0736 - val_loss: 0.0117 - val_root_mean_squared_error: 0.1084\nEpoch 164/200\n11/11 [==============================] - 0s 28ms/step - loss: 0.0059 - root_mean_squared_error: 0.0770 - val_loss: 0.0126 - val_root_mean_squared_error: 0.1120\nEpoch 165/200\n11/11 [==============================] - 0s 28ms/step - loss: 0.0055 - root_mean_squared_error: 0.0744 - val_loss: 0.0105 - val_root_mean_squared_error: 0.1024\nEpoch 166/200\n11/11 [==============================] - 0s 28ms/step - loss: 0.0052 - root_mean_squared_error: 0.0723 - val_loss: 0.0105 - val_root_mean_squared_error: 0.1024\nEpoch 167/200\n11/11 [==============================] - 0s 28ms/step - loss: 0.0056 - root_mean_squared_error: 0.0747 - val_loss: 0.0107 - val_root_mean_squared_error: 0.1033\nEpoch 168/200\n11/11 [==============================] - 0s 28ms/step - loss: 0.0056 - root_mean_squared_error: 0.0752 - val_loss: 0.0107 - val_root_mean_squared_error: 0.1035\nEpoch 169/200\n11/11 [==============================] - 0s 29ms/step - loss: 0.0057 - root_mean_squared_error: 0.0758 - val_loss: 0.0106 - val_root_mean_squared_error: 0.1030\nEpoch 170/200\n11/11 [==============================] - 0s 29ms/step - loss: 0.0074 - root_mean_squared_error: 0.0859 - val_loss: 0.0109 - val_root_mean_squared_error: 0.1043\nEpoch 171/200\n11/11 [==============================] - 0s 29ms/step - loss: 0.0090 - root_mean_squared_error: 0.0948 - val_loss: 0.0121 - val_root_mean_squared_error: 0.1098\nEpoch 172/200\n11/11 [==============================] - 0s 30ms/step - loss: 0.0061 - root_mean_squared_error: 0.0781 - val_loss: 0.0101 - val_root_mean_squared_error: 0.1006\nEpoch 173/200\n11/11 [==============================] - 0s 29ms/step - loss: 0.0046 - root_mean_squared_error: 0.0681 - val_loss: 0.0103 - val_root_mean_squared_error: 0.1013\nEpoch 174/200\n11/11 [==============================] - 0s 30ms/step - loss: 0.0059 - root_mean_squared_error: 0.0770 - val_loss: 0.0103 - val_root_mean_squared_error: 0.1013\nEpoch 175/200\n11/11 [==============================] - 0s 30ms/step - loss: 0.0069 - root_mean_squared_error: 0.0832 - val_loss: 0.0149 - val_root_mean_squared_error: 0.1220\nEpoch 176/200\n11/11 [==============================] - 0s 29ms/step - loss: 0.0065 - root_mean_squared_error: 0.0806 - val_loss: 0.0166 - val_root_mean_squared_error: 0.1287\nEpoch 177/200\n11/11 [==============================] - 0s 28ms/step - loss: 0.0069 - root_mean_squared_error: 0.0828 - val_loss: 0.0102 - val_root_mean_squared_error: 0.1008\nEpoch 178/200\n11/11 [==============================] - 0s 27ms/step - loss: 0.0054 - root_mean_squared_error: 0.0736 - val_loss: 0.0113 - val_root_mean_squared_error: 0.1061\nEpoch 179/200\n11/11 [==============================] - 0s 28ms/step - loss: 0.0053 - root_mean_squared_error: 0.0730 - val_loss: 0.0105 - val_root_mean_squared_error: 0.1026\nEpoch 180/200\n11/11 [==============================] - 0s 31ms/step - loss: 0.0057 - root_mean_squared_error: 0.0754 - val_loss: 0.0102 - val_root_mean_squared_error: 0.1011\nEpoch 181/200\n11/11 [==============================] - 0s 29ms/step - loss: 0.0071 - root_mean_squared_error: 0.0844 - val_loss: 0.0173 - val_root_mean_squared_error: 0.1313\nEpoch 182/200\n11/11 [==============================] - 0s 27ms/step - loss: 0.0075 - root_mean_squared_error: 0.0867 - val_loss: 0.0111 - val_root_mean_squared_error: 0.1054\nEpoch 183/200\n11/11 [==============================] - 0s 28ms/step - loss: 0.0060 - root_mean_squared_error: 0.0772 - val_loss: 0.0129 - val_root_mean_squared_error: 0.1136\nEpoch 184/200\n11/11 [==============================] - 0s 27ms/step - loss: 0.0062 - root_mean_squared_error: 0.0787 - val_loss: 0.0107 - val_root_mean_squared_error: 0.1034\nEpoch 185/200\n11/11 [==============================] - 0s 27ms/step - loss: 0.0046 - root_mean_squared_error: 0.0681 - val_loss: 0.0104 - val_root_mean_squared_error: 0.1022\nEpoch 186/200\n11/11 [==============================] - 0s 28ms/step - loss: 0.0050 - root_mean_squared_error: 0.0708 - val_loss: 0.0105 - val_root_mean_squared_error: 0.1024\nEpoch 187/200\n11/11 [==============================] - 0s 27ms/step - loss: 0.0048 - root_mean_squared_error: 0.0695 - val_loss: 0.0097 - val_root_mean_squared_error: 0.0985\nEpoch 188/200\n11/11 [==============================] - 0s 29ms/step - loss: 0.0053 - root_mean_squared_error: 0.0726 - val_loss: 0.0099 - val_root_mean_squared_error: 0.0994\nEpoch 189/200\n11/11 [==============================] - 0s 29ms/step - loss: 0.0048 - root_mean_squared_error: 0.0696 - val_loss: 0.0101 - val_root_mean_squared_error: 0.1006\nEpoch 190/200\n11/11 [==============================] - 0s 27ms/step - loss: 0.0056 - root_mean_squared_error: 0.0747 - val_loss: 0.0119 - val_root_mean_squared_error: 0.1092\nEpoch 191/200\n11/11 [==============================] - 0s 28ms/step - loss: 0.0061 - root_mean_squared_error: 0.0778 - val_loss: 0.0104 - val_root_mean_squared_error: 0.1019\nEpoch 192/200\n11/11 [==============================] - 0s 28ms/step - loss: 0.0050 - root_mean_squared_error: 0.0704 - val_loss: 0.0111 - val_root_mean_squared_error: 0.1055\nEpoch 193/200\n11/11 [==============================] - 0s 29ms/step - loss: 0.0057 - root_mean_squared_error: 0.0753 - val_loss: 0.0100 - val_root_mean_squared_error: 0.1000\nEpoch 194/200\n11/11 [==============================] - 0s 28ms/step - loss: 0.0040 - root_mean_squared_error: 0.0634 - val_loss: 0.0100 - val_root_mean_squared_error: 0.1001\nEpoch 195/200\n11/11 [==============================] - 0s 28ms/step - loss: 0.0055 - root_mean_squared_error: 0.0745 - val_loss: 0.0131 - val_root_mean_squared_error: 0.1143\nEpoch 196/200\n11/11 [==============================] - 0s 29ms/step - loss: 0.0065 - root_mean_squared_error: 0.0807 - val_loss: 0.0117 - val_root_mean_squared_error: 0.1080\nEpoch 197/200\n11/11 [==============================] - 0s 31ms/step - loss: 0.0064 - root_mean_squared_error: 0.0802 - val_loss: 0.0108 - val_root_mean_squared_error: 0.1041\nEpoch 198/200\n11/11 [==============================] - 0s 28ms/step - loss: 0.0055 - root_mean_squared_error: 0.0742 - val_loss: 0.0098 - val_root_mean_squared_error: 0.0992\nEpoch 199/200\n11/11 [==============================] - 0s 28ms/step - loss: 0.0065 - root_mean_squared_error: 0.0809 - val_loss: 0.0124 - val_root_mean_squared_error: 0.1115\nEpoch 200/200\n11/11 [==============================] - 0s 28ms/step - loss: 0.0048 - root_mean_squared_error: 0.0693 - val_loss: 0.0096 - val_root_mean_squared_error: 0.0980\nmae: tf.Tensor(33.636, shape=(), dtype=float32)\nmse: tf.Tensor(2514.0896, shape=(), dtype=float32)\nrmse: tf.Tensor(50.140697, shape=(), dtype=float32)\nmae: tf.Tensor(33.636, shape=(), dtype=float32)\nmse: tf.Tensor(2514.0896, shape=(), dtype=float32)\nrmse: tf.Tensor(50.140697, shape=(), dtype=float32)\nShutting down background jobs, please wait a moment...\nDone!\nWaiting for the remaining 90 operations to synchronize with Neptune. Do not kill this process.\nAll 90 operations synced, thanks for waiting!\nExplore the metadata in the Neptune app:\nhttps://app.neptune.ai/milestone2-california-water-shortage/deeplearning-lstm/e/DEEPLSTM-141\n"
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Model3 Hyper-parameter tuning\n* a simple or bidirectional encoding LSTM layer\n* a simple or bidirectional decoding LSTM layer\n* a Dense unit\n* a Dropout Unit",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "cell_id": "00026-df34b2bd-0c41-470b-89c8-e57a491c8fab",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "cell_id": "00027-6351f110-c2bc-4bd9-aece-01cd07181bfe",
    "deepnote_cell_type": "code"
   },
   "source": "class Model3(keras_tuner.HyperModel):\n    def build(self, hp):\n        model = Sequential()\n        lstm_units = hp.Int(\"lstm_units\", min_value=20, max_value=300, step=20)\n        lstm_activ = hp.Choice(\"lstm_activation\", values=[\"relu\", \"tanh\", \"sigmoid\"])\n        model.add(LSTM(units=lstm_units, activation=lstm_activ, input_shape=(7, nb_features)))\n        model.add(RepeatVector(1))\n        lstm_units_2 = hp.Int(\"2nd_lstm_units\", min_value=20, max_value=300, step=20)\n        lstm_activ_2 = hp.Choice(\"2nd_lstm_activation\", values=[\"relu\", \"tanh\", \"sigmoid\"])\n        model.add(LSTM(units=lstm_units_2, activation=lstm_activ_2, return_sequences=True))\n        dense_units = hp.Int(\"dense_units\", min_value=11, max_value=101, step=10)\n        dense_activation = hp.Choice(\"dense_activation\", values=[\"relu\", \"tanh\", \"sigmoid\"])\n        model.add(TimeDistributed(Dense(dense_units, activation=dense_activation)))\n        hp_dropout = hp.Float(\"dropout_rate\", min_value=0.05, max_value=0.25, step=0.05)\n        model.add(Dropout(hp_dropout))\n        model.add(Dense(1, activation=output_activation))\n\n        hp_learning_rate = hp.Choice(\"learning_rate\", values=[1e-2, 1e-3, 1e-4])\n        model.compile(loss=\"mse\", optimizer=Adam(learning_rate=hp_learning_rate), metrics=[keras.metrics.RootMeanSquaredError()])\n\n        return model\n\n    def fit(self, hp, model, *args, **kwargs):\n        return model.fit(\n            *args,\n            validation_split=hp.Float(\"validation_split\", min_value=0.05, max_value=0.2, step=0.05),\n            batch_size=hp.Int(\"batch_size\", min_value=32, max_value=192, step=32),\n            #epochs=hp.Int(\"epochs\", min_value=50, max_value=400, step=50),\n            **kwargs,\n        )",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "cell_id": "00028-90c2100f-25be-4429-bb0f-9db6840a80ce",
    "deepnote_cell_type": "code"
   },
   "source": "stop_early = tensorflow.keras.callbacks.EarlyStopping(monitor=\"val_root_mean_squared_error\", patience=7, verbose=1)\ntuner = keras_tuner.Hyperband(Model3(),\n                              objective=keras_tuner.Objective(\"val_root_mean_squared_error\", direction=\"min\"),\n                              max_epochs=200000,\n                              factor=2000,\n                              overwrite=True,\n                              directory=\"keras_tuner\",\n                              project_name=\"keras_tuner\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    },
    "cell_id": "00029-8d2c311b-63d9-4cb7-9299-2b91235e196d",
    "deepnote_cell_type": "code"
   },
   "source": "tuner.search(X_train, y_train, epochs=200, callbacks=[stop_early])\n\n# Get the optimal hyperparameters\nbest_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n\nprint(f\"\"\"\nThe hyperparameter search is complete.\nlstm_units: {best_hps.get('lstm_units')}\nlstm_activation: {best_hps.get('lstm_activation')}\n2nd_lstm_units: {best_hps.get('2nd_lstm_units')}\n2nd_lstm_activation: {best_hps.get('2nd_lstm_activation')}\ndense_units: {best_hps.get('dense_units')}\ndense_activation: {best_hps.get('dense_activation')}\ndropout_rate: {best_hps.get('dropout_rate')}\nlearning_rate: {best_hps.get('learning_rate')}\nbatch_size: {best_hps.get('batch_size')}\nvalidation_split: {best_hps.get('validation_split')}\n\"\"\")",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Trial 1109 Complete [00h 00m 55s]\nval_root_mean_squared_error: 0.08242339640855789\n\nBest val_root_mean_squared_error So Far: 0.07807664573192596\nTotal elapsed time: 05h 08m 35s\n\nSearch: Running Trial #1110\n\nValue             |Best Value So Far |Hyperparameter\n280               |300               |lstm_units\nrelu              |sigmoid           |lstm_activation\n280               |140               |2nd_lstm_units\ntanh              |tanh              |2nd_lstm_activation\n71                |21                |dense_units\nsigmoid           |tanh              |dense_activation\n0.2               |0.2               |dropout_rate\n0.0001            |0.001             |learning_rate\n0.05              |0.1               |validation_split\n32                |32                |batch_size\n100               |100               |tuner/epochs\n0                 |0                 |tuner/initial_epoch\n1                 |1                 |tuner/bracket\n0                 |0                 |tuner/round\n\nEpoch 1/100\n13/13 [==============================] - 4s 158ms/step - loss: 0.4688 - root_mean_squared_error: 0.6847 - val_loss: 0.4293 - val_root_mean_squared_error: 0.6552\nEpoch 2/100\n13/13 [==============================] - 2s 138ms/step - loss: 0.4196 - root_mean_squared_error: 0.6478 - val_loss: 0.3394 - val_root_mean_squared_error: 0.5825\nEpoch 3/100\n13/13 [==============================] - 2s 180ms/step - loss: 0.3454 - root_mean_squared_error: 0.5877 - val_loss: 0.2436 - val_root_mean_squared_error: 0.4936\nEpoch 4/100\n13/13 [==============================] - 2s 155ms/step - loss: 0.2649 - root_mean_squared_error: 0.5147 - val_loss: 0.1206 - val_root_mean_squared_error: 0.3473\nEpoch 5/100\n13/13 [==============================] - 1s 110ms/step - loss: 0.1653 - root_mean_squared_error: 0.4065 - val_loss: 0.0369 - val_root_mean_squared_error: 0.1922\nEpoch 6/100\n13/13 [==============================] - 2s 142ms/step - loss: 0.1532 - root_mean_squared_error: 0.3914 - val_loss: 0.0362 - val_root_mean_squared_error: 0.1902\nEpoch 7/100\n13/13 [==============================] - 2s 162ms/step - loss: 0.1451 - root_mean_squared_error: 0.3809 - val_loss: 0.0414 - val_root_mean_squared_error: 0.2035\nEpoch 8/100\n13/13 [==============================] - 2s 132ms/step - loss: 0.1423 - root_mean_squared_error: 0.3772 - val_loss: 0.0361 - val_root_mean_squared_error: 0.1900\nEpoch 9/100\n13/13 [==============================] - 2s 115ms/step - loss: 0.1472 - root_mean_squared_error: 0.3836 - val_loss: 0.0342 - val_root_mean_squared_error: 0.1850\nEpoch 10/100\n13/13 [==============================] - 2s 165ms/step - loss: 0.1332 - root_mean_squared_error: 0.3650 - val_loss: 0.0319 - val_root_mean_squared_error: 0.1787\nEpoch 11/100\n13/13 [==============================] - 2s 190ms/step - loss: 0.1151 - root_mean_squared_error: 0.3392 - val_loss: 0.0319 - val_root_mean_squared_error: 0.1787\nEpoch 12/100\n13/13 [==============================] - 2s 152ms/step - loss: 0.1333 - root_mean_squared_error: 0.3651 - val_loss: 0.0302 - val_root_mean_squared_error: 0.1739\nEpoch 13/100\n13/13 [==============================] - 1s 98ms/step - loss: 0.1491 - root_mean_squared_error: 0.3861 - val_loss: 0.0307 - val_root_mean_squared_error: 0.1752\nEpoch 14/100\n13/13 [==============================] - 2s 138ms/step - loss: 0.1313 - root_mean_squared_error: 0.3624 - val_loss: 0.0327 - val_root_mean_squared_error: 0.1809\nEpoch 15/100\n13/13 [==============================] - 3s 199ms/step - loss: 0.1274 - root_mean_squared_error: 0.3569 - val_loss: 0.0299 - val_root_mean_squared_error: 0.1728\nEpoch 16/100\n13/13 [==============================] - 2s 126ms/step - loss: 0.1245 - root_mean_squared_error: 0.3528 - val_loss: 0.0297 - val_root_mean_squared_error: 0.1723\nEpoch 17/100\n13/13 [==============================] - 2s 120ms/step - loss: 0.1269 - root_mean_squared_error: 0.3563 - val_loss: 0.0280 - val_root_mean_squared_error: 0.1673\nEpoch 18/100\n13/13 [==============================] - 2s 183ms/step - loss: 0.1130 - root_mean_squared_error: 0.3362 - val_loss: 0.0286 - val_root_mean_squared_error: 0.1691\nEpoch 19/100\n13/13 [==============================] - 3s 193ms/step - loss: 0.1156 - root_mean_squared_error: 0.3400 - val_loss: 0.0276 - val_root_mean_squared_error: 0.1662\nEpoch 20/100\n13/13 [==============================] - 1s 94ms/step - loss: 0.1168 - root_mean_squared_error: 0.3417 - val_loss: 0.0284 - val_root_mean_squared_error: 0.1686\nEpoch 21/100\n13/13 [==============================] - 1s 100ms/step - loss: 0.1122 - root_mean_squared_error: 0.3350 - val_loss: 0.0279 - val_root_mean_squared_error: 0.1671\nEpoch 22/100\n13/13 [==============================] - 2s 182ms/step - loss: 0.1031 - root_mean_squared_error: 0.3210 - val_loss: 0.0276 - val_root_mean_squared_error: 0.1662\nEpoch 23/100\n13/13 [==============================] - 2s 180ms/step - loss: 0.0974 - root_mean_squared_error: 0.3121 - val_loss: 0.0269 - val_root_mean_squared_error: 0.1641\nEpoch 24/100\n13/13 [==============================] - 1s 111ms/step - loss: 0.1020 - root_mean_squared_error: 0.3194 - val_loss: 0.0257 - val_root_mean_squared_error: 0.1602\nEpoch 25/100\n13/13 [==============================] - 1s 109ms/step - loss: 0.0912 - root_mean_squared_error: 0.3020 - val_loss: 0.0295 - val_root_mean_squared_error: 0.1719\nEpoch 26/100\n13/13 [==============================] - 2s 160ms/step - loss: 0.0877 - root_mean_squared_error: 0.2962 - val_loss: 0.0258 - val_root_mean_squared_error: 0.1607\nEpoch 27/100\n13/13 [==============================] - 2s 159ms/step - loss: 0.0694 - root_mean_squared_error: 0.2634 - val_loss: 0.0230 - val_root_mean_squared_error: 0.1518\nEpoch 28/100\n13/13 [==============================] - 1s 104ms/step - loss: 0.0592 - root_mean_squared_error: 0.2434 - val_loss: 0.0236 - val_root_mean_squared_error: 0.1537\nEpoch 29/100\n13/13 [==============================] - 1s 99ms/step - loss: 0.0595 - root_mean_squared_error: 0.2440 - val_loss: 0.0231 - val_root_mean_squared_error: 0.1520\nEpoch 30/100\n13/13 [==============================] - 1s 109ms/step - loss: 0.0428 - root_mean_squared_error: 0.2068 - val_loss: 0.0236 - val_root_mean_squared_error: 0.1536\nEpoch 31/100\n13/13 [==============================] - 1s 115ms/step - loss: 0.0408 - root_mean_squared_error: 0.2020 - val_loss: 0.0249 - val_root_mean_squared_error: 0.1578\nEpoch 32/100\n11/13 [========================>.....] - ETA: 0s - loss: 0.0363 - root_mean_squared_error: 0.1906"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Model 3 Evaluation",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "cell_id": "00030-ec923094-0be9-40e4-9736-5ca824755d2e",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "cell_id": "00031-9913ce7b-1d5e-4fa7-8a81-3d66d64eca57",
    "deepnote_cell_type": "code"
   },
   "source": "m3_hyper_parameters = {\n    \"random_seed\": RANDOM_SEED,\n    \"test_size\": test_size,\n    \"validation_split\": 0.1,\n    \"learning_rate\": 0.01,\n    \"batch_size\": 64,\n    \"epochs\": 200,\n    \"lstm_units\": 200,\n    \"2nd_lstm_units\": 100,\n    \"lstm_activation\": \"sigmoid\",\n    \"dense_units\": 81,\n    \"dense_activation\": \"tanh\",\n    \"dropout\": 0.2,\n    \"output_activation\": \"linear\",\n    \"nb_features\": nb_features,\n    \"optimizer\": \"Adam\"\n}\n\nm3_optimizer = {\n    \"RMSprop\": RMSprop(learning_rate=m3_hyper_parameters[\"learning_rate\"]),\n    \"Adam\": Adam(learning_rate=m3_hyper_parameters[\"learning_rate\"]),\n    \"Adamax\": Adamax(learning_rate=m3_hyper_parameters[\"learning_rate\"]),\n    \"Adagrad\": Adagrad(learning_rate=m3_hyper_parameters[\"learning_rate\"])\n}\n\nmodel3 = Sequential()\nmodel3.add(LSTM(m3_hyper_parameters[\"lstm_units\"], activation=m3_hyper_parameters[\"lstm_activation\"], input_shape=(7, nb_features)))\nmodel3.add(RepeatVector(1))\nmodel3.add(LSTM(m3_hyper_parameters[\"2nd_lstm_units\"], activation=m3_hyper_parameters[\"lstm_activation\"], return_sequences=True))\nmodel3.add(TimeDistributed(Dense(m3_hyper_parameters[\"dense_units\"], activation=m3_hyper_parameters[\"dense_activation\"])))\nmodel3.add(Dropout(m3_hyper_parameters[\"dropout\"]))\nmodel3.add(Dense(1, activation=m3_hyper_parameters[\"output_activation\"]))\nmodel3.summary()",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Model: \"sequential_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n lstm_2 (LSTM)               (None, 200)               225600    \n                                                                 \n repeat_vector_1 (RepeatVect  (None, 1, 200)           0         \n or)                                                             \n                                                                 \n lstm_3 (LSTM)               (None, 1, 100)            120400    \n                                                                 \n time_distributed_1 (TimeDis  (None, 1, 81)            8181      \n tributed)                                                       \n                                                                 \n dropout_1 (Dropout)         (None, 1, 81)             0         \n                                                                 \n dense_3 (Dense)             (None, 1, 1)              82        \n                                                                 \n=================================================================\nTotal params: 354,263\nTrainable params: 354,263\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "cell_id": "00032-952cf30e-4b57-44ec-a9fd-e4615937b6af",
    "deepnote_cell_type": "code"
   },
   "source": "run = neptune.init(\n    project=\"milestone2-california-water-shortage/deeplearning-lstm\",\n    api_token=neptune_key,\n    name=\"Advanced Model 4\",\n    tags=[\"WithDetailedWellCounts\", \"OneUnidirectionalLSTM\", \"OneRepeatVector\", \"OneUnidirectionalLSTM\", \"TimeDistributedDense\", \"Dropout\"]\n)\nneptune_cbk = NeptuneCallback(run=run, base_namespace='metrics')\nrun['hyper-parameters'] = m3_hyper_parameters\n\nmodel3.compile(loss=\"mse\", optimizer=m3_optimizer[m3_hyper_parameters[\"optimizer\"]], metrics=[keras.metrics.RootMeanSquaredError()])\nmodel3.fit(X_train, y_train_3d,\n                     validation_split=m3_hyper_parameters[\"validation_split\"],\n                     batch_size=m3_hyper_parameters[\"batch_size\"],\n                     epochs=m3_hyper_parameters[\"epochs\"],\n                     shuffle=False,\n                     callbacks=[neptune_cbk])\nyhat = model3.predict(X_test, verbose=0)\nyhat_inverse = scaler.inverse_transform(yhat.squeeze(2))\ny_test_inverse = scaler.inverse_transform(y_test)\nevaluate_forecast(y_test_inverse, yhat_inverse)\nmae, mse, rmse = evaluate_forecast(y_test_inverse, yhat_inverse)\nrun[\"eval/mae\"] = mae\nrun[\"eval/mse\"] = mse\nrun[\"eval/rmse\"] = rmse\nrun.stop()",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "https://app.neptune.ai/milestone2-california-water-shortage/deeplearning-lstm/e/DEEPLSTM-158\n"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "Info (NVML): Not Supported. GPU usage metrics may not be reported. For more information, see https://docs-legacy.neptune.ai/logging-and-managing-experiment-results/logging-experiment-data.html#hardware-consumption \n"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Remember to stop your run once you’ve finished logging your metadata (https://docs.neptune.ai/api-reference/run#.stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.\nEpoch 1/200\n6/6 [==============================] - 3s 129ms/step - loss: 4.5125 - root_mean_squared_error: 2.1243 - val_loss: 1.7245 - val_root_mean_squared_error: 1.3132\nEpoch 2/200\n6/6 [==============================] - 1s 93ms/step - loss: 0.7826 - root_mean_squared_error: 0.8847 - val_loss: 0.7797 - val_root_mean_squared_error: 0.8830\nEpoch 3/200\n6/6 [==============================] - 0s 68ms/step - loss: 0.5239 - root_mean_squared_error: 0.7238 - val_loss: 0.1825 - val_root_mean_squared_error: 0.4272\nEpoch 4/200\n6/6 [==============================] - 0s 69ms/step - loss: 0.0561 - root_mean_squared_error: 0.2368 - val_loss: 0.0279 - val_root_mean_squared_error: 0.1669\nEpoch 5/200\n6/6 [==============================] - 0s 65ms/step - loss: 0.0766 - root_mean_squared_error: 0.2767 - val_loss: 0.0405 - val_root_mean_squared_error: 0.2011\nEpoch 6/200\n6/6 [==============================] - 0s 70ms/step - loss: 0.0762 - root_mean_squared_error: 0.2761 - val_loss: 0.0242 - val_root_mean_squared_error: 0.1556\nEpoch 7/200\n6/6 [==============================] - 0s 65ms/step - loss: 0.0505 - root_mean_squared_error: 0.2246 - val_loss: 0.0303 - val_root_mean_squared_error: 0.1742\nEpoch 8/200\n6/6 [==============================] - 0s 65ms/step - loss: 0.0462 - root_mean_squared_error: 0.2148 - val_loss: 0.0378 - val_root_mean_squared_error: 0.1944\nEpoch 9/200\n6/6 [==============================] - 0s 70ms/step - loss: 0.0461 - root_mean_squared_error: 0.2148 - val_loss: 0.0371 - val_root_mean_squared_error: 0.1926\nEpoch 10/200\n6/6 [==============================] - 0s 68ms/step - loss: 0.0425 - root_mean_squared_error: 0.2061 - val_loss: 0.0321 - val_root_mean_squared_error: 0.1792\nEpoch 11/200\n6/6 [==============================] - 0s 67ms/step - loss: 0.0417 - root_mean_squared_error: 0.2042 - val_loss: 0.0280 - val_root_mean_squared_error: 0.1674\nEpoch 12/200\n6/6 [==============================] - 0s 66ms/step - loss: 0.0418 - root_mean_squared_error: 0.2044 - val_loss: 0.0262 - val_root_mean_squared_error: 0.1619\nEpoch 13/200\n6/6 [==============================] - 0s 67ms/step - loss: 0.0409 - root_mean_squared_error: 0.2023 - val_loss: 0.0257 - val_root_mean_squared_error: 0.1605\nEpoch 14/200\n6/6 [==============================] - 0s 66ms/step - loss: 0.0429 - root_mean_squared_error: 0.2072 - val_loss: 0.0261 - val_root_mean_squared_error: 0.1615\nEpoch 15/200\n6/6 [==============================] - 0s 66ms/step - loss: 0.0412 - root_mean_squared_error: 0.2031 - val_loss: 0.0269 - val_root_mean_squared_error: 0.1641\nEpoch 16/200\n6/6 [==============================] - 0s 68ms/step - loss: 0.0404 - root_mean_squared_error: 0.2010 - val_loss: 0.0274 - val_root_mean_squared_error: 0.1655\nEpoch 17/200\n6/6 [==============================] - 0s 67ms/step - loss: 0.0405 - root_mean_squared_error: 0.2012 - val_loss: 0.0273 - val_root_mean_squared_error: 0.1651\nEpoch 18/200\n6/6 [==============================] - 0s 72ms/step - loss: 0.0405 - root_mean_squared_error: 0.2012 - val_loss: 0.0269 - val_root_mean_squared_error: 0.1640\nEpoch 19/200\n6/6 [==============================] - 1s 107ms/step - loss: 0.0395 - root_mean_squared_error: 0.1987 - val_loss: 0.0265 - val_root_mean_squared_error: 0.1629\nEpoch 20/200\n6/6 [==============================] - 0s 78ms/step - loss: 0.0390 - root_mean_squared_error: 0.1976 - val_loss: 0.0261 - val_root_mean_squared_error: 0.1617\nEpoch 21/200\n6/6 [==============================] - 0s 66ms/step - loss: 0.0390 - root_mean_squared_error: 0.1974 - val_loss: 0.0260 - val_root_mean_squared_error: 0.1612\nEpoch 22/200\n6/6 [==============================] - 0s 71ms/step - loss: 0.0369 - root_mean_squared_error: 0.1920 - val_loss: 0.0257 - val_root_mean_squared_error: 0.1603\nEpoch 23/200\n6/6 [==============================] - 0s 67ms/step - loss: 0.0363 - root_mean_squared_error: 0.1904 - val_loss: 0.0252 - val_root_mean_squared_error: 0.1587\nEpoch 24/200\n6/6 [==============================] - 1s 91ms/step - loss: 0.0360 - root_mean_squared_error: 0.1897 - val_loss: 0.0249 - val_root_mean_squared_error: 0.1578\nEpoch 25/200\n6/6 [==============================] - 1s 92ms/step - loss: 0.0342 - root_mean_squared_error: 0.1848 - val_loss: 0.0246 - val_root_mean_squared_error: 0.1569\nEpoch 26/200\n6/6 [==============================] - 0s 66ms/step - loss: 0.0336 - root_mean_squared_error: 0.1832 - val_loss: 0.0242 - val_root_mean_squared_error: 0.1557\nEpoch 27/200\n6/6 [==============================] - 0s 67ms/step - loss: 0.0316 - root_mean_squared_error: 0.1779 - val_loss: 0.0236 - val_root_mean_squared_error: 0.1538\nEpoch 28/200\n6/6 [==============================] - 0s 66ms/step - loss: 0.0304 - root_mean_squared_error: 0.1744 - val_loss: 0.0231 - val_root_mean_squared_error: 0.1520\nEpoch 29/200\n6/6 [==============================] - 0s 67ms/step - loss: 0.0280 - root_mean_squared_error: 0.1672 - val_loss: 0.0225 - val_root_mean_squared_error: 0.1499\nEpoch 30/200\n6/6 [==============================] - 0s 68ms/step - loss: 0.0269 - root_mean_squared_error: 0.1641 - val_loss: 0.0219 - val_root_mean_squared_error: 0.1480\nEpoch 31/200\n6/6 [==============================] - 0s 66ms/step - loss: 0.0251 - root_mean_squared_error: 0.1585 - val_loss: 0.0215 - val_root_mean_squared_error: 0.1465\nEpoch 32/200\n6/6 [==============================] - 0s 68ms/step - loss: 0.0229 - root_mean_squared_error: 0.1515 - val_loss: 0.0208 - val_root_mean_squared_error: 0.1443\nEpoch 33/200\n6/6 [==============================] - 0s 68ms/step - loss: 0.0207 - root_mean_squared_error: 0.1439 - val_loss: 0.0202 - val_root_mean_squared_error: 0.1422\nEpoch 34/200\n6/6 [==============================] - 0s 67ms/step - loss: 0.0193 - root_mean_squared_error: 0.1389 - val_loss: 0.0196 - val_root_mean_squared_error: 0.1400\nEpoch 35/200\n6/6 [==============================] - 0s 69ms/step - loss: 0.0176 - root_mean_squared_error: 0.1327 - val_loss: 0.0190 - val_root_mean_squared_error: 0.1379\nEpoch 36/200\n"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [19]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m run[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhyper-parameters\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m m3_hyper_parameters\n\u001b[0;32m     10\u001b[0m model3\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m\"\u001b[39m, optimizer\u001b[38;5;241m=\u001b[39mm3_optimizer[m3_hyper_parameters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizer\u001b[39m\u001b[38;5;124m\"\u001b[39m]], metrics\u001b[38;5;241m=\u001b[39m[keras\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mRootMeanSquaredError()])\n\u001b[1;32m---> 11\u001b[0m \u001b[43mmodel3\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_3d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mm3_hyper_parameters\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalidation_split\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mm3_hyper_parameters\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbatch_size\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mm3_hyper_parameters\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepochs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mneptune_cbk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m yhat \u001b[38;5;241m=\u001b[39m model3\u001b[38;5;241m.\u001b[39mpredict(X_test, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     18\u001b[0m yhat_inverse \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39minverse_transform(yhat\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m2\u001b[39m))\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\milestone2\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\milestone2\\lib\\site-packages\\keras\\engine\\training.py:1409\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1402\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1403\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   1404\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   1405\u001b[0m     step_num\u001b[38;5;241m=\u001b[39mstep,\n\u001b[0;32m   1406\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   1407\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m   1408\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1409\u001b[0m   tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1410\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1411\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\milestone2\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\milestone2\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\milestone2\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateless_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\milestone2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2453\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2450\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2451\u001b[0m   (graph_function,\n\u001b[0;32m   2452\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2453\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2454\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\milestone2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1860\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1856\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1857\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1858\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1859\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1860\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1861\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1862\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1863\u001b[0m     args,\n\u001b[0;32m   1864\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1865\u001b[0m     executing_eagerly)\n\u001b[0;32m   1866\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\milestone2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:497\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    495\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    496\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 497\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    503\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    504\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    505\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    506\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    509\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    510\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\milestone2\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "cell_id": "00033-0808a7b0-d158-4166-8b9e-b05b6a34a60a",
    "deepnote_cell_type": "code"
   },
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=b042e2da-6536-449d-95b8-d85fa08825de' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
   "metadata": {
    "tags": [],
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown"
   }
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "deepnote_notebook_id": "99d2f4f1-5d87-4f76-b2d0-8eab3dc4868a",
  "deepnote": {},
  "deepnote_execution_queue": []
 }
}