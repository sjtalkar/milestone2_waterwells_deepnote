{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Hyperpamaters Tuning"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, TimeDistributed, RepeatVector\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import keras_tuner\n",
    "\n",
    "from lib.read_data import read_and_join_output_file\n",
    "#from lib.create_pipeline import create_transformation_pipeline\n",
    "from lib.deeplearning import create_transformation_pipelines, evaluate_forecast, reshape_data_to_3d, get_sets_shapes\n",
    "from lib.transform_impute import convert_back_df\n",
    "from lib.split_data import train_test_group_time_split"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "RANDOM_SEED = 31\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tensorflow.random.set_seed(RANDOM_SEED)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tensorflow.config.list_physical_devices('GPU')))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preparing the Dataset\n",
    "The train and test sets are split by Township-Ranges, i.e. some Township-Ranges data are either fully in the train or test set.\n",
    "The target value is the value of that variable for 2021\n",
    "Thus train/test sets are of shape (number of Township-Ranges, 7 years (2014-2020), the number of features).\n",
    "The input of 1 data point in the model is of shape (7x81\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "                     TOTALDRILLDEPTH_AVG  WELLYIELD_AVG  STATICWATERLEVEL_AVG  \\\nTOWNSHIP_RANGE YEAR                                                             \nT01N R02E      2014             0.000000       0.000489              0.020868   \n               2015             0.000000       0.000000              0.000000   \n               2016             0.000000       0.003259              0.036728   \n               2017             0.066667       0.006410              0.025876   \n               2018             0.053651       0.000652              0.063022   \n...                                  ...            ...                   ...   \nT32S R30E      2016             0.000000       0.000000              0.000000   \n               2017             0.000000       0.000000              0.000000   \n               2018             0.000000       0.000000              0.000000   \n               2019             0.000000       0.000000              0.000000   \n               2020             0.000000       0.000000              0.000000   \n\n                     TOPOFPERFORATEDINTERVAL_AVG  \\\nTOWNSHIP_RANGE YEAR                                \nT01N R02E      2014                     0.052288   \n               2015                     0.000000   \n               2016                     0.084967   \n               2017                     0.082789   \n               2018                     0.077124   \n...                                          ...   \nT32S R30E      2016                     0.000000   \n               2017                     0.000000   \n               2018                     0.000000   \n               2019                     0.000000   \n               2020                     0.000000   \n\n                     BOTTOMOFPERFORATEDINTERVAL_AVG  TOTALCOMPLETEDDEPTH_AVG  \\\nTOWNSHIP_RANGE YEAR                                                            \nT01N R02E      2014                        0.076699                 0.127841   \n               2015                        0.000000                 0.000000   \n               2016                        0.058252                 0.056818   \n               2017                        0.064725                 0.074495   \n               2018                        0.053592                 0.064015   \n...                                             ...                      ...   \nT32S R30E      2016                        0.000000                 0.000000   \n               2017                        0.000000                 0.000000   \n               2018                        0.000000                 0.000000   \n               2019                        0.000000                 0.000000   \n               2020                        0.000000                 0.000000   \n\n                     VEGETATION_BLUE_OAK-GRAY_PINE  \\\nTOWNSHIP_RANGE YEAR                                  \nT01N R02E      2014                       0.010798   \n               2015                       0.010798   \n               2016                       0.010798   \n               2017                       0.010798   \n               2018                       0.010798   \n...                                            ...   \nT32S R30E      2016                       0.033178   \n               2017                       0.033178   \n               2018                       0.033178   \n               2019                       0.033178   \n               2020                       0.033178   \n\n                     VEGETATION_CALIFORNIA_COAST_LIVE_OAK  \\\nTOWNSHIP_RANGE YEAR                                         \nT01N R02E      2014                              0.002749   \n               2015                              0.002749   \n               2016                              0.002749   \n               2017                              0.002749   \n               2018                              0.002749   \n...                                                   ...   \nT32S R30E      2016                              0.000000   \n               2017                              0.000000   \n               2018                              0.000000   \n               2019                              0.000000   \n               2020                              0.000000   \n\n                     VEGETATION_CANYON_LIVE_OAK  VEGETATION_HARD_CHAPARRAL  \\\nTOWNSHIP_RANGE YEAR                                                          \nT01N R02E      2014                    0.000000                   0.000633   \n               2015                    0.000000                   0.000633   \n               2016                    0.000000                   0.000633   \n               2017                    0.000000                   0.000633   \n               2018                    0.000000                   0.000633   \n...                                         ...                        ...   \nT32S R30E      2016                    0.002023                   0.003535   \n               2017                    0.002023                   0.003535   \n               2018                    0.002023                   0.003535   \n               2019                    0.002023                   0.003535   \n               2020                    0.002023                   0.003535   \n\n                     ...  POPULATION_DENSITY  PCT_OF_CAPACITY  \\\nTOWNSHIP_RANGE YEAR  ...                                        \nT01N R02E      2014  ...            0.391791         0.776467   \n               2015  ...            0.394044         0.776467   \n               2016  ...            0.395968         0.776467   \n               2017  ...            0.406050         0.776467   \n               2018  ...            0.405447         0.776467   \n...                  ...                 ...              ...   \nT32S R30E      2016  ...            0.004489         0.496289   \n               2017  ...            0.004477         0.496289   \n               2018  ...            0.004494         0.496289   \n               2019  ...            0.004511         0.580893   \n               2020  ...            0.004533         0.499980   \n\n                     GROUNDSURFACEELEVATION_AVG  AVERAGE_YEARLY_PRECIPITATION  \\\nTOWNSHIP_RANGE YEAR                                                             \nT01N R02E      2014                    0.043092                      0.286941   \n               2015                    0.037376                      0.301232   \n               2016                    0.016622                      0.357881   \n               2017                    0.031660                      0.689154   \n               2018                    0.051869                      0.252603   \n...                                         ...                           ...   \nT32S R30E      2016                    0.058099                      0.118655   \n               2017                    0.058099                      0.180043   \n               2018                    0.058099                      0.084816   \n               2019                    0.058099                      0.168764   \n               2020                    0.058099                      0.166161   \n\n                     SHORTAGE_COUNT   GSE_GWE  WELL_COUNT_AGRICULTURE  \\\nTOWNSHIP_RANGE YEAR                                                     \nT01N R02E      2014             0.0  0.083371                0.021277   \n               2015             0.0  0.081869                0.000000   \n               2016             0.0  0.071257                0.000000   \n               2017             0.0  0.070044                0.000000   \n               2018             0.0  0.067062                0.021277   \n...                             ...       ...                     ...   \nT32S R30E      2016             0.0  0.667084                0.000000   \n               2017             0.0  0.566686                0.000000   \n               2018             0.0  0.597051                0.000000   \n               2019             0.0  0.608404                0.000000   \n               2020             0.0  0.595798                0.000000   \n\n                     WELL_COUNT_DOMESTIC  WELL_COUNT_INDUSTRIAL  \\\nTOWNSHIP_RANGE YEAR                                               \nT01N R02E      2014             0.013889                    0.0   \n               2015             0.000000                    0.0   \n               2016             0.013889                    0.0   \n               2017             0.041667                    0.0   \n               2018             0.013889                    0.0   \n...                                  ...                    ...   \nT32S R30E      2016             0.000000                    0.0   \n               2017             0.000000                    0.0   \n               2018             0.000000                    0.0   \n               2019             0.000000                    0.0   \n               2020             0.000000                    0.0   \n\n                     WELL_COUNT_PUBLIC  \nTOWNSHIP_RANGE YEAR                     \nT01N R02E      2014                0.0  \n               2015                0.0  \n               2016                0.0  \n               2017                0.0  \n               2018                0.0  \n...                                ...  \nT32S R30E      2016                0.0  \n               2017                0.0  \n               2018                0.0  \n               2019                0.0  \n               2020                0.0  \n\n[2842 rows x 81 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th>TOTALDRILLDEPTH_AVG</th>\n      <th>WELLYIELD_AVG</th>\n      <th>STATICWATERLEVEL_AVG</th>\n      <th>TOPOFPERFORATEDINTERVAL_AVG</th>\n      <th>BOTTOMOFPERFORATEDINTERVAL_AVG</th>\n      <th>TOTALCOMPLETEDDEPTH_AVG</th>\n      <th>VEGETATION_BLUE_OAK-GRAY_PINE</th>\n      <th>VEGETATION_CALIFORNIA_COAST_LIVE_OAK</th>\n      <th>VEGETATION_CANYON_LIVE_OAK</th>\n      <th>VEGETATION_HARD_CHAPARRAL</th>\n      <th>...</th>\n      <th>POPULATION_DENSITY</th>\n      <th>PCT_OF_CAPACITY</th>\n      <th>GROUNDSURFACEELEVATION_AVG</th>\n      <th>AVERAGE_YEARLY_PRECIPITATION</th>\n      <th>SHORTAGE_COUNT</th>\n      <th>GSE_GWE</th>\n      <th>WELL_COUNT_AGRICULTURE</th>\n      <th>WELL_COUNT_DOMESTIC</th>\n      <th>WELL_COUNT_INDUSTRIAL</th>\n      <th>WELL_COUNT_PUBLIC</th>\n    </tr>\n    <tr>\n      <th>TOWNSHIP_RANGE</th>\n      <th>YEAR</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">T01N R02E</th>\n      <th>2014</th>\n      <td>0.000000</td>\n      <td>0.000489</td>\n      <td>0.020868</td>\n      <td>0.052288</td>\n      <td>0.076699</td>\n      <td>0.127841</td>\n      <td>0.010798</td>\n      <td>0.002749</td>\n      <td>0.000000</td>\n      <td>0.000633</td>\n      <td>...</td>\n      <td>0.391791</td>\n      <td>0.776467</td>\n      <td>0.043092</td>\n      <td>0.286941</td>\n      <td>0.0</td>\n      <td>0.083371</td>\n      <td>0.021277</td>\n      <td>0.013889</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2015</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.010798</td>\n      <td>0.002749</td>\n      <td>0.000000</td>\n      <td>0.000633</td>\n      <td>...</td>\n      <td>0.394044</td>\n      <td>0.776467</td>\n      <td>0.037376</td>\n      <td>0.301232</td>\n      <td>0.0</td>\n      <td>0.081869</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2016</th>\n      <td>0.000000</td>\n      <td>0.003259</td>\n      <td>0.036728</td>\n      <td>0.084967</td>\n      <td>0.058252</td>\n      <td>0.056818</td>\n      <td>0.010798</td>\n      <td>0.002749</td>\n      <td>0.000000</td>\n      <td>0.000633</td>\n      <td>...</td>\n      <td>0.395968</td>\n      <td>0.776467</td>\n      <td>0.016622</td>\n      <td>0.357881</td>\n      <td>0.0</td>\n      <td>0.071257</td>\n      <td>0.000000</td>\n      <td>0.013889</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2017</th>\n      <td>0.066667</td>\n      <td>0.006410</td>\n      <td>0.025876</td>\n      <td>0.082789</td>\n      <td>0.064725</td>\n      <td>0.074495</td>\n      <td>0.010798</td>\n      <td>0.002749</td>\n      <td>0.000000</td>\n      <td>0.000633</td>\n      <td>...</td>\n      <td>0.406050</td>\n      <td>0.776467</td>\n      <td>0.031660</td>\n      <td>0.689154</td>\n      <td>0.0</td>\n      <td>0.070044</td>\n      <td>0.000000</td>\n      <td>0.041667</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2018</th>\n      <td>0.053651</td>\n      <td>0.000652</td>\n      <td>0.063022</td>\n      <td>0.077124</td>\n      <td>0.053592</td>\n      <td>0.064015</td>\n      <td>0.010798</td>\n      <td>0.002749</td>\n      <td>0.000000</td>\n      <td>0.000633</td>\n      <td>...</td>\n      <td>0.405447</td>\n      <td>0.776467</td>\n      <td>0.051869</td>\n      <td>0.252603</td>\n      <td>0.0</td>\n      <td>0.067062</td>\n      <td>0.021277</td>\n      <td>0.013889</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">T32S R30E</th>\n      <th>2016</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.033178</td>\n      <td>0.000000</td>\n      <td>0.002023</td>\n      <td>0.003535</td>\n      <td>...</td>\n      <td>0.004489</td>\n      <td>0.496289</td>\n      <td>0.058099</td>\n      <td>0.118655</td>\n      <td>0.0</td>\n      <td>0.667084</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2017</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.033178</td>\n      <td>0.000000</td>\n      <td>0.002023</td>\n      <td>0.003535</td>\n      <td>...</td>\n      <td>0.004477</td>\n      <td>0.496289</td>\n      <td>0.058099</td>\n      <td>0.180043</td>\n      <td>0.0</td>\n      <td>0.566686</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2018</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.033178</td>\n      <td>0.000000</td>\n      <td>0.002023</td>\n      <td>0.003535</td>\n      <td>...</td>\n      <td>0.004494</td>\n      <td>0.496289</td>\n      <td>0.058099</td>\n      <td>0.084816</td>\n      <td>0.0</td>\n      <td>0.597051</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2019</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.033178</td>\n      <td>0.000000</td>\n      <td>0.002023</td>\n      <td>0.003535</td>\n      <td>...</td>\n      <td>0.004511</td>\n      <td>0.580893</td>\n      <td>0.058099</td>\n      <td>0.168764</td>\n      <td>0.0</td>\n      <td>0.608404</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2020</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.033178</td>\n      <td>0.000000</td>\n      <td>0.002023</td>\n      <td>0.003535</td>\n      <td>...</td>\n      <td>0.004533</td>\n      <td>0.499980</td>\n      <td>0.058099</td>\n      <td>0.166161</td>\n      <td>0.0</td>\n      <td>0.595798</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>2842 rows Ã— 81 columns</p>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_size=0.15\n",
    "# Load the data from the ETL output files\n",
    "X = read_and_join_output_file()\n",
    "# Split the data into a training and a test set\n",
    "X_train_df, X_test_df, y_train_df, y_test_df = train_test_group_time_split(X, index=[\"TOWNSHIP_RANGE\", \"YEAR\"], group=\"TOWNSHIP_RANGE\", test_size=test_size, random_seed=RANDOM_SEED)\n",
    "# Create, fit and apply the data imputation pipeline to the training and test sets\n",
    "impute_pipeline, columns = create_transformation_pipelines(X_train_df)\n",
    "X_train_impute = impute_pipeline.fit_transform(X_train_df)\n",
    "X_test_impute = impute_pipeline.transform(X_test_df)\n",
    "# Convert the X_train and X_test back to dataframes\n",
    "X_train_impute_df = pd.DataFrame(X_train_impute, index=X_train_df.index, columns=columns)\n",
    "X_test_impute_df = pd.DataFrame(X_test_impute, index=X_test_df.index, columns=columns)\n",
    "# Keep only the GSE_GWE variable as the outcome variable\n",
    "scaler = MinMaxScaler()\n",
    "y_train = scaler.fit_transform(y_train_df[[\"GSE_GWE\"]])\n",
    "y_train_3d = y_train[..., np.newaxis]\n",
    "y_test = scaler.transform(y_test_df[[\"GSE_GWE\"]])\n",
    "nb_features = len(X_train_impute_df.columns)\n",
    "X_train_impute_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "                  nb_items  nb_timestamps  nb_features\ntraining dataset       406              7           81\ntest dataset            72              7           81",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>nb_items</th>\n      <th>nb_timestamps</th>\n      <th>nb_features</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>training dataset</th>\n      <td>406</td>\n      <td>7</td>\n      <td>81</td>\n    </tr>\n    <tr>\n      <th>test dataset</th>\n      <td>72</td>\n      <td>7</td>\n      <td>81</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test = reshape_data_to_3d([X_train_impute_df, X_test_impute_df])\n",
    "get_sets_shapes(X_train, X_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Simple Model Hyper-parameter Tuning\n",
    "This model is just made of a single LSTM layer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class Model1(keras_tuner.HyperModel):\n",
    "    def build(self, hp):\n",
    "        model = Sequential()\n",
    "        hp_units = hp.Int(\"units\", min_value=10, max_value=300, step=10)\n",
    "        hp_activ = hp.Choice(\"activation\", values=[\"tanh\", \"sigmoid\"])\n",
    "        model.add(LSTM(units=hp_units, activation=hp_activ, input_shape=(7, nb_features)))\n",
    "        model.add(Dense(1, activation=\"linear\"))\n",
    "        hp_learning_rate = hp.Choice(\"learning_rate\", values=[1e-2, 1e-3, 1e-4])\n",
    "        model.compile(loss=\"mse\", optimizer=Adam(learning_rate=hp_learning_rate), metrics=[keras.metrics.RootMeanSquaredError()])\n",
    "        return model\n",
    "\n",
    "    def fit(self, hp, model, *args, **kwargs):\n",
    "        return model.fit(\n",
    "            *args,\n",
    "            validation_split=hp.Choice(\"validation_split\", values=[0.05, 0.1, 0.15, 0.2]),\n",
    "            batch_size=hp.Int(\"batch_size\", min_value=32, max_value=192, step=16),\n",
    "            epochs=hp.Int(\"epochs\", min_value=30, max_value=500, step=5),\n",
    "            shuffle=True,\n",
    "            **kwargs,\n",
    "        )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 250 Complete [00h 00m 05s]\n",
      "val_root_mean_squared_error: 0.09763942658901215\n",
      "\n",
      "Best val_root_mean_squared_error So Far: 0.0824510008096695\n",
      "Total elapsed time: 01h 05m 47s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "stop_early = tensorflow.keras.callbacks.EarlyStopping(monitor='val_root_mean_squared_error', patience=10, verbose=1)\n",
    "tuner = keras_tuner.BayesianOptimization(Model1(),\n",
    "                             objective=keras_tuner.Objective(\"val_root_mean_squared_error\", direction=\"min\"),\n",
    "                             max_trials=250,\n",
    "                             beta=3.2,\n",
    "                             seed=RANDOM_SEED,\n",
    "                             overwrite=True,\n",
    "                             directory=\"keras_tuner\",\n",
    "                             project_name=\"model1_tuner\")\n",
    "tuner.search(X_train, y_train, callbacks=[stop_early])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Best Model Hyperparameters"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The hyperparameter search is complete.\n",
      "validation_split: 0.15\n",
      "lstm_units: 60\n",
      "lstm_activation: tanh\n",
      "learning_rate: 0.01\n",
      "batch_size: 112\n",
      "epochs: 30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the optimal hyperparameters\n",
    "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(f\"\"\"\n",
    "The hyperparameter search is complete.\n",
    "validation_split: {best_hps.get('validation_split')}\n",
    "lstm_units: {best_hps.get('units')}\n",
    "lstm_activation: {best_hps.get('activation')}\n",
    "learning_rate: {best_hps.get('learning_rate')}\n",
    "batch_size: {best_hps.get('batch_size')}\n",
    "epochs: {best_hps.get('epochs')}\n",
    "\"\"\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Hyperparameters Tuning Summary"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results summary\n",
      "Results in keras_tuner\\model1_tuner\n",
      "Showing 10 best trials\n",
      "<keras_tuner.engine.objective.Objective object at 0x00000220D05A6130>\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 60\n",
      "activation: tanh\n",
      "learning_rate: 0.01\n",
      "validation_split: 0.15\n",
      "batch_size: 112\n",
      "epochs: 30\n",
      "Score: 0.0824510008096695\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 60\n",
      "activation: tanh\n",
      "learning_rate: 0.01\n",
      "validation_split: 0.15\n",
      "batch_size: 112\n",
      "epochs: 30\n",
      "Score: 0.08390998095273972\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 60\n",
      "activation: tanh\n",
      "learning_rate: 0.01\n",
      "validation_split: 0.15\n",
      "batch_size: 112\n",
      "epochs: 30\n",
      "Score: 0.08468343317508698\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 60\n",
      "activation: tanh\n",
      "learning_rate: 0.01\n",
      "validation_split: 0.15\n",
      "batch_size: 112\n",
      "epochs: 30\n",
      "Score: 0.08546236157417297\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 60\n",
      "activation: tanh\n",
      "learning_rate: 0.01\n",
      "validation_split: 0.15\n",
      "batch_size: 112\n",
      "epochs: 30\n",
      "Score: 0.08679324388504028\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 60\n",
      "activation: tanh\n",
      "learning_rate: 0.01\n",
      "validation_split: 0.15\n",
      "batch_size: 112\n",
      "epochs: 30\n",
      "Score: 0.08681566268205643\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 60\n",
      "activation: tanh\n",
      "learning_rate: 0.01\n",
      "validation_split: 0.15\n",
      "batch_size: 112\n",
      "epochs: 30\n",
      "Score: 0.08719680458307266\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 60\n",
      "activation: tanh\n",
      "learning_rate: 0.01\n",
      "validation_split: 0.15\n",
      "batch_size: 112\n",
      "epochs: 30\n",
      "Score: 0.0871998742222786\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 60\n",
      "activation: tanh\n",
      "learning_rate: 0.01\n",
      "validation_split: 0.15\n",
      "batch_size: 112\n",
      "epochs: 30\n",
      "Score: 0.08782270550727844\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 60\n",
      "activation: tanh\n",
      "learning_rate: 0.01\n",
      "validation_split: 0.15\n",
      "batch_size: 112\n",
      "epochs: 30\n",
      "Score: 0.0879221186041832\n"
     ]
    }
   ],
   "source": [
    "tuner.results_summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model2 Hyper-parameter tuning\n",
    "This model is made of\n",
    "* a simple or bidirectional LSTM layer\n",
    "* a Dense unit\n",
    "* a Dropout Unit"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "class Model2(keras_tuner.HyperModel):\n",
    "    def build(self, hp):\n",
    "        model = Sequential()\n",
    "        lstm_units = hp.Int(\"lstm_units\", min_value=10, max_value=300, step=10)\n",
    "        lstm_activ = hp.Choice(\"lstm_activation\", values=[\"tanh\", \"sigmoid\"])\n",
    "        model.add(LSTM(units=lstm_units, activation=lstm_activ, input_shape=(7, nb_features)))\n",
    "        dense_units = hp.Int(\"dense_units\", min_value=11, max_value=101, step=2)\n",
    "        dense_activation = hp.Choice(\"dense_activation\", values=[\"relu\", \"tanh\", \"sigmoid\"])\n",
    "        model.add(Dense(dense_units, activation=dense_activation))\n",
    "        hp_dropout = hp.Float(\"dropout_rate\", min_value=0.05, max_value=0.25, step=0.05)\n",
    "        model.add(Dropout(hp_dropout))\n",
    "        model.add(Dense(1, activation=\"linear\"))\n",
    "        hp_learning_rate = hp.Choice(\"learning_rate\", values=[1e-2, 1e-3, 1e-4])\n",
    "        model.compile(loss=\"mse\", optimizer=Adam(learning_rate=hp_learning_rate), metrics=[keras.metrics.RootMeanSquaredError()])\n",
    "        return model\n",
    "\n",
    "    def fit(self, hp, model, *args, **kwargs):\n",
    "        return model.fit(\n",
    "            *args,\n",
    "            validation_split=hp.Choice(\"validation_split\", values=[0.05, 0.1, 0.15, 0.2]),\n",
    "            batch_size=hp.Int(\"batch_size\", min_value=32, max_value=192, step=16),\n",
    "            epochs=hp.Int(\"epochs\", min_value=30, max_value=500, step=5),\n",
    "            shuffle=True,\n",
    "            **kwargs,\n",
    "        )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 400 Complete [00h 00m 06s]\n",
      "val_root_mean_squared_error: 0.08002614974975586\n",
      "\n",
      "Best val_root_mean_squared_error So Far: 0.06468775868415833\n",
      "Total elapsed time: 03h 14m 13s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "stop_early = tensorflow.keras.callbacks.EarlyStopping(monitor=\"val_root_mean_squared_error\", patience=10, verbose=1)\n",
    "tuner = keras_tuner.BayesianOptimization(Model2(),\n",
    "                              objective=keras_tuner.Objective(\"val_root_mean_squared_error\", direction=\"min\"),\n",
    "                              max_trials=400,\n",
    "                              beta=3.2,\n",
    "                              seed=RANDOM_SEED,\n",
    "                              overwrite=True,\n",
    "                              directory=\"keras_tuner\",\n",
    "                              project_name=\"model2_tuner\")\n",
    "tuner.search(X_train, y_train, callbacks=[stop_early])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Best Model Hyperparameters"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The hyperparameter search is complete.\n",
      "validation_split: 0.05\n",
      "lstm_units: 10\n",
      "lstm_activation: sigmoid\n",
      "dense_units: 11\n",
      "dense_activation: relu\n",
      "dropout_rate: 0.05\n",
      "learning_rate: 0.01\n",
      "batch_size: 32\n",
      "epochs: 500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the optimal hyperparameters\n",
    "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(f\"\"\"\n",
    "The hyperparameter search is complete.\n",
    "validation_split: {best_hps.get('validation_split')}\n",
    "lstm_units: {best_hps.get('lstm_units')}\n",
    "lstm_activation: {best_hps.get('lstm_activation')}\n",
    "dense_units: {best_hps.get('dense_units')}\n",
    "dense_activation: {best_hps.get('dense_activation')}\n",
    "dropout_rate: {best_hps.get('dropout_rate')}\n",
    "learning_rate: {best_hps.get('learning_rate')}\n",
    "batch_size: {best_hps.get('batch_size')}\n",
    "epochs: {best_hps.get('epochs')}\n",
    "\"\"\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Hyperparameters Tuning Summary"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results summary\n",
      "Results in keras_tuner\\model2_tuner\n",
      "Showing 10 best trials\n",
      "<keras_tuner.engine.objective.Objective object at 0x00000220D8130B50>\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "lstm_units: 10\n",
      "lstm_activation: sigmoid\n",
      "dense_units: 11\n",
      "dense_activation: relu\n",
      "dropout_rate: 0.05\n",
      "learning_rate: 0.01\n",
      "validation_split: 0.05\n",
      "batch_size: 32\n",
      "epochs: 500\n",
      "Score: 0.06468775868415833\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "lstm_units: 10\n",
      "lstm_activation: sigmoid\n",
      "dense_units: 11\n",
      "dense_activation: relu\n",
      "dropout_rate: 0.1\n",
      "learning_rate: 0.01\n",
      "validation_split: 0.05\n",
      "batch_size: 32\n",
      "epochs: 500\n",
      "Score: 0.06481722742319107\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "lstm_units: 10\n",
      "lstm_activation: sigmoid\n",
      "dense_units: 11\n",
      "dense_activation: relu\n",
      "dropout_rate: 0.15000000000000002\n",
      "learning_rate: 0.01\n",
      "validation_split: 0.05\n",
      "batch_size: 32\n",
      "epochs: 415\n",
      "Score: 0.06549284607172012\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "lstm_units: 10\n",
      "lstm_activation: sigmoid\n",
      "dense_units: 11\n",
      "dense_activation: relu\n",
      "dropout_rate: 0.15000000000000002\n",
      "learning_rate: 0.01\n",
      "validation_split: 0.05\n",
      "batch_size: 32\n",
      "epochs: 415\n",
      "Score: 0.06606409698724747\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "lstm_units: 10\n",
      "lstm_activation: sigmoid\n",
      "dense_units: 57\n",
      "dense_activation: relu\n",
      "dropout_rate: 0.2\n",
      "learning_rate: 0.01\n",
      "validation_split: 0.05\n",
      "batch_size: 32\n",
      "epochs: 145\n",
      "Score: 0.06691084057092667\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "lstm_units: 10\n",
      "lstm_activation: sigmoid\n",
      "dense_units: 11\n",
      "dense_activation: relu\n",
      "dropout_rate: 0.05\n",
      "learning_rate: 0.01\n",
      "validation_split: 0.05\n",
      "batch_size: 32\n",
      "epochs: 500\n",
      "Score: 0.06699465960264206\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "lstm_units: 10\n",
      "lstm_activation: sigmoid\n",
      "dense_units: 11\n",
      "dense_activation: relu\n",
      "dropout_rate: 0.15000000000000002\n",
      "learning_rate: 0.01\n",
      "validation_split: 0.05\n",
      "batch_size: 32\n",
      "epochs: 410\n",
      "Score: 0.06715192645788193\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "lstm_units: 10\n",
      "lstm_activation: sigmoid\n",
      "dense_units: 11\n",
      "dense_activation: relu\n",
      "dropout_rate: 0.05\n",
      "learning_rate: 0.01\n",
      "validation_split: 0.05\n",
      "batch_size: 32\n",
      "epochs: 500\n",
      "Score: 0.06768625229597092\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "lstm_units: 10\n",
      "lstm_activation: sigmoid\n",
      "dense_units: 21\n",
      "dense_activation: relu\n",
      "dropout_rate: 0.05\n",
      "learning_rate: 0.01\n",
      "validation_split: 0.05\n",
      "batch_size: 32\n",
      "epochs: 360\n",
      "Score: 0.06787063926458359\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "lstm_units: 10\n",
      "lstm_activation: sigmoid\n",
      "dense_units: 11\n",
      "dense_activation: relu\n",
      "dropout_rate: 0.05\n",
      "learning_rate: 0.01\n",
      "validation_split: 0.05\n",
      "batch_size: 32\n",
      "epochs: 500\n",
      "Score: 0.06824542582035065\n"
     ]
    }
   ],
   "source": [
    "tuner.results_summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model3 Hyper-parameter tuning\n",
    "* a simple or bidirectional encoding LSTM layer\n",
    "* a simple or bidirectional decoding LSTM layer\n",
    "* a Dense unit\n",
    "* a Dropout Unit"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "class Model3(keras_tuner.HyperModel):\n",
    "    def build(self, hp):\n",
    "        model = Sequential()\n",
    "        lstm_units = hp.Int(\"lstm_units\", min_value=10, max_value=300, step=10)\n",
    "        lstm_activ = hp.Choice(\"lstm_activation\", values=[\"tanh\", \"sigmoid\"])\n",
    "        model.add(LSTM(units=lstm_units, activation=\"sigmoid\", input_shape=(7, nb_features)))\n",
    "        model.add(RepeatVector(1))\n",
    "        lstm_units_2 = hp.Int(\"2nd_lstm_units\", min_value=10, max_value=300, step=10)\n",
    "        lstm_activ_2 = hp.Choice(\"2nd_lstm_activation\", values=[\"tanh\", \"sigmoid\"])\n",
    "        model.add(LSTM(units=lstm_units_2, activation=\"sigmoid\", return_sequences=True))\n",
    "        dense_units = hp.Int(\"dense_units\", min_value=11, max_value=101, step=2)\n",
    "        dense_activation = hp.Choice(\"dense_activation\", values=[\"relu\", \"tanh\", \"sigmoid\"])\n",
    "        model.add(TimeDistributed(Dense(dense_units, activation=dense_activation)))\n",
    "        hp_dropout = hp.Float(\"dropout_rate\", min_value=0.05, max_value=0.25, step=0.05)\n",
    "        model.add(Dropout(hp_dropout))\n",
    "        model.add(Dense(1, activation=\"linear\"))\n",
    "        hp_learning_rate = hp.Choice(\"learning_rate\", values=[1e-2, 1e-3, 1e-4])\n",
    "        model.compile(loss=\"mse\", optimizer=Adam(learning_rate=hp_learning_rate), metrics=[keras.metrics.RootMeanSquaredError()])\n",
    "        return model\n",
    "\n",
    "    def fit(self, hp, model, *args, **kwargs):\n",
    "        return model.fit(\n",
    "            *args,\n",
    "            validation_split=hp.Choice(\"validation_split\", values=[0.05, 0.1, 0.15, 0.2]),\n",
    "            batch_size=hp.Int(\"batch_size\", min_value=32, max_value=192, step=16),\n",
    "            epochs=hp.Int(\"epochs\", min_value=30, max_value=500, step=5),\n",
    "            shuffle=True,\n",
    "            **kwargs,\n",
    "        )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 500 Complete [00h 00m 18s]\n",
      "val_root_mean_squared_error: 0.1150362491607666\n",
      "\n",
      "Best val_root_mean_squared_error So Far: 0.06309118866920471\n",
      "Total elapsed time: 07h 29m 36s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "stop_early = tensorflow.keras.callbacks.EarlyStopping(monitor=\"val_root_mean_squared_error\", patience=10, verbose=1)\n",
    "tuner = keras_tuner.BayesianOptimization(Model3(),\n",
    "                              objective=keras_tuner.Objective(\"val_root_mean_squared_error\", direction=\"min\"),\n",
    "                              max_trials=400,\n",
    "                              beta=3.2,\n",
    "                              seed=RANDOM_SEED,\n",
    "                              overwrite=True,\n",
    "                              directory=\"keras_tuner\",\n",
    "                              project_name=\"model3_tuner\")\n",
    "tuner.search(X_train, y_train, callbacks=[stop_early])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Best Model Hyperparameters"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The hyperparameter search is complete.\n",
      "validation_split: 0.05\n",
      "lstm_units: 300\n",
      "lstm_activation: sigmoid\n",
      "2nd_lstm_units: 300\n",
      "2nd_lstm_activation: tanh\n",
      "dense_units: 11\n",
      "dense_activation: sigmoid\n",
      "dropout_rate: 0.05\n",
      "learning_rate: 0.01\n",
      "batch_size: 32\n",
      "epochs: 30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the optimal hyperparameters\n",
    "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(f\"\"\"\n",
    "The hyperparameter search is complete.\n",
    "validation_split: {best_hps.get('validation_split')}\n",
    "lstm_units: {best_hps.get('lstm_units')}\n",
    "lstm_activation: {best_hps.get('lstm_activation')}\n",
    "2nd_lstm_units: {best_hps.get('2nd_lstm_units')}\n",
    "2nd_lstm_activation: {best_hps.get('2nd_lstm_activation')}\n",
    "dense_units: {best_hps.get('dense_units')}\n",
    "dense_activation: {best_hps.get('dense_activation')}\n",
    "dropout_rate: {best_hps.get('dropout_rate')}\n",
    "learning_rate: {best_hps.get('learning_rate')}\n",
    "batch_size: {best_hps.get('batch_size')}\n",
    "epochs: {best_hps.get('epochs')}\n",
    "\"\"\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Hyperparameters Tuning Summary"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results summary\n",
      "Results in keras_tuner\\model3_tuner\n",
      "Showing 10 best trials\n",
      "<keras_tuner.engine.objective.Objective object at 0x000002575F038250>\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "lstm_units: 300\n",
      "lstm_activation: sigmoid\n",
      "2nd_lstm_units: 300\n",
      "2nd_lstm_activation: tanh\n",
      "dense_units: 11\n",
      "dense_activation: sigmoid\n",
      "dropout_rate: 0.05\n",
      "learning_rate: 0.01\n",
      "validation_split: 0.05\n",
      "batch_size: 32\n",
      "epochs: 30\n",
      "Score: 0.06309118866920471\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "lstm_units: 300\n",
      "lstm_activation: sigmoid\n",
      "2nd_lstm_units: 300\n",
      "2nd_lstm_activation: tanh\n",
      "dense_units: 11\n",
      "dense_activation: sigmoid\n",
      "dropout_rate: 0.05\n",
      "learning_rate: 0.01\n",
      "validation_split: 0.05\n",
      "batch_size: 32\n",
      "epochs: 30\n",
      "Score: 0.06499446928501129\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "lstm_units: 300\n",
      "lstm_activation: sigmoid\n",
      "2nd_lstm_units: 300\n",
      "2nd_lstm_activation: sigmoid\n",
      "dense_units: 11\n",
      "dense_activation: sigmoid\n",
      "dropout_rate: 0.05\n",
      "learning_rate: 0.01\n",
      "validation_split: 0.05\n",
      "batch_size: 32\n",
      "epochs: 30\n",
      "Score: 0.06594016402959824\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "lstm_units: 300\n",
      "lstm_activation: sigmoid\n",
      "2nd_lstm_units: 300\n",
      "2nd_lstm_activation: tanh\n",
      "dense_units: 11\n",
      "dense_activation: sigmoid\n",
      "dropout_rate: 0.05\n",
      "learning_rate: 0.01\n",
      "validation_split: 0.05\n",
      "batch_size: 32\n",
      "epochs: 30\n",
      "Score: 0.06661056727170944\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "lstm_units: 300\n",
      "lstm_activation: sigmoid\n",
      "2nd_lstm_units: 300\n",
      "2nd_lstm_activation: tanh\n",
      "dense_units: 11\n",
      "dense_activation: sigmoid\n",
      "dropout_rate: 0.05\n",
      "learning_rate: 0.01\n",
      "validation_split: 0.05\n",
      "batch_size: 32\n",
      "epochs: 30\n",
      "Score: 0.06901232153177261\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "lstm_units: 300\n",
      "lstm_activation: sigmoid\n",
      "2nd_lstm_units: 300\n",
      "2nd_lstm_activation: tanh\n",
      "dense_units: 11\n",
      "dense_activation: sigmoid\n",
      "dropout_rate: 0.05\n",
      "learning_rate: 0.01\n",
      "validation_split: 0.05\n",
      "batch_size: 32\n",
      "epochs: 30\n",
      "Score: 0.06999509781599045\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "lstm_units: 300\n",
      "lstm_activation: sigmoid\n",
      "2nd_lstm_units: 300\n",
      "2nd_lstm_activation: tanh\n",
      "dense_units: 11\n",
      "dense_activation: sigmoid\n",
      "dropout_rate: 0.05\n",
      "learning_rate: 0.01\n",
      "validation_split: 0.05\n",
      "batch_size: 32\n",
      "epochs: 30\n",
      "Score: 0.07048741728067398\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "lstm_units: 300\n",
      "lstm_activation: tanh\n",
      "2nd_lstm_units: 300\n",
      "2nd_lstm_activation: tanh\n",
      "dense_units: 11\n",
      "dense_activation: sigmoid\n",
      "dropout_rate: 0.05\n",
      "learning_rate: 0.01\n",
      "validation_split: 0.05\n",
      "batch_size: 32\n",
      "epochs: 30\n",
      "Score: 0.07083387672901154\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "lstm_units: 300\n",
      "lstm_activation: tanh\n",
      "2nd_lstm_units: 300\n",
      "2nd_lstm_activation: tanh\n",
      "dense_units: 11\n",
      "dense_activation: sigmoid\n",
      "dropout_rate: 0.05\n",
      "learning_rate: 0.01\n",
      "validation_split: 0.05\n",
      "batch_size: 32\n",
      "epochs: 30\n",
      "Score: 0.07127515226602554\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "lstm_units: 300\n",
      "lstm_activation: sigmoid\n",
      "2nd_lstm_units: 300\n",
      "2nd_lstm_activation: sigmoid\n",
      "dense_units: 11\n",
      "dense_activation: sigmoid\n",
      "dropout_rate: 0.05\n",
      "learning_rate: 0.01\n",
      "validation_split: 0.05\n",
      "batch_size: 32\n",
      "epochs: 30\n",
      "Score: 0.07147232443094254\n"
     ]
    }
   ],
   "source": [
    "tuner.results_summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}