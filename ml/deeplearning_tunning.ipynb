{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import neptune.new as neptune\n",
    "from neptune.new.integrations.tensorflow_keras import NeptuneCallback\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, TimeDistributed, Conv1D, MaxPooling1D, Flatten, Bidirectional, Input, Flatten, Activation, Reshape, RepeatVector, Concatenate\n",
    "from keras.optimizers import RMSprop, Adam, Adamax, Adagrad\n",
    "\n",
    "import keras_tuner\n",
    "\n",
    "from lib.read_data import read_and_join_output_file\n",
    "#from lib.create_pipeline import create_transformation_pipeline\n",
    "from lib.deeplearning import create_transformation_pipelines\n",
    "from lib.transform_impute import convert_back_df\n",
    "from lib.split_data import train_test_group_time_split"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "RANDOM_SEED = 31\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tensorflow.random.set_seed(RANDOM_SEED)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tensorflow.config.list_physical_devices('GPU')))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# During experiment we can try to use neptune.ai to log all the Tensorflow experiments results\n",
    "neptune_key = pickle.load(open(\"./neptune.pkl\", \"rb\"))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preparing the Dataset\n",
    "The train and test sets are split by Township-Ranges, i.e. some Township-Ranges data are either fully in the train or test set.\n",
    "The target value is the value of that variable for 2021\n",
    "Thus train/test sets are of shape (number of Township-Ranges, 7 years (2014-2020), the number of features).\n",
    "The input of 1 data point in the model is of shape (7x81\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "                     TOTALDRILLDEPTH_AVG  WELLYIELD_AVG  STATICWATERLEVEL_AVG  \\\nTOWNSHIP_RANGE YEAR                                                             \nT01N R02E      2014             0.000000       0.000489              0.020868   \n               2015             0.000000       0.000000              0.000000   \n               2016             0.000000       0.003259              0.036728   \n               2017             0.066667       0.006410              0.025876   \n               2018             0.053651       0.000652              0.063022   \n...                                  ...            ...                   ...   \nT32S R30E      2016             0.000000       0.000000              0.000000   \n               2017             0.000000       0.000000              0.000000   \n               2018             0.000000       0.000000              0.000000   \n               2019             0.000000       0.000000              0.000000   \n               2020             0.000000       0.000000              0.000000   \n\n                     TOPOFPERFORATEDINTERVAL_AVG  \\\nTOWNSHIP_RANGE YEAR                                \nT01N R02E      2014                     0.052288   \n               2015                     0.000000   \n               2016                     0.084967   \n               2017                     0.082789   \n               2018                     0.077124   \n...                                          ...   \nT32S R30E      2016                     0.000000   \n               2017                     0.000000   \n               2018                     0.000000   \n               2019                     0.000000   \n               2020                     0.000000   \n\n                     BOTTOMOFPERFORATEDINTERVAL_AVG  TOTALCOMPLETEDDEPTH_AVG  \\\nTOWNSHIP_RANGE YEAR                                                            \nT01N R02E      2014                        0.076699                 0.127841   \n               2015                        0.000000                 0.000000   \n               2016                        0.058252                 0.056818   \n               2017                        0.064725                 0.074495   \n               2018                        0.053592                 0.064015   \n...                                             ...                      ...   \nT32S R30E      2016                        0.000000                 0.000000   \n               2017                        0.000000                 0.000000   \n               2018                        0.000000                 0.000000   \n               2019                        0.000000                 0.000000   \n               2020                        0.000000                 0.000000   \n\n                     VEGETATION_BLUE_OAK-GRAY_PINE  \\\nTOWNSHIP_RANGE YEAR                                  \nT01N R02E      2014                       0.010798   \n               2015                       0.010798   \n               2016                       0.010798   \n               2017                       0.010798   \n               2018                       0.010798   \n...                                            ...   \nT32S R30E      2016                       0.033178   \n               2017                       0.033178   \n               2018                       0.033178   \n               2019                       0.033178   \n               2020                       0.033178   \n\n                     VEGETATION_CALIFORNIA_COAST_LIVE_OAK  \\\nTOWNSHIP_RANGE YEAR                                         \nT01N R02E      2014                              0.002749   \n               2015                              0.002749   \n               2016                              0.002749   \n               2017                              0.002749   \n               2018                              0.002749   \n...                                                   ...   \nT32S R30E      2016                              0.000000   \n               2017                              0.000000   \n               2018                              0.000000   \n               2019                              0.000000   \n               2020                              0.000000   \n\n                     VEGETATION_CANYON_LIVE_OAK  VEGETATION_HARD_CHAPARRAL  \\\nTOWNSHIP_RANGE YEAR                                                          \nT01N R02E      2014                    0.000000                   0.000633   \n               2015                    0.000000                   0.000633   \n               2016                    0.000000                   0.000633   \n               2017                    0.000000                   0.000633   \n               2018                    0.000000                   0.000633   \n...                                         ...                        ...   \nT32S R30E      2016                    0.002023                   0.003535   \n               2017                    0.002023                   0.003535   \n               2018                    0.002023                   0.003535   \n               2019                    0.002023                   0.003535   \n               2020                    0.002023                   0.003535   \n\n                     ...  POPULATION_DENSITY  PCT_OF_CAPACITY  \\\nTOWNSHIP_RANGE YEAR  ...                                        \nT01N R02E      2014  ...            0.391791         0.776467   \n               2015  ...            0.394044         0.776467   \n               2016  ...            0.395968         0.776467   \n               2017  ...            0.406050         0.776467   \n               2018  ...            0.405447         0.776467   \n...                  ...                 ...              ...   \nT32S R30E      2016  ...            0.004489         0.496289   \n               2017  ...            0.004477         0.496289   \n               2018  ...            0.004494         0.496289   \n               2019  ...            0.004511         0.580893   \n               2020  ...            0.004533         0.499980   \n\n                     GROUNDSURFACEELEVATION_AVG  AVERAGE_YEARLY_PRECIPITATION  \\\nTOWNSHIP_RANGE YEAR                                                             \nT01N R02E      2014                    0.043092                      0.286941   \n               2015                    0.037376                      0.301232   \n               2016                    0.016622                      0.357881   \n               2017                    0.031660                      0.689154   \n               2018                    0.051869                      0.252603   \n...                                         ...                           ...   \nT32S R30E      2016                    0.058099                      0.118655   \n               2017                    0.058099                      0.180043   \n               2018                    0.058099                      0.084816   \n               2019                    0.058099                      0.168764   \n               2020                    0.058099                      0.166161   \n\n                     SHORTAGE_COUNT   GSE_GWE  WELL_COUNT_AGRICULTURE  \\\nTOWNSHIP_RANGE YEAR                                                     \nT01N R02E      2014             0.0  0.288740                0.021277   \n               2015             0.0  0.286127                0.000000   \n               2016             0.0  0.266940                0.000000   \n               2017             0.0  0.264658                0.000000   \n               2018             0.0  0.258964                0.021277   \n...                             ...       ...                     ...   \nT32S R30E      2016             0.0  0.816752                0.000000   \n               2017             0.0  0.752786                0.000000   \n               2018             0.0  0.772691                0.000000   \n               2019             0.0  0.780003                0.000000   \n               2020             0.0  0.771880                0.000000   \n\n                     WELL_COUNT_DOMESTIC  WELL_COUNT_INDUSTRIAL  \\\nTOWNSHIP_RANGE YEAR                                               \nT01N R02E      2014             0.013889                    0.0   \n               2015             0.000000                    0.0   \n               2016             0.013889                    0.0   \n               2017             0.041667                    0.0   \n               2018             0.013889                    0.0   \n...                                  ...                    ...   \nT32S R30E      2016             0.000000                    0.0   \n               2017             0.000000                    0.0   \n               2018             0.000000                    0.0   \n               2019             0.000000                    0.0   \n               2020             0.000000                    0.0   \n\n                     WELL_COUNT_PUBLIC  \nTOWNSHIP_RANGE YEAR                     \nT01N R02E      2014                0.0  \n               2015                0.0  \n               2016                0.0  \n               2017                0.0  \n               2018                0.0  \n...                                ...  \nT32S R30E      2016                0.0  \n               2017                0.0  \n               2018                0.0  \n               2019                0.0  \n               2020                0.0  \n\n[2842 rows x 81 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th>TOTALDRILLDEPTH_AVG</th>\n      <th>WELLYIELD_AVG</th>\n      <th>STATICWATERLEVEL_AVG</th>\n      <th>TOPOFPERFORATEDINTERVAL_AVG</th>\n      <th>BOTTOMOFPERFORATEDINTERVAL_AVG</th>\n      <th>TOTALCOMPLETEDDEPTH_AVG</th>\n      <th>VEGETATION_BLUE_OAK-GRAY_PINE</th>\n      <th>VEGETATION_CALIFORNIA_COAST_LIVE_OAK</th>\n      <th>VEGETATION_CANYON_LIVE_OAK</th>\n      <th>VEGETATION_HARD_CHAPARRAL</th>\n      <th>...</th>\n      <th>POPULATION_DENSITY</th>\n      <th>PCT_OF_CAPACITY</th>\n      <th>GROUNDSURFACEELEVATION_AVG</th>\n      <th>AVERAGE_YEARLY_PRECIPITATION</th>\n      <th>SHORTAGE_COUNT</th>\n      <th>GSE_GWE</th>\n      <th>WELL_COUNT_AGRICULTURE</th>\n      <th>WELL_COUNT_DOMESTIC</th>\n      <th>WELL_COUNT_INDUSTRIAL</th>\n      <th>WELL_COUNT_PUBLIC</th>\n    </tr>\n    <tr>\n      <th>TOWNSHIP_RANGE</th>\n      <th>YEAR</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">T01N R02E</th>\n      <th>2014</th>\n      <td>0.000000</td>\n      <td>0.000489</td>\n      <td>0.020868</td>\n      <td>0.052288</td>\n      <td>0.076699</td>\n      <td>0.127841</td>\n      <td>0.010798</td>\n      <td>0.002749</td>\n      <td>0.000000</td>\n      <td>0.000633</td>\n      <td>...</td>\n      <td>0.391791</td>\n      <td>0.776467</td>\n      <td>0.043092</td>\n      <td>0.286941</td>\n      <td>0.0</td>\n      <td>0.288740</td>\n      <td>0.021277</td>\n      <td>0.013889</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2015</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.010798</td>\n      <td>0.002749</td>\n      <td>0.000000</td>\n      <td>0.000633</td>\n      <td>...</td>\n      <td>0.394044</td>\n      <td>0.776467</td>\n      <td>0.037376</td>\n      <td>0.301232</td>\n      <td>0.0</td>\n      <td>0.286127</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2016</th>\n      <td>0.000000</td>\n      <td>0.003259</td>\n      <td>0.036728</td>\n      <td>0.084967</td>\n      <td>0.058252</td>\n      <td>0.056818</td>\n      <td>0.010798</td>\n      <td>0.002749</td>\n      <td>0.000000</td>\n      <td>0.000633</td>\n      <td>...</td>\n      <td>0.395968</td>\n      <td>0.776467</td>\n      <td>0.016622</td>\n      <td>0.357881</td>\n      <td>0.0</td>\n      <td>0.266940</td>\n      <td>0.000000</td>\n      <td>0.013889</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2017</th>\n      <td>0.066667</td>\n      <td>0.006410</td>\n      <td>0.025876</td>\n      <td>0.082789</td>\n      <td>0.064725</td>\n      <td>0.074495</td>\n      <td>0.010798</td>\n      <td>0.002749</td>\n      <td>0.000000</td>\n      <td>0.000633</td>\n      <td>...</td>\n      <td>0.406050</td>\n      <td>0.776467</td>\n      <td>0.031660</td>\n      <td>0.689154</td>\n      <td>0.0</td>\n      <td>0.264658</td>\n      <td>0.000000</td>\n      <td>0.041667</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2018</th>\n      <td>0.053651</td>\n      <td>0.000652</td>\n      <td>0.063022</td>\n      <td>0.077124</td>\n      <td>0.053592</td>\n      <td>0.064015</td>\n      <td>0.010798</td>\n      <td>0.002749</td>\n      <td>0.000000</td>\n      <td>0.000633</td>\n      <td>...</td>\n      <td>0.405447</td>\n      <td>0.776467</td>\n      <td>0.051869</td>\n      <td>0.252603</td>\n      <td>0.0</td>\n      <td>0.258964</td>\n      <td>0.021277</td>\n      <td>0.013889</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">T32S R30E</th>\n      <th>2016</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.033178</td>\n      <td>0.000000</td>\n      <td>0.002023</td>\n      <td>0.003535</td>\n      <td>...</td>\n      <td>0.004489</td>\n      <td>0.496289</td>\n      <td>0.058099</td>\n      <td>0.118655</td>\n      <td>0.0</td>\n      <td>0.816752</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2017</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.033178</td>\n      <td>0.000000</td>\n      <td>0.002023</td>\n      <td>0.003535</td>\n      <td>...</td>\n      <td>0.004477</td>\n      <td>0.496289</td>\n      <td>0.058099</td>\n      <td>0.180043</td>\n      <td>0.0</td>\n      <td>0.752786</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2018</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.033178</td>\n      <td>0.000000</td>\n      <td>0.002023</td>\n      <td>0.003535</td>\n      <td>...</td>\n      <td>0.004494</td>\n      <td>0.496289</td>\n      <td>0.058099</td>\n      <td>0.084816</td>\n      <td>0.0</td>\n      <td>0.772691</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2019</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.033178</td>\n      <td>0.000000</td>\n      <td>0.002023</td>\n      <td>0.003535</td>\n      <td>...</td>\n      <td>0.004511</td>\n      <td>0.580893</td>\n      <td>0.058099</td>\n      <td>0.168764</td>\n      <td>0.0</td>\n      <td>0.780003</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2020</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.033178</td>\n      <td>0.000000</td>\n      <td>0.002023</td>\n      <td>0.003535</td>\n      <td>...</td>\n      <td>0.004533</td>\n      <td>0.499980</td>\n      <td>0.058099</td>\n      <td>0.166161</td>\n      <td>0.0</td>\n      <td>0.771880</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>2842 rows × 81 columns</p>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_size=0.15\n",
    "# Load the data from the ETL output files\n",
    "X = read_and_join_output_file()\n",
    "#X[\"WELL_COUNT\"] = X[\"WELL_COUNT_PUBLIC\"] + X[\"WELL_COUNT_AGRICULTURE\"] + X[\"WELL_COUNT_DOMESTIC\"] + X[\"WELL_COUNT_INDUSTRIAL\"]\n",
    "#X.drop(columns=[\"WELL_COUNT_PUBLIC\", \"WELL_COUNT_AGRICULTURE\", \"WELL_COUNT_DOMESTIC\", \"WELL_COUNT_INDUSTRIAL\"], inplace=True)\n",
    "# Split the data into a training and a test set\n",
    "X_train_df, X_test_df, y_train_df, y_test_df = train_test_group_time_split(X, index=[\"TOWNSHIP_RANGE\", \"YEAR\"], group=\"TOWNSHIP_RANGE\", test_size=test_size, random_seed=RANDOM_SEED)\n",
    "# Create, fit and apply the data imputation pipeline to the training and test sets\n",
    "impute_pipeline, columns = create_transformation_pipelines(X_train_df)\n",
    "X_train_impute = impute_pipeline.fit_transform(X_train_df)\n",
    "X_test_impute = impute_pipeline.transform(X_test_df)\n",
    "# Convert the X_train and X_test back to dataframes\n",
    "X_train_impute_df = pd.DataFrame(X_train_impute, index=X_train_df.index, columns=columns)\n",
    "X_test_impute_df = pd.DataFrame(X_test_impute, index=X_test_df.index, columns=columns)\n",
    "X_train_impute_df[\"GSE_GWE\"] = np.sqrt(X_train_impute_df[\"GSE_GWE\"])\n",
    "X_test_impute_df[\"GSE_GWE\"] = np.sqrt(X_test_impute_df[\"GSE_GWE\"])\n",
    "# Keep only the GSE_GWE variable as the outcome variable\n",
    "scaler = MinMaxScaler()\n",
    "y_train = scaler.fit_transform(y_train_df[[\"GSE_GWE\"]])\n",
    "y_train = np.sqrt(y_train)\n",
    "y_test = scaler.transform(y_test_df[[\"GSE_GWE\"]])\n",
    "y_train_3d = y_train[..., np.newaxis]\n",
    "X_train_impute_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# Change the shape of the input array to (number of Township-Ranges, 7 years (2014-2020), the number of features)\n",
    "X_train = X_train_impute_df.values.reshape(len(X_train_impute_df.index.get_level_values(0).unique()), len(X_train_impute_df.index.get_level_values(1).unique()), X_train_impute_df.shape[1])\n",
    "X_test = X_test_impute_df.values.reshape(len(X_test_impute_df.index.get_level_values(0).unique()), len(X_test_impute_df.index.get_level_values(1).unique()), X_test_impute_df.shape[1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Checking the train, validation and test input (X) datasets sizes:\n",
      "Size of the X_train dataset: (406, 7, 81)\n",
      "Size of the X_test dataset: (72, 7, 81)\n",
      "====================================================================================================\n",
      "Checking the train, validation and test output (y) datasets sizes:\n",
      "Size of the y_train dataset: (406, 1)\n",
      "Size of the y_test dataset: (72, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*100)\n",
    "print(\"Checking the train, validation and test input (X) datasets sizes:\")\n",
    "print(f\"Size of the X_train dataset: {X_train.shape}\")\n",
    "#print(f\"Size of the X_val dataset: {X_val.shape}\")\n",
    "print(f\"Size of the X_test dataset: {X_test.shape}\")\n",
    "print(\"=\"*100)\n",
    "print(\"Checking the train, validation and test output (y) datasets sizes:\")\n",
    "print(f\"Size of the y_train dataset: {y_train.shape}\")\n",
    "#print(f\"Size of the y_val dataset: {y_val_df.shape}\")\n",
    "print(f\"Size of the y_test dataset: {y_test.shape}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def evaluate_forecast(y_test_inverse, yhat_inverse):\n",
    "    mse_ = keras.metrics.MeanSquaredError()\n",
    "    mae_ = keras.metrics.MeanAbsoluteError()\n",
    "    rmse_ = keras.metrics.RootMeanSquaredError()\n",
    "    mae = mae_(y_test_inverse,yhat_inverse)\n",
    "    print('mae:', mae)\n",
    "    mse = mse_(y_test_inverse,yhat_inverse)\n",
    "    print('mse:', mse)\n",
    "    rmse = rmse_(y_test_inverse,yhat_inverse)\n",
    "    print('rmse:', rmse)\n",
    "    return mae, mse, rmse"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "nb_features = len(X_train_impute_df.columns)\n",
    "output_activation = \"linear\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Simple Model Hyper-parameter Tuning\n",
    "This model is just made of a single LSTM layer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "class Model1(keras_tuner.HyperModel):\n",
    "    def build(self, hp):\n",
    "        model = Sequential()\n",
    "        hp_units = hp.Int(\"units\", min_value=20, max_value=300, step=20)\n",
    "        hp_activ = hp.Choice(\"activation\", values=[\"relu\", \"tanh\", \"sigmoid\"])\n",
    "        model.add(LSTM(units=hp_units, activation=hp_activ, input_shape=(7, nb_features)))\n",
    "        model.add(Dense(1, activation=output_activation))\n",
    "\n",
    "        hp_learning_rate = hp.Choice(\"learning_rate\", values=[1e-2, 1e-3, 1e-4])\n",
    "        model.compile(loss=\"mse\", optimizer=Adam(learning_rate=hp_learning_rate), metrics=[keras.metrics.RootMeanSquaredError()])\n",
    "\n",
    "        return model\n",
    "\n",
    "    def fit(self, hp, model, *args, **kwargs):\n",
    "        return model.fit(\n",
    "            *args,\n",
    "            validation_split=hp.Float(\"validation_split\", min_value=0.05, max_value=0.2, step=0.05),\n",
    "            batch_size=hp.Int(\"batch_size\", min_value=32, max_value=192, step=32),\n",
    "            #epochs=hp.Int(\"epochs\", min_value=50, max_value=400, step=50),\n",
    "            **kwargs,\n",
    "        )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "stop_early = tensorflow.keras.callbacks.EarlyStopping(monitor='val_root_mean_squared_error', patience=7, verbose=1)\n",
    "tuner = keras_tuner.Hyperband(Model1(),\n",
    "                              objective=keras_tuner.Objective(\"val_root_mean_squared_error\", direction=\"min\"),\n",
    "                              max_epochs=100000,\n",
    "                              factor=1000,\n",
    "                              overwrite=True,\n",
    "                              directory=\"keras_tuner\",\n",
    "                              project_name=\"keras_tuner\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 902 Complete [00h 00m 04s]\n",
      "val_root_mean_squared_error: 0.12750886380672455\n",
      "\n",
      "Best val_root_mean_squared_error So Far: 0.0701412484049797\n",
      "Total elapsed time: 02h 37m 23s\n",
      "\n",
      "Search: Running Trial #903\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "200               |80                |units\n",
      "relu              |sigmoid           |activation\n",
      "0.001             |0.01              |learning_rate\n",
      "0.1               |0.05              |validation_split\n",
      "128               |32                |batch_size\n",
      "100               |100               |tuner/epochs\n",
      "0                 |0                 |tuner/initial_epoch\n",
      "1                 |1                 |tuner/bracket\n",
      "0                 |0                 |tuner/round\n",
      "\n",
      "Epoch 1/100\n",
      "3/3 [==============================] - 2s 281ms/step - loss: 0.1836 - root_mean_squared_error: 0.4285 - val_loss: 0.0876 - val_root_mean_squared_error: 0.2960\n",
      "Epoch 2/100\n",
      "3/3 [==============================] - 0s 142ms/step - loss: 0.0541 - root_mean_squared_error: 0.2326 - val_loss: 0.0179 - val_root_mean_squared_error: 0.1339\n",
      "Epoch 3/100\n",
      "3/3 [==============================] - 0s 63ms/step - loss: 0.0498 - root_mean_squared_error: 0.2233 - val_loss: 0.0317 - val_root_mean_squared_error: 0.1779\n",
      "Epoch 4/100\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0373 - root_mean_squared_error: 0.1930"
     ]
    }
   ],
   "source": [
    "tuner.search(X_train, y_train, epochs=100, callbacks=[stop_early])\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(f\"\"\"\n",
    "The hyperparameter search is complete.\n",
    "lstm_units: {best_hps.get('units')}\n",
    "lstm_activation: {best_hps.get('activation')}\n",
    "learning_rate: {best_hps.get('learning_rate')}\n",
    "batch_size: {best_hps.get('batch_size')}\n",
    "validation_split: {best_hps.get('validation_split')}\n",
    "\"\"\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Simple Model Evaluation\n",
    "The results of the evaluation of the tuned model compared to the test set are storred in Neptune"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "m1_hyper_parameters = {\n",
    "    \"random_seed\": RANDOM_SEED,\n",
    "    \"test_size\": test_size,\n",
    "    \"validation_split\": 0.1,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"batch_size\": 32,\n",
    "    \"epochs\": 200,\n",
    "    \"lstm_units\": 40,\n",
    "    \"lstm_activation\": \"sigmoid\",\n",
    "    \"output_activation\": output_activation,\n",
    "    \"nb_features\": nb_features,\n",
    "    \"optimizer\": \"Adam\"\n",
    "}\n",
    "\n",
    "m1_optimizer = {\n",
    "    \"RMSprop\": RMSprop(learning_rate=m1_hyper_parameters[\"learning_rate\"]),\n",
    "    \"Adam\": Adam(learning_rate=m1_hyper_parameters[\"learning_rate\"]),\n",
    "    \"Adamax\": Adamax(learning_rate=m1_hyper_parameters[\"learning_rate\"]),\n",
    "    \"Adagrad\": Adagrad(learning_rate=m1_hyper_parameters[\"learning_rate\"])\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_1 (LSTM)               (None, 70)                42560     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 71        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 42,631\n",
      "Trainable params: 42,631\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1 = Sequential()\n",
    "model1.add(LSTM(m1_hyper_parameters[\"lstm_units\"], activation=m1_hyper_parameters[\"lstm_activation\"], input_shape=(7, nb_features)))\n",
    "model1.add(Dense(1, activation=m1_hyper_parameters[\"output_activation\"]))\n",
    "model1.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/milestone2-california-water-shortage/deeplearning-lstm/e/DEEPLSTM-87\n",
      "Remember to stop your run once you’ve finished logging your metadata (https://docs.neptune.ai/api-reference/run#.stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.\n",
      "Epoch 1/200\n",
      "12/12 [==============================] - 1s 27ms/step - loss: 0.2526 - val_loss: 0.0368\n",
      "Epoch 2/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0431 - val_loss: 0.0152\n",
      "Epoch 3/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0233 - val_loss: 0.0116\n",
      "Epoch 4/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0175 - val_loss: 0.0121\n",
      "Epoch 5/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0148 - val_loss: 0.0084\n",
      "Epoch 6/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0129 - val_loss: 0.0082\n",
      "Epoch 7/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0112 - val_loss: 0.0062\n",
      "Epoch 8/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0099 - val_loss: 0.0050\n",
      "Epoch 9/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0086 - val_loss: 0.0053\n",
      "Epoch 10/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0076 - val_loss: 0.0041\n",
      "Epoch 11/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0067 - val_loss: 0.0040\n",
      "Epoch 12/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0059 - val_loss: 0.0039\n",
      "Epoch 13/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0053 - val_loss: 0.0084\n",
      "Epoch 14/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0051 - val_loss: 0.0051\n",
      "Epoch 15/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0042 - val_loss: 0.0044\n",
      "Epoch 16/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0037 - val_loss: 0.0041\n",
      "Epoch 17/200\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.0035 - val_loss: 0.0043\n",
      "Epoch 18/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0032 - val_loss: 0.0035\n",
      "Epoch 19/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0030 - val_loss: 0.0044\n",
      "Epoch 20/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0029 - val_loss: 0.0045\n",
      "Epoch 21/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0029 - val_loss: 0.0077\n",
      "Epoch 22/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0028 - val_loss: 0.0059\n",
      "Epoch 23/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0028 - val_loss: 0.0074\n",
      "Epoch 24/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 25/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0024 - val_loss: 0.0048\n",
      "Epoch 26/200\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.0025 - val_loss: 0.0034\n",
      "Epoch 27/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0024 - val_loss: 0.0021\n",
      "Epoch 28/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0022 - val_loss: 0.0028\n",
      "Epoch 29/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0022 - val_loss: 0.0026\n",
      "Epoch 30/200\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.0022 - val_loss: 0.0022\n",
      "Epoch 31/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0021 - val_loss: 0.0015\n",
      "Epoch 32/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0022 - val_loss: 0.0035\n",
      "Epoch 33/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0021 - val_loss: 0.0028\n",
      "Epoch 34/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0021 - val_loss: 0.0014\n",
      "Epoch 35/200\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.0021 - val_loss: 0.0031\n",
      "Epoch 36/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0023 - val_loss: 0.0011\n",
      "Epoch 37/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0023 - val_loss: 0.0035\n",
      "Epoch 38/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0020 - val_loss: 0.0029\n",
      "Epoch 39/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0021 - val_loss: 9.7630e-04\n",
      "Epoch 40/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0021 - val_loss: 0.0023\n",
      "Epoch 41/200\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.0022 - val_loss: 9.6225e-04\n",
      "Epoch 42/200\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.0023 - val_loss: 0.0034\n",
      "Epoch 43/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0020 - val_loss: 0.0015\n",
      "Epoch 44/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0019 - val_loss: 0.0011\n",
      "Epoch 45/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0025 - val_loss: 0.0020\n",
      "Epoch 46/200\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.0020 - val_loss: 0.0010\n",
      "Epoch 47/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0024 - val_loss: 0.0030\n",
      "Epoch 48/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0021 - val_loss: 0.0013\n",
      "Epoch 49/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0021 - val_loss: 0.0028\n",
      "Epoch 50/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0021 - val_loss: 0.0014\n",
      "Epoch 51/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0019 - val_loss: 0.0014\n",
      "Epoch 52/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0019 - val_loss: 0.0011\n",
      "Epoch 53/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0021 - val_loss: 0.0059\n",
      "Epoch 54/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0032 - val_loss: 0.0040\n",
      "Epoch 55/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0034 - val_loss: 0.0023\n",
      "Epoch 56/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0022 - val_loss: 0.0022\n",
      "Epoch 57/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0022 - val_loss: 0.0017\n",
      "Epoch 58/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0021 - val_loss: 0.0012\n",
      "Epoch 59/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0023 - val_loss: 0.0024\n",
      "Epoch 60/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0018 - val_loss: 0.0013\n",
      "Epoch 61/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0018 - val_loss: 0.0028\n",
      "Epoch 62/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0019 - val_loss: 0.0013\n",
      "Epoch 63/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 64/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0018 - val_loss: 0.0035\n",
      "Epoch 65/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0021 - val_loss: 0.0012\n",
      "Epoch 66/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0020 - val_loss: 0.0011\n",
      "Epoch 67/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0018 - val_loss: 0.0031\n",
      "Epoch 68/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0019 - val_loss: 9.9339e-04\n",
      "Epoch 69/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0020 - val_loss: 0.0016\n",
      "Epoch 70/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0018 - val_loss: 0.0017\n",
      "Epoch 71/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0018 - val_loss: 0.0034\n",
      "Epoch 72/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0018 - val_loss: 0.0016\n",
      "Epoch 73/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0019 - val_loss: 0.0039\n",
      "Epoch 74/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0020 - val_loss: 0.0011\n",
      "Epoch 75/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0023 - val_loss: 0.0020\n",
      "Epoch 76/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0018 - val_loss: 0.0012\n",
      "Epoch 77/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0018 - val_loss: 0.0011\n",
      "Epoch 78/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0019 - val_loss: 0.0059\n",
      "Epoch 79/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0027 - val_loss: 0.0017\n",
      "Epoch 80/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0020 - val_loss: 0.0015\n",
      "Epoch 81/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0026 - val_loss: 0.0025\n",
      "Epoch 82/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0019 - val_loss: 0.0027\n",
      "Epoch 83/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0020 - val_loss: 0.0019\n",
      "Epoch 84/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0018 - val_loss: 0.0016\n",
      "Epoch 85/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0018 - val_loss: 0.0041\n",
      "Epoch 86/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0021 - val_loss: 0.0012\n",
      "Epoch 87/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0026 - val_loss: 0.0011\n",
      "Epoch 88/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0018 - val_loss: 0.0023\n",
      "Epoch 89/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0018 - val_loss: 0.0023\n",
      "Epoch 90/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0018 - val_loss: 0.0017\n",
      "Epoch 91/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0017 - val_loss: 0.0013\n",
      "Epoch 92/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0020 - val_loss: 0.0022\n",
      "Epoch 93/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 94/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0019 - val_loss: 0.0014\n",
      "Epoch 95/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0019 - val_loss: 0.0036\n",
      "Epoch 96/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0019 - val_loss: 0.0014\n",
      "Epoch 97/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0019 - val_loss: 0.0022\n",
      "Epoch 98/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0016 - val_loss: 0.0014\n",
      "Epoch 99/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0020 - val_loss: 0.0013\n",
      "Epoch 100/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0019 - val_loss: 0.0044\n",
      "Epoch 101/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0022 - val_loss: 0.0034\n",
      "Epoch 102/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0019 - val_loss: 0.0013\n",
      "Epoch 103/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0019 - val_loss: 0.0016\n",
      "Epoch 104/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0017 - val_loss: 0.0018\n",
      "Epoch 105/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0018 - val_loss: 0.0050\n",
      "Epoch 106/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0018 - val_loss: 0.0022\n",
      "Epoch 107/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0021 - val_loss: 0.0013\n",
      "Epoch 108/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0022 - val_loss: 0.0016\n",
      "Epoch 109/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0019 - val_loss: 0.0018\n",
      "Epoch 110/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0018 - val_loss: 0.0028\n",
      "Epoch 111/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0019 - val_loss: 0.0011\n",
      "Epoch 112/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0021 - val_loss: 0.0015\n",
      "Epoch 113/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0018 - val_loss: 0.0019\n",
      "Epoch 114/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0019 - val_loss: 0.0017\n",
      "Epoch 115/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0017 - val_loss: 0.0031\n",
      "Epoch 116/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0018 - val_loss: 0.0020\n",
      "Epoch 117/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0019 - val_loss: 0.0014\n",
      "Epoch 118/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0018 - val_loss: 0.0014\n",
      "Epoch 119/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0020 - val_loss: 0.0014\n",
      "Epoch 120/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0019 - val_loss: 0.0014\n",
      "Epoch 121/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0020 - val_loss: 0.0024\n",
      "Epoch 122/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0018 - val_loss: 0.0022\n",
      "Epoch 123/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0016 - val_loss: 0.0026\n",
      "Epoch 124/200\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.0016 - val_loss: 0.0017\n",
      "Epoch 125/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0016 - val_loss: 0.0023\n",
      "Epoch 126/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0017 - val_loss: 0.0028\n",
      "Epoch 127/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0021 - val_loss: 0.0013\n",
      "Epoch 128/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0026 - val_loss: 0.0014\n",
      "Epoch 129/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0019 - val_loss: 0.0031\n",
      "Epoch 130/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0017 - val_loss: 0.0023\n",
      "Epoch 131/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0016 - val_loss: 0.0013\n",
      "Epoch 132/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0016 - val_loss: 0.0021\n",
      "Epoch 133/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0017 - val_loss: 0.0037\n",
      "Epoch 134/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0018 - val_loss: 0.0029\n",
      "Epoch 135/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0016 - val_loss: 0.0023\n",
      "Epoch 136/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0015 - val_loss: 0.0019\n",
      "Epoch 137/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0017 - val_loss: 0.0021\n",
      "Epoch 138/200\n",
      "12/12 [==============================] - 0s 12ms/step - loss: 0.0019 - val_loss: 0.0018\n",
      "Epoch 139/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 140/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0015 - val_loss: 0.0025\n",
      "Epoch 141/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0017 - val_loss: 0.0015\n",
      "Epoch 142/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0017 - val_loss: 0.0016\n",
      "Epoch 143/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0023 - val_loss: 0.0015\n",
      "Epoch 144/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0017 - val_loss: 0.0039\n",
      "Epoch 145/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0017 - val_loss: 0.0019\n",
      "Epoch 146/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0017 - val_loss: 0.0025\n",
      "Epoch 147/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0016 - val_loss: 0.0014\n",
      "Epoch 148/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0017 - val_loss: 0.0015\n",
      "Epoch 149/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0016 - val_loss: 0.0028\n",
      "Epoch 150/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0015 - val_loss: 0.0021\n",
      "Epoch 151/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0016 - val_loss: 0.0022\n",
      "Epoch 152/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0015 - val_loss: 0.0026\n",
      "Epoch 153/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0017 - val_loss: 0.0044\n",
      "Epoch 154/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0018 - val_loss: 0.0020\n",
      "Epoch 155/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0016 - val_loss: 0.0025\n",
      "Epoch 156/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0019 - val_loss: 0.0014\n",
      "Epoch 157/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0016 - val_loss: 0.0012\n",
      "Epoch 158/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0017 - val_loss: 0.0019\n",
      "Epoch 159/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0019 - val_loss: 0.0015\n",
      "Epoch 160/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 161/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0014 - val_loss: 0.0024\n",
      "Epoch 162/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0014 - val_loss: 0.0041\n",
      "Epoch 163/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0017 - val_loss: 0.0020\n",
      "Epoch 164/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0014 - val_loss: 0.0020\n",
      "Epoch 165/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0015 - val_loss: 0.0019\n",
      "Epoch 166/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 167/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0016 - val_loss: 0.0013\n",
      "Epoch 168/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 169/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0015 - val_loss: 0.0020\n",
      "Epoch 170/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0013 - val_loss: 0.0015\n",
      "Epoch 171/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 172/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0014 - val_loss: 0.0018\n",
      "Epoch 173/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0013 - val_loss: 0.0018\n",
      "Epoch 174/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0016 - val_loss: 0.0023\n",
      "Epoch 175/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 176/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0018 - val_loss: 0.0016\n",
      "Epoch 177/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0015 - val_loss: 0.0018\n",
      "Epoch 178/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 179/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0013 - val_loss: 0.0021\n",
      "Epoch 180/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0017 - val_loss: 0.0018\n",
      "Epoch 181/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0016 - val_loss: 0.0019\n",
      "Epoch 182/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0016 - val_loss: 0.0018\n",
      "Epoch 183/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0013 - val_loss: 0.0039\n",
      "Epoch 184/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0015 - val_loss: 0.0038\n",
      "Epoch 185/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0017 - val_loss: 0.0020\n",
      "Epoch 186/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0016 - val_loss: 0.0027\n",
      "Epoch 187/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0015 - val_loss: 0.0023\n",
      "Epoch 188/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0012 - val_loss: 0.0021\n",
      "Epoch 189/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 190/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 191/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0019 - val_loss: 0.0014\n",
      "Epoch 192/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0022 - val_loss: 0.0019\n",
      "Epoch 193/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0017 - val_loss: 0.0016\n",
      "Epoch 194/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0021 - val_loss: 0.0023\n",
      "Epoch 195/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0024 - val_loss: 0.0044\n",
      "Epoch 196/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0020 - val_loss: 0.0046\n",
      "Epoch 197/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0014 - val_loss: 0.0043\n",
      "Epoch 198/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0013 - val_loss: 0.0016\n",
      "Epoch 199/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0012 - val_loss: 0.0026\n",
      "Epoch 200/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0014 - val_loss: 0.0016\n",
      "mae: tf.Tensor(31.096588, shape=(), dtype=float32)\n",
      "mse: tf.Tensor(2664.7837, shape=(), dtype=float32)\n",
      "rmse: tf.Tensor(51.621544, shape=(), dtype=float32)\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 193 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 193 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://app.neptune.ai/milestone2-california-water-shortage/deeplearning-lstm/e/DEEPLSTM-87\n"
     ]
    }
   ],
   "source": [
    "# Start experiment\n",
    "run = neptune.init(\n",
    "    project=\"milestone2-california-water-shortage/deeplearning-lstm\",\n",
    "    api_token=neptune_key,\n",
    "    name=\"Basic Model\",\n",
    "    tags=[\"WithDetailedWellCounts\", \"UnidirectionalLSTM\"]\n",
    ")\n",
    "neptune_cbk = NeptuneCallback(run=run, base_namespace='metrics')\n",
    "run['hyper-parameters'] = m1_hyper_parameters\n",
    "\n",
    "model1.compile(loss=\"mse\", optimizer=m1_optimizer[m1_hyper_parameters[\"optimizer\"]], metrics=[keras.metrics.RootMeanSquaredError()])\n",
    "model1.fit(X_train, y_train,\n",
    "                     validation_split=m1_hyper_parameters[\"validation_split\"],\n",
    "                     batch_size=m1_hyper_parameters[\"batch_size\"],\n",
    "                     epochs=m1_hyper_parameters[\"epochs\"],\n",
    "                     shuffle=True,\n",
    "                     callbacks=[neptune_cbk])\n",
    "yhat = model1.predict(X_test, verbose=0)\n",
    "yhat_inverse = scaler.inverse_transform(yhat)\n",
    "y_test_inverse = scaler.inverse_transform(y_test)\n",
    "mae, mse, rmse = evaluate_forecast(y_test_inverse, yhat_inverse)\n",
    "run[\"eval/mae\"] = mae\n",
    "run[\"eval/mse\"] = mse\n",
    "run[\"eval/rmse\"] = rmse\n",
    "run.stop()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model2 Hyper-parameter tuning\n",
    "This model is made of\n",
    "* a simple or bidirectional LSTM layer\n",
    "* a Dense unit\n",
    "* a Dropout Unit"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "class Model2(keras_tuner.HyperModel):\n",
    "    def build(self, hp):\n",
    "        model = Sequential()\n",
    "        lstm_units = hp.Int(\"lstm_units\", min_value=20, max_value=300, step=20)\n",
    "        lstm_activ = hp.Choice(\"lstm_activation\", values=[\"relu\", \"tanh\", \"sigmoid\"])\n",
    "        model.add(LSTM(units=lstm_units, activation=lstm_activ, input_shape=(7, nb_features)))\n",
    "        dense_units = hp.Int(\"dense_units\", min_value=11, max_value=101, step=10)\n",
    "        dense_activation = hp.Choice(\"dense_activation\", values=[\"relu\", \"tanh\", \"sigmoid\"])\n",
    "        model.add(Dense(dense_units, activation=dense_activation))\n",
    "        hp_dropout = hp.Float(\"dropout_rate\", min_value=0.05, max_value=0.25, step=0.05)\n",
    "        model.add(Dropout(hp_dropout))\n",
    "        model.add(Dense(1, activation=output_activation))\n",
    "\n",
    "        hp_learning_rate = hp.Choice(\"learning_rate\", values=[1e-2, 1e-3, 1e-4])\n",
    "        model.compile(loss=\"mse\", optimizer=Adam(learning_rate=hp_learning_rate), metrics=[keras.metrics.RootMeanSquaredError()])\n",
    "\n",
    "        return model\n",
    "\n",
    "    def fit(self, hp, model, *args, **kwargs):\n",
    "        return model.fit(\n",
    "            *args,\n",
    "            validation_split=hp.Float(\"validation_split\", min_value=0.05, max_value=0.2, step=0.05),\n",
    "            batch_size=hp.Int(\"batch_size\", min_value=32, max_value=192, step=32),\n",
    "            #epochs=hp.Int(\"epochs\", min_value=50, max_value=400, step=50),\n",
    "            **kwargs,\n",
    "        )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "stop_early = tensorflow.keras.callbacks.EarlyStopping(monitor=\"val_root_mean_squared_error\", patience=7, verbose=1)\n",
    "tuner = keras_tuner.Hyperband(Model2(),\n",
    "                              objective=keras_tuner.Objective(\"val_root_mean_squared_error\", direction=\"min\"),\n",
    "                              max_epochs=200000,\n",
    "                              factor=2000,\n",
    "                              overwrite=True,\n",
    "                              directory=\"keras_tuner\",\n",
    "                              project_name=\"keras_tuner\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 2220 Complete [00h 00m 14s]\n",
      "val_root_mean_squared_error: 0.15608908236026764\n",
      "\n",
      "Best val_root_mean_squared_error So Far: 0.07610634714365005\n",
      "Total elapsed time: 14h 26m 03s\n",
      "\n",
      "Search: Running Trial #2221\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "240               |200               |lstm_units\n",
      "relu              |relu              |lstm_activation\n",
      "11                |31                |dense_units\n",
      "tanh              |sigmoid           |dense_activation\n",
      "0.2               |0.2               |dropout_rate\n",
      "0.001             |0.001             |learning_rate\n",
      "0.15              |0.1               |validation_split\n",
      "128               |96                |batch_size\n",
      "100               |100               |tuner/epochs\n",
      "0                 |0                 |tuner/initial_epoch\n",
      "1                 |1                 |tuner/bracket\n",
      "0                 |0                 |tuner/round\n",
      "\n",
      "Epoch 1/100\n",
      "3/3 [==============================] - 3s 473ms/step - loss: 0.0659 - root_mean_squared_error: 0.2568 - val_loss: 0.0202 - val_root_mean_squared_error: 0.1420\n",
      "Epoch 2/100\n",
      "3/3 [==============================] - 1s 391ms/step - loss: 0.0440 - root_mean_squared_error: 0.2097 - val_loss: 0.0207 - val_root_mean_squared_error: 0.1439\n",
      "Epoch 3/100\n",
      "3/3 [==============================] - 1s 436ms/step - loss: 0.0226 - root_mean_squared_error: 0.1504 - val_loss: 0.0283 - val_root_mean_squared_error: 0.1682\n",
      "Epoch 4/100\n",
      "3/3 [==============================] - 1s 277ms/step - loss: 0.0218 - root_mean_squared_error: 0.1478 - val_loss: 0.0263 - val_root_mean_squared_error: 0.1621\n",
      "Epoch 5/100\n",
      "3/3 [==============================] - 1s 460ms/step - loss: 0.0183 - root_mean_squared_error: 0.1354 - val_loss: 0.0199 - val_root_mean_squared_error: 0.1410\n",
      "Epoch 6/100\n",
      "3/3 [==============================] - 1s 200ms/step - loss: 0.0155 - root_mean_squared_error: 0.1245 - val_loss: 0.0203 - val_root_mean_squared_error: 0.1426\n",
      "Epoch 7/100\n",
      "3/3 [==============================] - 1s 265ms/step - loss: 0.0129 - root_mean_squared_error: 0.1137 - val_loss: 0.0156 - val_root_mean_squared_error: 0.1249\n",
      "Epoch 8/100\n",
      "3/3 [==============================] - 1s 275ms/step - loss: 0.0093 - root_mean_squared_error: 0.0966 - val_loss: 0.0130 - val_root_mean_squared_error: 0.1142\n",
      "Epoch 9/100\n",
      "3/3 [==============================] - 1s 293ms/step - loss: 0.0094 - root_mean_squared_error: 0.0969 - val_loss: 0.0119 - val_root_mean_squared_error: 0.1092\n",
      "Epoch 10/100\n",
      "3/3 [==============================] - 1s 305ms/step - loss: 0.0081 - root_mean_squared_error: 0.0898 - val_loss: 0.0108 - val_root_mean_squared_error: 0.1041\n",
      "Epoch 11/100\n",
      "3/3 [==============================] - 1s 304ms/step - loss: 0.0090 - root_mean_squared_error: 0.0946 - val_loss: 0.0103 - val_root_mean_squared_error: 0.1013\n",
      "Epoch 12/100\n",
      "3/3 [==============================] - 1s 300ms/step - loss: 0.0071 - root_mean_squared_error: 0.0841 - val_loss: 0.0099 - val_root_mean_squared_error: 0.0995\n",
      "Epoch 13/100\n",
      "3/3 [==============================] - 1s 295ms/step - loss: 0.0065 - root_mean_squared_error: 0.0804 - val_loss: 0.0098 - val_root_mean_squared_error: 0.0991\n",
      "Epoch 14/100\n",
      "3/3 [==============================] - 1s 185ms/step - loss: 0.0069 - root_mean_squared_error: 0.0829 - val_loss: 0.0102 - val_root_mean_squared_error: 0.1010\n",
      "Epoch 15/100\n",
      "3/3 [==============================] - 1s 180ms/step - loss: 0.0068 - root_mean_squared_error: 0.0823 - val_loss: 0.0100 - val_root_mean_squared_error: 0.1002\n",
      "Epoch 16/100\n"
     ]
    }
   ],
   "source": [
    "tuner.search(X_train, y_train, epochs=200, callbacks=[stop_early])\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(f\"\"\"\n",
    "The hyperparameter search is complete.\n",
    "lstm_units: {best_hps.get('lstm_units')}\n",
    "lstm_activation: {best_hps.get('lstm_activation')}\n",
    "dense_units: {best_hps.get('dense_units')}\n",
    "dense_activation: {best_hps.get('dense_activation')}\n",
    "dropout_rate: {best_hps.get('dropout_rate')}\n",
    "learning_rate: {best_hps.get('learning_rate')}\n",
    "batch_size: {best_hps.get('batch_size')}\n",
    "validation_split: {best_hps.get('validation_split')}\n",
    "\"\"\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model 2 Evaluation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional (Bidirectiona  (None, 240)              193920    \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 11)                2651      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 11)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 12        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 196,583\n",
      "Trainable params: 196,583\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "m2_hyper_parameters = {\n",
    "    \"random_seed\": RANDOM_SEED,\n",
    "    \"test_size\": test_size,\n",
    "    \"validation_split\": 0.15,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"batch_size\": 32,\n",
    "    \"epochs\": 200,\n",
    "    \"lstm_units\": 120,\n",
    "    \"lstm_activation\": \"sigmoid\",\n",
    "    \"dense_units\": 11,\n",
    "    \"dense_activation\": \"tanh\",\n",
    "    \"dropout\": 0.25,\n",
    "    \"output_activation\": \"linear\",\n",
    "    \"nb_features\": nb_features,\n",
    "    \"optimizer\": \"Adam\"\n",
    "}\n",
    "\n",
    "m2_optimizer = {\n",
    "    \"RMSprop\": RMSprop(learning_rate=m2_hyper_parameters[\"learning_rate\"]),\n",
    "    \"Adam\": Adam(learning_rate=m2_hyper_parameters[\"learning_rate\"]),\n",
    "    \"Adamax\": Adamax(learning_rate=m2_hyper_parameters[\"learning_rate\"]),\n",
    "    \"Adagrad\": Adagrad(learning_rate=m2_hyper_parameters[\"learning_rate\"])\n",
    "}\n",
    "\n",
    "model2 = Sequential()\n",
    "model2.add(LSTM(m2_hyper_parameters[\"lstm_units\"], activation=m2_hyper_parameters[\"lstm_activation\"], input_shape=(7, nb_features)))\n",
    "model2.add(Dense(m2_hyper_parameters[\"dense_units\"], activation=m2_hyper_parameters[\"dense_activation\"]))\n",
    "model2.add(Dropout(m2_hyper_parameters[\"dropout\"]))\n",
    "model2.add(Dense(1, activation=m2_hyper_parameters[\"output_activation\"]))\n",
    "model2.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/milestone2-california-water-shortage/deeplearning-lstm/e/DEEPLSTM-141\n",
      "Remember to stop your run once you’ve finished logging your metadata (https://docs.neptune.ai/api-reference/run#.stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.\n",
      "Epoch 1/200\n",
      "11/11 [==============================] - 2s 58ms/step - loss: 0.1887 - root_mean_squared_error: 0.4344 - val_loss: 0.0370 - val_root_mean_squared_error: 0.1923\n",
      "Epoch 2/200\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 0.0777 - root_mean_squared_error: 0.2787 - val_loss: 0.0387 - val_root_mean_squared_error: 0.1968\n",
      "Epoch 3/200\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 0.0552 - root_mean_squared_error: 0.2349 - val_loss: 0.0387 - val_root_mean_squared_error: 0.1968\n",
      "Epoch 4/200\n",
      "11/11 [==============================] - 0s 27ms/step - loss: 0.0354 - root_mean_squared_error: 0.1882 - val_loss: 0.0357 - val_root_mean_squared_error: 0.1888\n",
      "Epoch 5/200\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 0.0290 - root_mean_squared_error: 0.1702 - val_loss: 0.0353 - val_root_mean_squared_error: 0.1879\n",
      "Epoch 6/200\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 0.0269 - root_mean_squared_error: 0.1640 - val_loss: 0.0341 - val_root_mean_squared_error: 0.1848\n",
      "Epoch 7/200\n",
      "11/11 [==============================] - 0s 27ms/step - loss: 0.0250 - root_mean_squared_error: 0.1580 - val_loss: 0.0310 - val_root_mean_squared_error: 0.1760\n",
      "Epoch 8/200\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 0.0215 - root_mean_squared_error: 0.1466 - val_loss: 0.0303 - val_root_mean_squared_error: 0.1741\n",
      "Epoch 9/200\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 0.0199 - root_mean_squared_error: 0.1409 - val_loss: 0.0284 - val_root_mean_squared_error: 0.1684\n",
      "Epoch 10/200\n",
      "11/11 [==============================] - 0s 27ms/step - loss: 0.0195 - root_mean_squared_error: 0.1395 - val_loss: 0.0275 - val_root_mean_squared_error: 0.1659\n",
      "Epoch 11/200\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 0.0205 - root_mean_squared_error: 0.1432 - val_loss: 0.0250 - val_root_mean_squared_error: 0.1581\n",
      "Epoch 12/200\n",
      "11/11 [==============================] - 0s 27ms/step - loss: 0.0169 - root_mean_squared_error: 0.1300 - val_loss: 0.0245 - val_root_mean_squared_error: 0.1564\n",
      "Epoch 13/200\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 0.0175 - root_mean_squared_error: 0.1322 - val_loss: 0.0243 - val_root_mean_squared_error: 0.1558\n",
      "Epoch 14/200\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 0.0185 - root_mean_squared_error: 0.1359 - val_loss: 0.0217 - val_root_mean_squared_error: 0.1474\n",
      "Epoch 15/200\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 0.0177 - root_mean_squared_error: 0.1329 - val_loss: 0.0221 - val_root_mean_squared_error: 0.1486\n",
      "Epoch 16/200\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 0.0153 - root_mean_squared_error: 0.1235 - val_loss: 0.0190 - val_root_mean_squared_error: 0.1379\n",
      "Epoch 17/200\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 0.0143 - root_mean_squared_error: 0.1194 - val_loss: 0.0179 - val_root_mean_squared_error: 0.1338\n",
      "Epoch 18/200\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 0.0137 - root_mean_squared_error: 0.1169 - val_loss: 0.0172 - val_root_mean_squared_error: 0.1310\n",
      "Epoch 19/200\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 0.0151 - root_mean_squared_error: 0.1230 - val_loss: 0.0164 - val_root_mean_squared_error: 0.1279\n",
      "Epoch 20/200\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 0.0160 - root_mean_squared_error: 0.1264 - val_loss: 0.0216 - val_root_mean_squared_error: 0.1469\n",
      "Epoch 21/200\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 0.0123 - root_mean_squared_error: 0.1110 - val_loss: 0.0187 - val_root_mean_squared_error: 0.1368\n",
      "Epoch 22/200\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 0.0134 - root_mean_squared_error: 0.1156 - val_loss: 0.0169 - val_root_mean_squared_error: 0.1298\n",
      "Epoch 23/200\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 0.0140 - root_mean_squared_error: 0.1182 - val_loss: 0.0148 - val_root_mean_squared_error: 0.1215\n",
      "Epoch 24/200\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 0.0117 - root_mean_squared_error: 0.1083 - val_loss: 0.0182 - val_root_mean_squared_error: 0.1348\n",
      "Epoch 25/200\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 0.0138 - root_mean_squared_error: 0.1176 - val_loss: 0.0148 - val_root_mean_squared_error: 0.1216\n",
      "Epoch 26/200\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 0.0109 - root_mean_squared_error: 0.1044 - val_loss: 0.0138 - val_root_mean_squared_error: 0.1174\n",
      "Epoch 27/200\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 0.0105 - root_mean_squared_error: 0.1024 - val_loss: 0.0136 - val_root_mean_squared_error: 0.1164\n",
      "Epoch 28/200\n",
      "11/11 [==============================] - 0s 27ms/step - loss: 0.0135 - root_mean_squared_error: 0.1162 - val_loss: 0.0193 - val_root_mean_squared_error: 0.1389\n",
      "Epoch 29/200\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 0.0128 - root_mean_squared_error: 0.1130 - val_loss: 0.0161 - val_root_mean_squared_error: 0.1270\n",
      "Epoch 30/200\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 0.0101 - root_mean_squared_error: 0.1003 - val_loss: 0.0148 - val_root_mean_squared_error: 0.1217\n",
      "Epoch 31/200\n",
      "11/11 [==============================] - 0s 27ms/step - loss: 0.0104 - root_mean_squared_error: 0.1021 - val_loss: 0.0140 - val_root_mean_squared_error: 0.1183\n",
      "Epoch 32/200\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 0.0091 - root_mean_squared_error: 0.0955 - val_loss: 0.0134 - val_root_mean_squared_error: 0.1158\n",
      "Epoch 33/200\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 0.0094 - root_mean_squared_error: 0.0968 - val_loss: 0.0132 - val_root_mean_squared_error: 0.1148\n",
      "Epoch 34/200\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 0.0119 - root_mean_squared_error: 0.1092 - val_loss: 0.0171 - val_root_mean_squared_error: 0.1307\n",
      "Epoch 35/200\n",
      "11/11 [==============================] - 0s 27ms/step - loss: 0.0109 - root_mean_squared_error: 0.1044 - val_loss: 0.0125 - val_root_mean_squared_error: 0.1118\n",
      "Epoch 36/200\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 0.0102 - root_mean_squared_error: 0.1011 - val_loss: 0.0135 - val_root_mean_squared_error: 0.1162\n",
      "Epoch 37/200\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 0.0096 - root_mean_squared_error: 0.0979 - val_loss: 0.0126 - val_root_mean_squared_error: 0.1122\n",
      "Epoch 38/200\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 0.0093 - root_mean_squared_error: 0.0963 - val_loss: 0.0128 - val_root_mean_squared_error: 0.1130\n",
      "Epoch 39/200\n",
      "11/11 [==============================] - 0s 27ms/step - loss: 0.0098 - root_mean_squared_error: 0.0992 - val_loss: 0.0133 - val_root_mean_squared_error: 0.1152\n",
      "Epoch 40/200\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 0.0098 - root_mean_squared_error: 0.0992 - val_loss: 0.0223 - val_root_mean_squared_error: 0.1493\n",
      "Epoch 41/200\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 0.0091 - root_mean_squared_error: 0.0955 - val_loss: 0.0164 - val_root_mean_squared_error: 0.1282\n",
      "Epoch 42/200\n",
      "11/11 [==============================] - 0s 31ms/step - loss: 0.0082 - root_mean_squared_error: 0.0908 - val_loss: 0.0135 - val_root_mean_squared_error: 0.1160\n",
      "Epoch 43/200\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 0.0108 - root_mean_squared_error: 0.1038 - val_loss: 0.0188 - val_root_mean_squared_error: 0.1369\n",
      "Epoch 44/200\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 0.0129 - root_mean_squared_error: 0.1136 - val_loss: 0.0118 - val_root_mean_squared_error: 0.1088\n",
      "Epoch 45/200\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 0.0082 - root_mean_squared_error: 0.0906 - val_loss: 0.0183 - val_root_mean_squared_error: 0.1353\n",
      "Epoch 46/200\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 0.0089 - root_mean_squared_error: 0.0946 - val_loss: 0.0140 - val_root_mean_squared_error: 0.1185\n",
      "Epoch 47/200\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 0.0123 - root_mean_squared_error: 0.1109 - val_loss: 0.0113 - val_root_mean_squared_error: 0.1065\n",
      "Epoch 48/200\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 0.0116 - root_mean_squared_error: 0.1078 - val_loss: 0.0166 - val_root_mean_squared_error: 0.1287\n",
      "Epoch 49/200\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 0.0120 - root_mean_squared_error: 0.1097 - val_loss: 0.0121 - val_root_mean_squared_error: 0.1100\n",
      "Epoch 50/200\n",
      "11/11 [==============================] - 0s 27ms/step - loss: 0.0073 - root_mean_squared_error: 0.0854 - val_loss: 0.0122 - val_root_mean_squared_error: 0.1105\n",
      "Epoch 51/200\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 0.0083 - root_mean_squared_error: 0.0910 - val_loss: 0.0116 - val_root_mean_squared_error: 0.1076\n",
      "Epoch 52/200\n",
      "11/11 [==============================] - 0s 27ms/step - loss: 0.0082 - root_mean_squared_error: 0.0905 - val_loss: 0.0115 - val_root_mean_squared_error: 0.1071\n",
      "Epoch 53/200\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 0.0107 - root_mean_squared_error: 0.1036 - val_loss: 0.0142 - val_root_mean_squared_error: 0.1191\n",
      "Epoch 54/200\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 0.0079 - root_mean_squared_error: 0.0892 - val_loss: 0.0133 - val_root_mean_squared_error: 0.1151\n",
      "Epoch 55/200\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 0.0071 - root_mean_squared_error: 0.0845 - val_loss: 0.0126 - val_root_mean_squared_error: 0.1124\n",
      "Epoch 56/200\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 0.0087 - root_mean_squared_error: 0.0932 - val_loss: 0.0168 - val_root_mean_squared_error: 0.1295\n",
      "Epoch 57/200\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 0.0102 - root_mean_squared_error: 0.1011 - val_loss: 0.0129 - val_root_mean_squared_error: 0.1137\n",
      "Epoch 58/200\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 0.0082 - root_mean_squared_error: 0.0908 - val_loss: 0.0120 - val_root_mean_squared_error: 0.1094\n",
      "Epoch 59/200\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 0.0077 - root_mean_squared_error: 0.0875 - val_loss: 0.0117 - val_root_mean_squared_error: 0.1082\n",
      "Epoch 60/200\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 0.0117 - root_mean_squared_error: 0.1083 - val_loss: 0.0120 - val_root_mean_squared_error: 0.1093\n",
      "Epoch 61/200\n",
      "11/11 [==============================] - 0s 31ms/step - loss: 0.0117 - root_mean_squared_error: 0.1082 - val_loss: 0.0119 - val_root_mean_squared_error: 0.1092\n",
      "Epoch 62/200\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.0104 - root_mean_squared_error: 0.1019 - val_loss: 0.0119 - val_root_mean_squared_error: 0.1089\n",
      "Epoch 63/200\n",
      "11/11 [==============================] - 0s 37ms/step - loss: 0.0102 - root_mean_squared_error: 0.1010 - val_loss: 0.0120 - val_root_mean_squared_error: 0.1095\n",
      "Epoch 64/200\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.0070 - root_mean_squared_error: 0.0836 - val_loss: 0.0128 - val_root_mean_squared_error: 0.1129\n",
      "Epoch 65/200\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 0.0079 - root_mean_squared_error: 0.0891 - val_loss: 0.0114 - val_root_mean_squared_error: 0.1066\n",
      "Epoch 66/200\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 0.0081 - root_mean_squared_error: 0.0900 - val_loss: 0.0112 - val_root_mean_squared_error: 0.1057\n",
      "Epoch 67/200\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 0.0068 - root_mean_squared_error: 0.0825 - val_loss: 0.0120 - val_root_mean_squared_error: 0.1097\n",
      "Epoch 68/200\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 0.0090 - root_mean_squared_error: 0.0947 - val_loss: 0.0116 - val_root_mean_squared_error: 0.1075\n",
      "Epoch 69/200\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 0.0085 - root_mean_squared_error: 0.0923 - val_loss: 0.0162 - val_root_mean_squared_error: 0.1272\n",
      "Epoch 70/200\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 0.0090 - root_mean_squared_error: 0.0948 - val_loss: 0.0181 - val_root_mean_squared_error: 0.1344\n",
      "Epoch 71/200\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 0.0081 - root_mean_squared_error: 0.0902 - val_loss: 0.0132 - val_root_mean_squared_error: 0.1150\n",
      "Epoch 72/200\n",
      "11/11 [==============================] - 0s 27ms/step - loss: 0.0084 - root_mean_squared_error: 0.0919 - val_loss: 0.0228 - val_root_mean_squared_error: 0.1511\n",
      "Epoch 73/200\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 0.0094 - root_mean_squared_error: 0.0971 - val_loss: 0.0136 - val_root_mean_squared_error: 0.1168\n",
      "Epoch 74/200\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 0.0072 - root_mean_squared_error: 0.0850 - val_loss: 0.0115 - val_root_mean_squared_error: 0.1070\n",
      "Epoch 75/200\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 0.0066 - root_mean_squared_error: 0.0813 - val_loss: 0.0113 - val_root_mean_squared_error: 0.1063\n",
      "Epoch 76/200\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 0.0056 - root_mean_squared_error: 0.0750 - val_loss: 0.0111 - val_root_mean_squared_error: 0.1052\n",
      "Epoch 77/200\n",
      "11/11 [==============================] - 0s 27ms/step - loss: 0.0063 - root_mean_squared_error: 0.0795 - val_loss: 0.0118 - val_root_mean_squared_error: 0.1084\n",
      "Epoch 78/200\n",
      "11/11 [==============================] - 0s 27ms/step - loss: 0.0071 - root_mean_squared_error: 0.0842 - val_loss: 0.0137 - val_root_mean_squared_error: 0.1169\n",
      "Epoch 79/200\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 0.0081 - root_mean_squared_error: 0.0897 - val_loss: 0.0132 - val_root_mean_squared_error: 0.1149\n",
      "Epoch 80/200\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 0.0070 - root_mean_squared_error: 0.0837 - val_loss: 0.0152 - val_root_mean_squared_error: 0.1231\n",
      "Epoch 81/200\n",
      "11/11 [==============================] - 0s 27ms/step - loss: 0.0080 - root_mean_squared_error: 0.0893 - val_loss: 0.0123 - val_root_mean_squared_error: 0.1110\n",
      "Epoch 82/200\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 0.0096 - root_mean_squared_error: 0.0979 - val_loss: 0.0175 - val_root_mean_squared_error: 0.1324\n",
      "Epoch 83/200\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 0.0085 - root_mean_squared_error: 0.0920 - val_loss: 0.0154 - val_root_mean_squared_error: 0.1243\n",
      "Epoch 84/200\n",
      "11/11 [==============================] - 0s 32ms/step - loss: 0.0074 - root_mean_squared_error: 0.0862 - val_loss: 0.0127 - val_root_mean_squared_error: 0.1127\n",
      "Epoch 85/200\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 0.0081 - root_mean_squared_error: 0.0900 - val_loss: 0.0117 - val_root_mean_squared_error: 0.1081\n",
      "Epoch 86/200\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 0.0074 - root_mean_squared_error: 0.0857 - val_loss: 0.0105 - val_root_mean_squared_error: 0.1024\n",
      "Epoch 87/200\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 0.0067 - root_mean_squared_error: 0.0816 - val_loss: 0.0112 - val_root_mean_squared_error: 0.1056\n",
      "Epoch 88/200\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 0.0060 - root_mean_squared_error: 0.0777 - val_loss: 0.0112 - val_root_mean_squared_error: 0.1056\n",
      "Epoch 89/200\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 0.0072 - root_mean_squared_error: 0.0849 - val_loss: 0.0128 - val_root_mean_squared_error: 0.1133\n",
      "Epoch 90/200\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 0.0065 - root_mean_squared_error: 0.0808 - val_loss: 0.0108 - val_root_mean_squared_error: 0.1039\n",
      "Epoch 91/200\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 0.0061 - root_mean_squared_error: 0.0783 - val_loss: 0.0111 - val_root_mean_squared_error: 0.1053\n",
      "Epoch 92/200\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 0.0056 - root_mean_squared_error: 0.0750 - val_loss: 0.0110 - val_root_mean_squared_error: 0.1046\n",
      "Epoch 93/200\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 0.0063 - root_mean_squared_error: 0.0791 - val_loss: 0.0112 - val_root_mean_squared_error: 0.1061\n",
      "Epoch 94/200\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 0.0061 - root_mean_squared_error: 0.0781 - val_loss: 0.0110 - val_root_mean_squared_error: 0.1050\n",
      "Epoch 95/200\n",
      "11/11 [==============================] - 0s 27ms/step - loss: 0.0072 - root_mean_squared_error: 0.0851 - val_loss: 0.0123 - val_root_mean_squared_error: 0.1108\n",
      "Epoch 96/200\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 0.0080 - root_mean_squared_error: 0.0893 - val_loss: 0.0112 - val_root_mean_squared_error: 0.1060\n",
      "Epoch 97/200\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 0.0072 - root_mean_squared_error: 0.0851 - val_loss: 0.0110 - val_root_mean_squared_error: 0.1050\n",
      "Epoch 98/200\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 0.0067 - root_mean_squared_error: 0.0818 - val_loss: 0.0118 - val_root_mean_squared_error: 0.1086\n",
      "Epoch 99/200\n",
      "11/11 [==============================] - 0s 27ms/step - loss: 0.0070 - root_mean_squared_error: 0.0835 - val_loss: 0.0114 - val_root_mean_squared_error: 0.1067\n",
      "Epoch 100/200\n",
      "11/11 [==============================] - 0s 27ms/step - loss: 0.0068 - root_mean_squared_error: 0.0827 - val_loss: 0.0119 - val_root_mean_squared_error: 0.1090\n",
      "Epoch 101/200\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 0.0058 - root_mean_squared_error: 0.0760 - val_loss: 0.0112 - val_root_mean_squared_error: 0.1056\n",
      "Epoch 102/200\n",
      "11/11 [==============================] - 0s 27ms/step - loss: 0.0066 - root_mean_squared_error: 0.0812 - val_loss: 0.0105 - val_root_mean_squared_error: 0.1025\n",
      "Epoch 103/200\n",
      "11/11 [==============================] - 0s 27ms/step - loss: 0.0070 - root_mean_squared_error: 0.0834 - val_loss: 0.0106 - val_root_mean_squared_error: 0.1032\n",
      "Epoch 104/200\n",
      "11/11 [==============================] - 0s 27ms/step - loss: 0.0072 - root_mean_squared_error: 0.0847 - val_loss: 0.0110 - val_root_mean_squared_error: 0.1050\n",
      "Epoch 105/200\n",
      "11/11 [==============================] - 0s 27ms/step - loss: 0.0074 - root_mean_squared_error: 0.0859 - val_loss: 0.0126 - val_root_mean_squared_error: 0.1123\n",
      "Epoch 106/200\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 0.0089 - root_mean_squared_error: 0.0941 - val_loss: 0.0127 - val_root_mean_squared_error: 0.1126\n",
      "Epoch 107/200\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 0.0102 - root_mean_squared_error: 0.1011 - val_loss: 0.0137 - val_root_mean_squared_error: 0.1171\n",
      "Epoch 108/200\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 0.0073 - root_mean_squared_error: 0.0853 - val_loss: 0.0130 - val_root_mean_squared_error: 0.1142\n",
      "Epoch 109/200\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 0.0061 - root_mean_squared_error: 0.0784 - val_loss: 0.0118 - val_root_mean_squared_error: 0.1085\n",
      "Epoch 110/200\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 0.0073 - root_mean_squared_error: 0.0855 - val_loss: 0.0153 - val_root_mean_squared_error: 0.1236\n",
      "Epoch 111/200\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 0.0071 - root_mean_squared_error: 0.0843 - val_loss: 0.0117 - val_root_mean_squared_error: 0.1084\n",
      "Epoch 112/200\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 0.0083 - root_mean_squared_error: 0.0911 - val_loss: 0.0100 - val_root_mean_squared_error: 0.1001\n",
      "Epoch 113/200\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 0.0081 - root_mean_squared_error: 0.0902 - val_loss: 0.0120 - val_root_mean_squared_error: 0.1097\n",
      "Epoch 114/200\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 0.0094 - root_mean_squared_error: 0.0969 - val_loss: 0.0149 - val_root_mean_squared_error: 0.1220\n",
      "Epoch 115/200\n",
      "11/11 [==============================] - 0s 27ms/step - loss: 0.0123 - root_mean_squared_error: 0.1107 - val_loss: 0.0147 - val_root_mean_squared_error: 0.1212\n",
      "Epoch 116/200\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 0.0098 - root_mean_squared_error: 0.0988 - val_loss: 0.0110 - val_root_mean_squared_error: 0.1051\n",
      "Epoch 117/200\n",
      "11/11 [==============================] - 0s 27ms/step - loss: 0.0059 - root_mean_squared_error: 0.0767 - val_loss: 0.0107 - val_root_mean_squared_error: 0.1035\n",
      "Epoch 118/200\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 0.0064 - root_mean_squared_error: 0.0802 - val_loss: 0.0107 - val_root_mean_squared_error: 0.1036\n",
      "Epoch 119/200\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 0.0077 - root_mean_squared_error: 0.0876 - val_loss: 0.0102 - val_root_mean_squared_error: 0.1009\n",
      "Epoch 120/200\n",
      "11/11 [==============================] - 0s 27ms/step - loss: 0.0080 - root_mean_squared_error: 0.0896 - val_loss: 0.0117 - val_root_mean_squared_error: 0.1083\n",
      "Epoch 121/200\n",
      "11/11 [==============================] - 0s 27ms/step - loss: 0.0073 - root_mean_squared_error: 0.0857 - val_loss: 0.0113 - val_root_mean_squared_error: 0.1065\n",
      "Epoch 122/200\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 0.0073 - root_mean_squared_error: 0.0853 - val_loss: 0.0120 - val_root_mean_squared_error: 0.1095\n",
      "Epoch 123/200\n",
      "11/11 [==============================] - 0s 27ms/step - loss: 0.0070 - root_mean_squared_error: 0.0836 - val_loss: 0.0111 - val_root_mean_squared_error: 0.1051\n",
      "Epoch 124/200\n",
      "11/11 [==============================] - 0s 27ms/step - loss: 0.0063 - root_mean_squared_error: 0.0793 - val_loss: 0.0117 - val_root_mean_squared_error: 0.1080\n",
      "Epoch 125/200\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 0.0063 - root_mean_squared_error: 0.0791 - val_loss: 0.0150 - val_root_mean_squared_error: 0.1224\n",
      "Epoch 126/200\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 0.0081 - root_mean_squared_error: 0.0898 - val_loss: 0.0115 - val_root_mean_squared_error: 0.1075\n",
      "Epoch 127/200\n",
      "11/11 [==============================] - 0s 31ms/step - loss: 0.0079 - root_mean_squared_error: 0.0891 - val_loss: 0.0102 - val_root_mean_squared_error: 0.1008\n",
      "Epoch 128/200\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 0.0090 - root_mean_squared_error: 0.0949 - val_loss: 0.0107 - val_root_mean_squared_error: 0.1032\n",
      "Epoch 129/200\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 0.0072 - root_mean_squared_error: 0.0848 - val_loss: 0.0099 - val_root_mean_squared_error: 0.0996\n",
      "Epoch 130/200\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 0.0069 - root_mean_squared_error: 0.0832 - val_loss: 0.0112 - val_root_mean_squared_error: 0.1058\n",
      "Epoch 131/200\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 0.0060 - root_mean_squared_error: 0.0778 - val_loss: 0.0100 - val_root_mean_squared_error: 0.0998\n",
      "Epoch 132/200\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 0.0070 - root_mean_squared_error: 0.0836 - val_loss: 0.0107 - val_root_mean_squared_error: 0.1034\n",
      "Epoch 133/200\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 0.0076 - root_mean_squared_error: 0.0874 - val_loss: 0.0109 - val_root_mean_squared_error: 0.1042\n",
      "Epoch 134/200\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 0.0071 - root_mean_squared_error: 0.0843 - val_loss: 0.0111 - val_root_mean_squared_error: 0.1053\n",
      "Epoch 135/200\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 0.0060 - root_mean_squared_error: 0.0777 - val_loss: 0.0113 - val_root_mean_squared_error: 0.1063\n",
      "Epoch 136/200\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 0.0095 - root_mean_squared_error: 0.0977 - val_loss: 0.0211 - val_root_mean_squared_error: 0.1451\n",
      "Epoch 137/200\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 0.0118 - root_mean_squared_error: 0.1086 - val_loss: 0.0136 - val_root_mean_squared_error: 0.1166\n",
      "Epoch 138/200\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 0.0076 - root_mean_squared_error: 0.0872 - val_loss: 0.0104 - val_root_mean_squared_error: 0.1022\n",
      "Epoch 139/200\n",
      "11/11 [==============================] - 0s 27ms/step - loss: 0.0079 - root_mean_squared_error: 0.0887 - val_loss: 0.0100 - val_root_mean_squared_error: 0.1002\n",
      "Epoch 140/200\n",
      "11/11 [==============================] - 0s 27ms/step - loss: 0.0058 - root_mean_squared_error: 0.0762 - val_loss: 0.0099 - val_root_mean_squared_error: 0.0993\n",
      "Epoch 141/200\n",
      "11/11 [==============================] - 0s 27ms/step - loss: 0.0059 - root_mean_squared_error: 0.0766 - val_loss: 0.0115 - val_root_mean_squared_error: 0.1074\n",
      "Epoch 142/200\n",
      "11/11 [==============================] - 0s 27ms/step - loss: 0.0067 - root_mean_squared_error: 0.0816 - val_loss: 0.0160 - val_root_mean_squared_error: 0.1263\n",
      "Epoch 143/200\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 0.0091 - root_mean_squared_error: 0.0954 - val_loss: 0.0106 - val_root_mean_squared_error: 0.1028\n",
      "Epoch 144/200\n",
      "11/11 [==============================] - 0s 27ms/step - loss: 0.0061 - root_mean_squared_error: 0.0783 - val_loss: 0.0102 - val_root_mean_squared_error: 0.1009\n",
      "Epoch 145/200\n",
      "11/11 [==============================] - 0s 27ms/step - loss: 0.0064 - root_mean_squared_error: 0.0799 - val_loss: 0.0111 - val_root_mean_squared_error: 0.1052\n",
      "Epoch 146/200\n",
      "11/11 [==============================] - 0s 27ms/step - loss: 0.0057 - root_mean_squared_error: 0.0754 - val_loss: 0.0123 - val_root_mean_squared_error: 0.1108\n",
      "Epoch 147/200\n",
      "11/11 [==============================] - 0s 27ms/step - loss: 0.0058 - root_mean_squared_error: 0.0764 - val_loss: 0.0109 - val_root_mean_squared_error: 0.1043\n",
      "Epoch 148/200\n",
      "11/11 [==============================] - 0s 27ms/step - loss: 0.0061 - root_mean_squared_error: 0.0781 - val_loss: 0.0104 - val_root_mean_squared_error: 0.1018\n",
      "Epoch 149/200\n",
      "11/11 [==============================] - 0s 27ms/step - loss: 0.0047 - root_mean_squared_error: 0.0686 - val_loss: 0.0104 - val_root_mean_squared_error: 0.1021\n",
      "Epoch 150/200\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 0.0064 - root_mean_squared_error: 0.0803 - val_loss: 0.0101 - val_root_mean_squared_error: 0.1004\n",
      "Epoch 151/200\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 0.0075 - root_mean_squared_error: 0.0865 - val_loss: 0.0096 - val_root_mean_squared_error: 0.0982\n",
      "Epoch 152/200\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 0.0061 - root_mean_squared_error: 0.0783 - val_loss: 0.0123 - val_root_mean_squared_error: 0.1107\n",
      "Epoch 153/200\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 0.0054 - root_mean_squared_error: 0.0736 - val_loss: 0.0097 - val_root_mean_squared_error: 0.0983\n",
      "Epoch 154/200\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 0.0064 - root_mean_squared_error: 0.0801 - val_loss: 0.0097 - val_root_mean_squared_error: 0.0987\n",
      "Epoch 155/200\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 0.0052 - root_mean_squared_error: 0.0721 - val_loss: 0.0110 - val_root_mean_squared_error: 0.1051\n",
      "Epoch 156/200\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 0.0058 - root_mean_squared_error: 0.0760 - val_loss: 0.0103 - val_root_mean_squared_error: 0.1016\n",
      "Epoch 157/200\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 0.0069 - root_mean_squared_error: 0.0828 - val_loss: 0.0107 - val_root_mean_squared_error: 0.1036\n",
      "Epoch 158/200\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 0.0046 - root_mean_squared_error: 0.0679 - val_loss: 0.0104 - val_root_mean_squared_error: 0.1019\n",
      "Epoch 159/200\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 0.0064 - root_mean_squared_error: 0.0802 - val_loss: 0.0127 - val_root_mean_squared_error: 0.1129\n",
      "Epoch 160/200\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 0.0061 - root_mean_squared_error: 0.0779 - val_loss: 0.0107 - val_root_mean_squared_error: 0.1034\n",
      "Epoch 161/200\n",
      "11/11 [==============================] - 0s 27ms/step - loss: 0.0074 - root_mean_squared_error: 0.0863 - val_loss: 0.0101 - val_root_mean_squared_error: 0.1004\n",
      "Epoch 162/200\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 0.0057 - root_mean_squared_error: 0.0757 - val_loss: 0.0102 - val_root_mean_squared_error: 0.1011\n",
      "Epoch 163/200\n",
      "11/11 [==============================] - 0s 27ms/step - loss: 0.0054 - root_mean_squared_error: 0.0736 - val_loss: 0.0117 - val_root_mean_squared_error: 0.1084\n",
      "Epoch 164/200\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 0.0059 - root_mean_squared_error: 0.0770 - val_loss: 0.0126 - val_root_mean_squared_error: 0.1120\n",
      "Epoch 165/200\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 0.0055 - root_mean_squared_error: 0.0744 - val_loss: 0.0105 - val_root_mean_squared_error: 0.1024\n",
      "Epoch 166/200\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 0.0052 - root_mean_squared_error: 0.0723 - val_loss: 0.0105 - val_root_mean_squared_error: 0.1024\n",
      "Epoch 167/200\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 0.0056 - root_mean_squared_error: 0.0747 - val_loss: 0.0107 - val_root_mean_squared_error: 0.1033\n",
      "Epoch 168/200\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 0.0056 - root_mean_squared_error: 0.0752 - val_loss: 0.0107 - val_root_mean_squared_error: 0.1035\n",
      "Epoch 169/200\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 0.0057 - root_mean_squared_error: 0.0758 - val_loss: 0.0106 - val_root_mean_squared_error: 0.1030\n",
      "Epoch 170/200\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 0.0074 - root_mean_squared_error: 0.0859 - val_loss: 0.0109 - val_root_mean_squared_error: 0.1043\n",
      "Epoch 171/200\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 0.0090 - root_mean_squared_error: 0.0948 - val_loss: 0.0121 - val_root_mean_squared_error: 0.1098\n",
      "Epoch 172/200\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 0.0061 - root_mean_squared_error: 0.0781 - val_loss: 0.0101 - val_root_mean_squared_error: 0.1006\n",
      "Epoch 173/200\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 0.0046 - root_mean_squared_error: 0.0681 - val_loss: 0.0103 - val_root_mean_squared_error: 0.1013\n",
      "Epoch 174/200\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 0.0059 - root_mean_squared_error: 0.0770 - val_loss: 0.0103 - val_root_mean_squared_error: 0.1013\n",
      "Epoch 175/200\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 0.0069 - root_mean_squared_error: 0.0832 - val_loss: 0.0149 - val_root_mean_squared_error: 0.1220\n",
      "Epoch 176/200\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 0.0065 - root_mean_squared_error: 0.0806 - val_loss: 0.0166 - val_root_mean_squared_error: 0.1287\n",
      "Epoch 177/200\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 0.0069 - root_mean_squared_error: 0.0828 - val_loss: 0.0102 - val_root_mean_squared_error: 0.1008\n",
      "Epoch 178/200\n",
      "11/11 [==============================] - 0s 27ms/step - loss: 0.0054 - root_mean_squared_error: 0.0736 - val_loss: 0.0113 - val_root_mean_squared_error: 0.1061\n",
      "Epoch 179/200\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 0.0053 - root_mean_squared_error: 0.0730 - val_loss: 0.0105 - val_root_mean_squared_error: 0.1026\n",
      "Epoch 180/200\n",
      "11/11 [==============================] - 0s 31ms/step - loss: 0.0057 - root_mean_squared_error: 0.0754 - val_loss: 0.0102 - val_root_mean_squared_error: 0.1011\n",
      "Epoch 181/200\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 0.0071 - root_mean_squared_error: 0.0844 - val_loss: 0.0173 - val_root_mean_squared_error: 0.1313\n",
      "Epoch 182/200\n",
      "11/11 [==============================] - 0s 27ms/step - loss: 0.0075 - root_mean_squared_error: 0.0867 - val_loss: 0.0111 - val_root_mean_squared_error: 0.1054\n",
      "Epoch 183/200\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 0.0060 - root_mean_squared_error: 0.0772 - val_loss: 0.0129 - val_root_mean_squared_error: 0.1136\n",
      "Epoch 184/200\n",
      "11/11 [==============================] - 0s 27ms/step - loss: 0.0062 - root_mean_squared_error: 0.0787 - val_loss: 0.0107 - val_root_mean_squared_error: 0.1034\n",
      "Epoch 185/200\n",
      "11/11 [==============================] - 0s 27ms/step - loss: 0.0046 - root_mean_squared_error: 0.0681 - val_loss: 0.0104 - val_root_mean_squared_error: 0.1022\n",
      "Epoch 186/200\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 0.0050 - root_mean_squared_error: 0.0708 - val_loss: 0.0105 - val_root_mean_squared_error: 0.1024\n",
      "Epoch 187/200\n",
      "11/11 [==============================] - 0s 27ms/step - loss: 0.0048 - root_mean_squared_error: 0.0695 - val_loss: 0.0097 - val_root_mean_squared_error: 0.0985\n",
      "Epoch 188/200\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 0.0053 - root_mean_squared_error: 0.0726 - val_loss: 0.0099 - val_root_mean_squared_error: 0.0994\n",
      "Epoch 189/200\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 0.0048 - root_mean_squared_error: 0.0696 - val_loss: 0.0101 - val_root_mean_squared_error: 0.1006\n",
      "Epoch 190/200\n",
      "11/11 [==============================] - 0s 27ms/step - loss: 0.0056 - root_mean_squared_error: 0.0747 - val_loss: 0.0119 - val_root_mean_squared_error: 0.1092\n",
      "Epoch 191/200\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 0.0061 - root_mean_squared_error: 0.0778 - val_loss: 0.0104 - val_root_mean_squared_error: 0.1019\n",
      "Epoch 192/200\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 0.0050 - root_mean_squared_error: 0.0704 - val_loss: 0.0111 - val_root_mean_squared_error: 0.1055\n",
      "Epoch 193/200\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 0.0057 - root_mean_squared_error: 0.0753 - val_loss: 0.0100 - val_root_mean_squared_error: 0.1000\n",
      "Epoch 194/200\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 0.0040 - root_mean_squared_error: 0.0634 - val_loss: 0.0100 - val_root_mean_squared_error: 0.1001\n",
      "Epoch 195/200\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 0.0055 - root_mean_squared_error: 0.0745 - val_loss: 0.0131 - val_root_mean_squared_error: 0.1143\n",
      "Epoch 196/200\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 0.0065 - root_mean_squared_error: 0.0807 - val_loss: 0.0117 - val_root_mean_squared_error: 0.1080\n",
      "Epoch 197/200\n",
      "11/11 [==============================] - 0s 31ms/step - loss: 0.0064 - root_mean_squared_error: 0.0802 - val_loss: 0.0108 - val_root_mean_squared_error: 0.1041\n",
      "Epoch 198/200\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 0.0055 - root_mean_squared_error: 0.0742 - val_loss: 0.0098 - val_root_mean_squared_error: 0.0992\n",
      "Epoch 199/200\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 0.0065 - root_mean_squared_error: 0.0809 - val_loss: 0.0124 - val_root_mean_squared_error: 0.1115\n",
      "Epoch 200/200\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 0.0048 - root_mean_squared_error: 0.0693 - val_loss: 0.0096 - val_root_mean_squared_error: 0.0980\n",
      "mae: tf.Tensor(33.636, shape=(), dtype=float32)\n",
      "mse: tf.Tensor(2514.0896, shape=(), dtype=float32)\n",
      "rmse: tf.Tensor(50.140697, shape=(), dtype=float32)\n",
      "mae: tf.Tensor(33.636, shape=(), dtype=float32)\n",
      "mse: tf.Tensor(2514.0896, shape=(), dtype=float32)\n",
      "rmse: tf.Tensor(50.140697, shape=(), dtype=float32)\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 90 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 90 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://app.neptune.ai/milestone2-california-water-shortage/deeplearning-lstm/e/DEEPLSTM-141\n"
     ]
    }
   ],
   "source": [
    "run = neptune.init(\n",
    "    project=\"milestone2-california-water-shortage/deeplearning-lstm\",\n",
    "    api_token=neptune_key,\n",
    "    name=\"Advanced Model 1\",\n",
    "    tags=[\"WithDetailedWellCounts\", \"BidirectionalLSTM\"]\n",
    ")\n",
    "neptune_cbk = NeptuneCallback(run=run, base_namespace='metrics')\n",
    "run['hyper-parameters'] = m2_hyper_parameters\n",
    "\n",
    "model2.compile(loss=\"mse\", optimizer=m2_optimizer[m2_hyper_parameters[\"optimizer\"]], metrics=[keras.metrics.RootMeanSquaredError()])\n",
    "model2.fit(X_train, y_train,\n",
    "                     validation_split=m2_hyper_parameters[\"validation_split\"],\n",
    "                     batch_size=m2_hyper_parameters[\"batch_size\"],\n",
    "                     epochs=m2_hyper_parameters[\"epochs\"],\n",
    "                     shuffle=True,\n",
    "                     callbacks=[neptune_cbk])\n",
    "yhat = model2.predict(X_test, verbose=0)\n",
    "yhat_inverse = scaler.inverse_transform(yhat)\n",
    "y_test_inverse = scaler.inverse_transform(y_test)\n",
    "evaluate_forecast(y_test_inverse, yhat_inverse)\n",
    "mae, mse, rmse = evaluate_forecast(y_test_inverse, yhat_inverse)\n",
    "run[\"eval/mae\"] = mae\n",
    "run[\"eval/mse\"] = mse\n",
    "run[\"eval/rmse\"] = rmse\n",
    "run.stop()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model3 Hyper-parameter tuning\n",
    "* a simple or bidirectional encoding LSTM layer\n",
    "* a simple or bidirectional decoding LSTM layer\n",
    "* a Dense unit\n",
    "* a Dropout Unit"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "class Model3(keras_tuner.HyperModel):\n",
    "    def build(self, hp):\n",
    "        model = Sequential()\n",
    "        lstm_units = hp.Int(\"lstm_units\", min_value=20, max_value=300, step=20)\n",
    "        lstm_activ = hp.Choice(\"lstm_activation\", values=[\"relu\", \"tanh\", \"sigmoid\"])\n",
    "        model.add(LSTM(units=lstm_units, activation=lstm_activ, input_shape=(7, nb_features)))\n",
    "        model.add(RepeatVector(1))\n",
    "        lstm_units_2 = hp.Int(\"2nd_lstm_units\", min_value=20, max_value=300, step=20)\n",
    "        lstm_activ_2 = hp.Choice(\"2nd_lstm_activation\", values=[\"relu\", \"tanh\", \"sigmoid\"])\n",
    "        model.add(LSTM(units=lstm_units_2, activation=lstm_activ_2, return_sequences=True))\n",
    "        dense_units = hp.Int(\"dense_units\", min_value=11, max_value=101, step=10)\n",
    "        dense_activation = hp.Choice(\"dense_activation\", values=[\"relu\", \"tanh\", \"sigmoid\"])\n",
    "        model.add(TimeDistributed(Dense(dense_units, activation=dense_activation)))\n",
    "        hp_dropout = hp.Float(\"dropout_rate\", min_value=0.05, max_value=0.25, step=0.05)\n",
    "        model.add(Dropout(hp_dropout))\n",
    "        model.add(Dense(1, activation=output_activation))\n",
    "\n",
    "        hp_learning_rate = hp.Choice(\"learning_rate\", values=[1e-2, 1e-3, 1e-4])\n",
    "        model.compile(loss=\"mse\", optimizer=Adam(learning_rate=hp_learning_rate), metrics=[keras.metrics.RootMeanSquaredError()])\n",
    "\n",
    "        return model\n",
    "\n",
    "    def fit(self, hp, model, *args, **kwargs):\n",
    "        return model.fit(\n",
    "            *args,\n",
    "            validation_split=hp.Float(\"validation_split\", min_value=0.05, max_value=0.2, step=0.05),\n",
    "            batch_size=hp.Int(\"batch_size\", min_value=32, max_value=192, step=32),\n",
    "            #epochs=hp.Int(\"epochs\", min_value=50, max_value=400, step=50),\n",
    "            **kwargs,\n",
    "        )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "stop_early = tensorflow.keras.callbacks.EarlyStopping(monitor=\"val_root_mean_squared_error\", patience=7, verbose=1)\n",
    "tuner = keras_tuner.Hyperband(Model3(),\n",
    "                              objective=keras_tuner.Objective(\"val_root_mean_squared_error\", direction=\"min\"),\n",
    "                              max_epochs=200000,\n",
    "                              factor=2000,\n",
    "                              overwrite=True,\n",
    "                              directory=\"keras_tuner\",\n",
    "                              project_name=\"keras_tuner\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1109 Complete [00h 00m 55s]\n",
      "val_root_mean_squared_error: 0.08242339640855789\n",
      "\n",
      "Best val_root_mean_squared_error So Far: 0.07807664573192596\n",
      "Total elapsed time: 05h 08m 35s\n",
      "\n",
      "Search: Running Trial #1110\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "280               |300               |lstm_units\n",
      "relu              |sigmoid           |lstm_activation\n",
      "280               |140               |2nd_lstm_units\n",
      "tanh              |tanh              |2nd_lstm_activation\n",
      "71                |21                |dense_units\n",
      "sigmoid           |tanh              |dense_activation\n",
      "0.2               |0.2               |dropout_rate\n",
      "0.0001            |0.001             |learning_rate\n",
      "0.05              |0.1               |validation_split\n",
      "32                |32                |batch_size\n",
      "100               |100               |tuner/epochs\n",
      "0                 |0                 |tuner/initial_epoch\n",
      "1                 |1                 |tuner/bracket\n",
      "0                 |0                 |tuner/round\n",
      "\n",
      "Epoch 1/100\n",
      "13/13 [==============================] - 4s 158ms/step - loss: 0.4688 - root_mean_squared_error: 0.6847 - val_loss: 0.4293 - val_root_mean_squared_error: 0.6552\n",
      "Epoch 2/100\n",
      "13/13 [==============================] - 2s 138ms/step - loss: 0.4196 - root_mean_squared_error: 0.6478 - val_loss: 0.3394 - val_root_mean_squared_error: 0.5825\n",
      "Epoch 3/100\n",
      "13/13 [==============================] - 2s 180ms/step - loss: 0.3454 - root_mean_squared_error: 0.5877 - val_loss: 0.2436 - val_root_mean_squared_error: 0.4936\n",
      "Epoch 4/100\n",
      "13/13 [==============================] - 2s 155ms/step - loss: 0.2649 - root_mean_squared_error: 0.5147 - val_loss: 0.1206 - val_root_mean_squared_error: 0.3473\n",
      "Epoch 5/100\n",
      "13/13 [==============================] - 1s 110ms/step - loss: 0.1653 - root_mean_squared_error: 0.4065 - val_loss: 0.0369 - val_root_mean_squared_error: 0.1922\n",
      "Epoch 6/100\n",
      "13/13 [==============================] - 2s 142ms/step - loss: 0.1532 - root_mean_squared_error: 0.3914 - val_loss: 0.0362 - val_root_mean_squared_error: 0.1902\n",
      "Epoch 7/100\n",
      "13/13 [==============================] - 2s 162ms/step - loss: 0.1451 - root_mean_squared_error: 0.3809 - val_loss: 0.0414 - val_root_mean_squared_error: 0.2035\n",
      "Epoch 8/100\n",
      "13/13 [==============================] - 2s 132ms/step - loss: 0.1423 - root_mean_squared_error: 0.3772 - val_loss: 0.0361 - val_root_mean_squared_error: 0.1900\n",
      "Epoch 9/100\n",
      "13/13 [==============================] - 2s 115ms/step - loss: 0.1472 - root_mean_squared_error: 0.3836 - val_loss: 0.0342 - val_root_mean_squared_error: 0.1850\n",
      "Epoch 10/100\n",
      "13/13 [==============================] - 2s 165ms/step - loss: 0.1332 - root_mean_squared_error: 0.3650 - val_loss: 0.0319 - val_root_mean_squared_error: 0.1787\n",
      "Epoch 11/100\n",
      "13/13 [==============================] - 2s 190ms/step - loss: 0.1151 - root_mean_squared_error: 0.3392 - val_loss: 0.0319 - val_root_mean_squared_error: 0.1787\n",
      "Epoch 12/100\n",
      "13/13 [==============================] - 2s 152ms/step - loss: 0.1333 - root_mean_squared_error: 0.3651 - val_loss: 0.0302 - val_root_mean_squared_error: 0.1739\n",
      "Epoch 13/100\n",
      "13/13 [==============================] - 1s 98ms/step - loss: 0.1491 - root_mean_squared_error: 0.3861 - val_loss: 0.0307 - val_root_mean_squared_error: 0.1752\n",
      "Epoch 14/100\n",
      "13/13 [==============================] - 2s 138ms/step - loss: 0.1313 - root_mean_squared_error: 0.3624 - val_loss: 0.0327 - val_root_mean_squared_error: 0.1809\n",
      "Epoch 15/100\n",
      "13/13 [==============================] - 3s 199ms/step - loss: 0.1274 - root_mean_squared_error: 0.3569 - val_loss: 0.0299 - val_root_mean_squared_error: 0.1728\n",
      "Epoch 16/100\n",
      "13/13 [==============================] - 2s 126ms/step - loss: 0.1245 - root_mean_squared_error: 0.3528 - val_loss: 0.0297 - val_root_mean_squared_error: 0.1723\n",
      "Epoch 17/100\n",
      "13/13 [==============================] - 2s 120ms/step - loss: 0.1269 - root_mean_squared_error: 0.3563 - val_loss: 0.0280 - val_root_mean_squared_error: 0.1673\n",
      "Epoch 18/100\n",
      "13/13 [==============================] - 2s 183ms/step - loss: 0.1130 - root_mean_squared_error: 0.3362 - val_loss: 0.0286 - val_root_mean_squared_error: 0.1691\n",
      "Epoch 19/100\n",
      "13/13 [==============================] - 3s 193ms/step - loss: 0.1156 - root_mean_squared_error: 0.3400 - val_loss: 0.0276 - val_root_mean_squared_error: 0.1662\n",
      "Epoch 20/100\n",
      "13/13 [==============================] - 1s 94ms/step - loss: 0.1168 - root_mean_squared_error: 0.3417 - val_loss: 0.0284 - val_root_mean_squared_error: 0.1686\n",
      "Epoch 21/100\n",
      "13/13 [==============================] - 1s 100ms/step - loss: 0.1122 - root_mean_squared_error: 0.3350 - val_loss: 0.0279 - val_root_mean_squared_error: 0.1671\n",
      "Epoch 22/100\n",
      "13/13 [==============================] - 2s 182ms/step - loss: 0.1031 - root_mean_squared_error: 0.3210 - val_loss: 0.0276 - val_root_mean_squared_error: 0.1662\n",
      "Epoch 23/100\n",
      "13/13 [==============================] - 2s 180ms/step - loss: 0.0974 - root_mean_squared_error: 0.3121 - val_loss: 0.0269 - val_root_mean_squared_error: 0.1641\n",
      "Epoch 24/100\n",
      "13/13 [==============================] - 1s 111ms/step - loss: 0.1020 - root_mean_squared_error: 0.3194 - val_loss: 0.0257 - val_root_mean_squared_error: 0.1602\n",
      "Epoch 25/100\n",
      "13/13 [==============================] - 1s 109ms/step - loss: 0.0912 - root_mean_squared_error: 0.3020 - val_loss: 0.0295 - val_root_mean_squared_error: 0.1719\n",
      "Epoch 26/100\n",
      "13/13 [==============================] - 2s 160ms/step - loss: 0.0877 - root_mean_squared_error: 0.2962 - val_loss: 0.0258 - val_root_mean_squared_error: 0.1607\n",
      "Epoch 27/100\n",
      "13/13 [==============================] - 2s 159ms/step - loss: 0.0694 - root_mean_squared_error: 0.2634 - val_loss: 0.0230 - val_root_mean_squared_error: 0.1518\n",
      "Epoch 28/100\n",
      "13/13 [==============================] - 1s 104ms/step - loss: 0.0592 - root_mean_squared_error: 0.2434 - val_loss: 0.0236 - val_root_mean_squared_error: 0.1537\n",
      "Epoch 29/100\n",
      "13/13 [==============================] - 1s 99ms/step - loss: 0.0595 - root_mean_squared_error: 0.2440 - val_loss: 0.0231 - val_root_mean_squared_error: 0.1520\n",
      "Epoch 30/100\n",
      "13/13 [==============================] - 1s 109ms/step - loss: 0.0428 - root_mean_squared_error: 0.2068 - val_loss: 0.0236 - val_root_mean_squared_error: 0.1536\n",
      "Epoch 31/100\n",
      "13/13 [==============================] - 1s 115ms/step - loss: 0.0408 - root_mean_squared_error: 0.2020 - val_loss: 0.0249 - val_root_mean_squared_error: 0.1578\n",
      "Epoch 32/100\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.0363 - root_mean_squared_error: 0.1906"
     ]
    }
   ],
   "source": [
    "tuner.search(X_train, y_train, epochs=200, callbacks=[stop_early])\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(f\"\"\"\n",
    "The hyperparameter search is complete.\n",
    "lstm_units: {best_hps.get('lstm_units')}\n",
    "lstm_activation: {best_hps.get('lstm_activation')}\n",
    "2nd_lstm_units: {best_hps.get('2nd_lstm_units')}\n",
    "2nd_lstm_activation: {best_hps.get('2nd_lstm_activation')}\n",
    "dense_units: {best_hps.get('dense_units')}\n",
    "dense_activation: {best_hps.get('dense_activation')}\n",
    "dropout_rate: {best_hps.get('dropout_rate')}\n",
    "learning_rate: {best_hps.get('learning_rate')}\n",
    "batch_size: {best_hps.get('batch_size')}\n",
    "validation_split: {best_hps.get('validation_split')}\n",
    "\"\"\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model 3 Evaluation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_2 (LSTM)               (None, 200)               225600    \n",
      "                                                                 \n",
      " repeat_vector_1 (RepeatVect  (None, 1, 200)           0         \n",
      " or)                                                             \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 1, 100)            120400    \n",
      "                                                                 \n",
      " time_distributed_1 (TimeDis  (None, 1, 81)            8181      \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 1, 81)             0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1, 1)              82        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 354,263\n",
      "Trainable params: 354,263\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "m3_hyper_parameters = {\n",
    "    \"random_seed\": RANDOM_SEED,\n",
    "    \"test_size\": test_size,\n",
    "    \"validation_split\": 0.1,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"batch_size\": 64,\n",
    "    \"epochs\": 200,\n",
    "    \"lstm_units\": 200,\n",
    "    \"2nd_lstm_units\": 100,\n",
    "    \"lstm_activation\": \"sigmoid\",\n",
    "    \"dense_units\": 81,\n",
    "    \"dense_activation\": \"tanh\",\n",
    "    \"dropout\": 0.2,\n",
    "    \"output_activation\": \"linear\",\n",
    "    \"nb_features\": nb_features,\n",
    "    \"optimizer\": \"Adam\"\n",
    "}\n",
    "\n",
    "m3_optimizer = {\n",
    "    \"RMSprop\": RMSprop(learning_rate=m3_hyper_parameters[\"learning_rate\"]),\n",
    "    \"Adam\": Adam(learning_rate=m3_hyper_parameters[\"learning_rate\"]),\n",
    "    \"Adamax\": Adamax(learning_rate=m3_hyper_parameters[\"learning_rate\"]),\n",
    "    \"Adagrad\": Adagrad(learning_rate=m3_hyper_parameters[\"learning_rate\"])\n",
    "}\n",
    "\n",
    "model3 = Sequential()\n",
    "model3.add(LSTM(m3_hyper_parameters[\"lstm_units\"], activation=m3_hyper_parameters[\"lstm_activation\"], input_shape=(7, nb_features)))\n",
    "model3.add(RepeatVector(1))\n",
    "model3.add(LSTM(m3_hyper_parameters[\"2nd_lstm_units\"], activation=m3_hyper_parameters[\"lstm_activation\"], return_sequences=True))\n",
    "model3.add(TimeDistributed(Dense(m3_hyper_parameters[\"dense_units\"], activation=m3_hyper_parameters[\"dense_activation\"])))\n",
    "model3.add(Dropout(m3_hyper_parameters[\"dropout\"]))\n",
    "model3.add(Dense(1, activation=m3_hyper_parameters[\"output_activation\"]))\n",
    "model3.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/milestone2-california-water-shortage/deeplearning-lstm/e/DEEPLSTM-158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Info (NVML): Not Supported. GPU usage metrics may not be reported. For more information, see https://docs-legacy.neptune.ai/logging-and-managing-experiment-results/logging-experiment-data.html#hardware-consumption \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remember to stop your run once you’ve finished logging your metadata (https://docs.neptune.ai/api-reference/run#.stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.\n",
      "Epoch 1/200\n",
      "6/6 [==============================] - 3s 129ms/step - loss: 4.5125 - root_mean_squared_error: 2.1243 - val_loss: 1.7245 - val_root_mean_squared_error: 1.3132\n",
      "Epoch 2/200\n",
      "6/6 [==============================] - 1s 93ms/step - loss: 0.7826 - root_mean_squared_error: 0.8847 - val_loss: 0.7797 - val_root_mean_squared_error: 0.8830\n",
      "Epoch 3/200\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.5239 - root_mean_squared_error: 0.7238 - val_loss: 0.1825 - val_root_mean_squared_error: 0.4272\n",
      "Epoch 4/200\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0561 - root_mean_squared_error: 0.2368 - val_loss: 0.0279 - val_root_mean_squared_error: 0.1669\n",
      "Epoch 5/200\n",
      "6/6 [==============================] - 0s 65ms/step - loss: 0.0766 - root_mean_squared_error: 0.2767 - val_loss: 0.0405 - val_root_mean_squared_error: 0.2011\n",
      "Epoch 6/200\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0762 - root_mean_squared_error: 0.2761 - val_loss: 0.0242 - val_root_mean_squared_error: 0.1556\n",
      "Epoch 7/200\n",
      "6/6 [==============================] - 0s 65ms/step - loss: 0.0505 - root_mean_squared_error: 0.2246 - val_loss: 0.0303 - val_root_mean_squared_error: 0.1742\n",
      "Epoch 8/200\n",
      "6/6 [==============================] - 0s 65ms/step - loss: 0.0462 - root_mean_squared_error: 0.2148 - val_loss: 0.0378 - val_root_mean_squared_error: 0.1944\n",
      "Epoch 9/200\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0461 - root_mean_squared_error: 0.2148 - val_loss: 0.0371 - val_root_mean_squared_error: 0.1926\n",
      "Epoch 10/200\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0425 - root_mean_squared_error: 0.2061 - val_loss: 0.0321 - val_root_mean_squared_error: 0.1792\n",
      "Epoch 11/200\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0417 - root_mean_squared_error: 0.2042 - val_loss: 0.0280 - val_root_mean_squared_error: 0.1674\n",
      "Epoch 12/200\n",
      "6/6 [==============================] - 0s 66ms/step - loss: 0.0418 - root_mean_squared_error: 0.2044 - val_loss: 0.0262 - val_root_mean_squared_error: 0.1619\n",
      "Epoch 13/200\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0409 - root_mean_squared_error: 0.2023 - val_loss: 0.0257 - val_root_mean_squared_error: 0.1605\n",
      "Epoch 14/200\n",
      "6/6 [==============================] - 0s 66ms/step - loss: 0.0429 - root_mean_squared_error: 0.2072 - val_loss: 0.0261 - val_root_mean_squared_error: 0.1615\n",
      "Epoch 15/200\n",
      "6/6 [==============================] - 0s 66ms/step - loss: 0.0412 - root_mean_squared_error: 0.2031 - val_loss: 0.0269 - val_root_mean_squared_error: 0.1641\n",
      "Epoch 16/200\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0404 - root_mean_squared_error: 0.2010 - val_loss: 0.0274 - val_root_mean_squared_error: 0.1655\n",
      "Epoch 17/200\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0405 - root_mean_squared_error: 0.2012 - val_loss: 0.0273 - val_root_mean_squared_error: 0.1651\n",
      "Epoch 18/200\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.0405 - root_mean_squared_error: 0.2012 - val_loss: 0.0269 - val_root_mean_squared_error: 0.1640\n",
      "Epoch 19/200\n",
      "6/6 [==============================] - 1s 107ms/step - loss: 0.0395 - root_mean_squared_error: 0.1987 - val_loss: 0.0265 - val_root_mean_squared_error: 0.1629\n",
      "Epoch 20/200\n",
      "6/6 [==============================] - 0s 78ms/step - loss: 0.0390 - root_mean_squared_error: 0.1976 - val_loss: 0.0261 - val_root_mean_squared_error: 0.1617\n",
      "Epoch 21/200\n",
      "6/6 [==============================] - 0s 66ms/step - loss: 0.0390 - root_mean_squared_error: 0.1974 - val_loss: 0.0260 - val_root_mean_squared_error: 0.1612\n",
      "Epoch 22/200\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0369 - root_mean_squared_error: 0.1920 - val_loss: 0.0257 - val_root_mean_squared_error: 0.1603\n",
      "Epoch 23/200\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0363 - root_mean_squared_error: 0.1904 - val_loss: 0.0252 - val_root_mean_squared_error: 0.1587\n",
      "Epoch 24/200\n",
      "6/6 [==============================] - 1s 91ms/step - loss: 0.0360 - root_mean_squared_error: 0.1897 - val_loss: 0.0249 - val_root_mean_squared_error: 0.1578\n",
      "Epoch 25/200\n",
      "6/6 [==============================] - 1s 92ms/step - loss: 0.0342 - root_mean_squared_error: 0.1848 - val_loss: 0.0246 - val_root_mean_squared_error: 0.1569\n",
      "Epoch 26/200\n",
      "6/6 [==============================] - 0s 66ms/step - loss: 0.0336 - root_mean_squared_error: 0.1832 - val_loss: 0.0242 - val_root_mean_squared_error: 0.1557\n",
      "Epoch 27/200\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0316 - root_mean_squared_error: 0.1779 - val_loss: 0.0236 - val_root_mean_squared_error: 0.1538\n",
      "Epoch 28/200\n",
      "6/6 [==============================] - 0s 66ms/step - loss: 0.0304 - root_mean_squared_error: 0.1744 - val_loss: 0.0231 - val_root_mean_squared_error: 0.1520\n",
      "Epoch 29/200\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0280 - root_mean_squared_error: 0.1672 - val_loss: 0.0225 - val_root_mean_squared_error: 0.1499\n",
      "Epoch 30/200\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0269 - root_mean_squared_error: 0.1641 - val_loss: 0.0219 - val_root_mean_squared_error: 0.1480\n",
      "Epoch 31/200\n",
      "6/6 [==============================] - 0s 66ms/step - loss: 0.0251 - root_mean_squared_error: 0.1585 - val_loss: 0.0215 - val_root_mean_squared_error: 0.1465\n",
      "Epoch 32/200\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0229 - root_mean_squared_error: 0.1515 - val_loss: 0.0208 - val_root_mean_squared_error: 0.1443\n",
      "Epoch 33/200\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0207 - root_mean_squared_error: 0.1439 - val_loss: 0.0202 - val_root_mean_squared_error: 0.1422\n",
      "Epoch 34/200\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0193 - root_mean_squared_error: 0.1389 - val_loss: 0.0196 - val_root_mean_squared_error: 0.1400\n",
      "Epoch 35/200\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0176 - root_mean_squared_error: 0.1327 - val_loss: 0.0190 - val_root_mean_squared_error: 0.1379\n",
      "Epoch 36/200\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [19]\u001B[0m, in \u001B[0;36m<cell line: 11>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      8\u001B[0m run[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhyper-parameters\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m m3_hyper_parameters\n\u001B[0;32m     10\u001B[0m model3\u001B[38;5;241m.\u001B[39mcompile(loss\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmse\u001B[39m\u001B[38;5;124m\"\u001B[39m, optimizer\u001B[38;5;241m=\u001B[39mm3_optimizer[m3_hyper_parameters[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moptimizer\u001B[39m\u001B[38;5;124m\"\u001B[39m]], metrics\u001B[38;5;241m=\u001B[39m[keras\u001B[38;5;241m.\u001B[39mmetrics\u001B[38;5;241m.\u001B[39mRootMeanSquaredError()])\n\u001B[1;32m---> 11\u001B[0m \u001B[43mmodel3\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train_3d\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     12\u001B[0m \u001B[43m                     \u001B[49m\u001B[43mvalidation_split\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mm3_hyper_parameters\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mvalidation_split\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     13\u001B[0m \u001B[43m                     \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mm3_hyper_parameters\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mbatch_size\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     14\u001B[0m \u001B[43m                     \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mm3_hyper_parameters\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mepochs\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     15\u001B[0m \u001B[43m                     \u001B[49m\u001B[43mshuffle\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m     16\u001B[0m \u001B[43m                     \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43mneptune_cbk\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     17\u001B[0m yhat \u001B[38;5;241m=\u001B[39m model3\u001B[38;5;241m.\u001B[39mpredict(X_test, verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n\u001B[0;32m     18\u001B[0m yhat_inverse \u001B[38;5;241m=\u001B[39m scaler\u001B[38;5;241m.\u001B[39minverse_transform(yhat\u001B[38;5;241m.\u001B[39msqueeze(\u001B[38;5;241m2\u001B[39m))\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\milestone2\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     62\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m     63\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 64\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m     65\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:  \u001B[38;5;66;03m# pylint: disable=broad-except\u001B[39;00m\n\u001B[0;32m     66\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\milestone2\\lib\\site-packages\\keras\\engine\\training.py:1409\u001B[0m, in \u001B[0;36mModel.fit\u001B[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[0;32m   1402\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m tf\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mexperimental\u001B[38;5;241m.\u001B[39mTrace(\n\u001B[0;32m   1403\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m   1404\u001B[0m     epoch_num\u001B[38;5;241m=\u001B[39mepoch,\n\u001B[0;32m   1405\u001B[0m     step_num\u001B[38;5;241m=\u001B[39mstep,\n\u001B[0;32m   1406\u001B[0m     batch_size\u001B[38;5;241m=\u001B[39mbatch_size,\n\u001B[0;32m   1407\u001B[0m     _r\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m):\n\u001B[0;32m   1408\u001B[0m   callbacks\u001B[38;5;241m.\u001B[39mon_train_batch_begin(step)\n\u001B[1;32m-> 1409\u001B[0m   tmp_logs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43miterator\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1410\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m data_handler\u001B[38;5;241m.\u001B[39mshould_sync:\n\u001B[0;32m   1411\u001B[0m     context\u001B[38;5;241m.\u001B[39masync_wait()\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\milestone2\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    148\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    149\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 150\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    151\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    152\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\milestone2\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001B[0m, in \u001B[0;36mFunction.__call__\u001B[1;34m(self, *args, **kwds)\u001B[0m\n\u001B[0;32m    912\u001B[0m compiler \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mxla\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jit_compile \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnonXla\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    914\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m OptionalXlaContext(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jit_compile):\n\u001B[1;32m--> 915\u001B[0m   result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)\n\u001B[0;32m    917\u001B[0m new_tracing_count \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexperimental_get_tracing_count()\n\u001B[0;32m    918\u001B[0m without_tracing \u001B[38;5;241m=\u001B[39m (tracing_count \u001B[38;5;241m==\u001B[39m new_tracing_count)\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\milestone2\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001B[0m, in \u001B[0;36mFunction._call\u001B[1;34m(self, *args, **kwds)\u001B[0m\n\u001B[0;32m    944\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock\u001B[38;5;241m.\u001B[39mrelease()\n\u001B[0;32m    945\u001B[0m   \u001B[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001B[39;00m\n\u001B[0;32m    946\u001B[0m   \u001B[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001B[39;00m\n\u001B[1;32m--> 947\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_stateless_fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)  \u001B[38;5;66;03m# pylint: disable=not-callable\u001B[39;00m\n\u001B[0;32m    948\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_stateful_fn \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    949\u001B[0m   \u001B[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001B[39;00m\n\u001B[0;32m    950\u001B[0m   \u001B[38;5;66;03m# in parallel.\u001B[39;00m\n\u001B[0;32m    951\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock\u001B[38;5;241m.\u001B[39mrelease()\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\milestone2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2453\u001B[0m, in \u001B[0;36mFunction.__call__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   2450\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock:\n\u001B[0;32m   2451\u001B[0m   (graph_function,\n\u001B[0;32m   2452\u001B[0m    filtered_flat_args) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_maybe_define_function(args, kwargs)\n\u001B[1;32m-> 2453\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mgraph_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_flat\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   2454\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfiltered_flat_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcaptured_inputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgraph_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcaptured_inputs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\milestone2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1860\u001B[0m, in \u001B[0;36mConcreteFunction._call_flat\u001B[1;34m(self, args, captured_inputs, cancellation_manager)\u001B[0m\n\u001B[0;32m   1856\u001B[0m possible_gradient_type \u001B[38;5;241m=\u001B[39m gradients_util\u001B[38;5;241m.\u001B[39mPossibleTapeGradientTypes(args)\n\u001B[0;32m   1857\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (possible_gradient_type \u001B[38;5;241m==\u001B[39m gradients_util\u001B[38;5;241m.\u001B[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001B[0;32m   1858\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m executing_eagerly):\n\u001B[0;32m   1859\u001B[0m   \u001B[38;5;66;03m# No tape is watching; skip to running the function.\u001B[39;00m\n\u001B[1;32m-> 1860\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_build_call_outputs(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_inference_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcall\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1861\u001B[0m \u001B[43m      \u001B[49m\u001B[43mctx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcancellation_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcancellation_manager\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m   1862\u001B[0m forward_backward \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_select_forward_and_backward_functions(\n\u001B[0;32m   1863\u001B[0m     args,\n\u001B[0;32m   1864\u001B[0m     possible_gradient_type,\n\u001B[0;32m   1865\u001B[0m     executing_eagerly)\n\u001B[0;32m   1866\u001B[0m forward_function, args_with_tangents \u001B[38;5;241m=\u001B[39m forward_backward\u001B[38;5;241m.\u001B[39mforward()\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\milestone2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:497\u001B[0m, in \u001B[0;36m_EagerDefinedFunction.call\u001B[1;34m(self, ctx, args, cancellation_manager)\u001B[0m\n\u001B[0;32m    495\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m _InterpolateFunctionError(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    496\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m cancellation_manager \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 497\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[43mexecute\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexecute\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    498\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msignature\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    499\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnum_outputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_num_outputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    500\u001B[0m \u001B[43m        \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    501\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattrs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattrs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    502\u001B[0m \u001B[43m        \u001B[49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mctx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    503\u001B[0m   \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    504\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m execute\u001B[38;5;241m.\u001B[39mexecute_with_cancellation(\n\u001B[0;32m    505\u001B[0m         \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msignature\u001B[38;5;241m.\u001B[39mname),\n\u001B[0;32m    506\u001B[0m         num_outputs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_outputs,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    509\u001B[0m         ctx\u001B[38;5;241m=\u001B[39mctx,\n\u001B[0;32m    510\u001B[0m         cancellation_manager\u001B[38;5;241m=\u001B[39mcancellation_manager)\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\milestone2\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001B[0m, in \u001B[0;36mquick_execute\u001B[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001B[0m\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m     53\u001B[0m   ctx\u001B[38;5;241m.\u001B[39mensure_initialized()\n\u001B[1;32m---> 54\u001B[0m   tensors \u001B[38;5;241m=\u001B[39m \u001B[43mpywrap_tfe\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTFE_Py_Execute\u001B[49m\u001B[43m(\u001B[49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_handle\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mop_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     55\u001B[0m \u001B[43m                                      \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattrs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_outputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     56\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m core\u001B[38;5;241m.\u001B[39m_NotOkStatusException \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m     57\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "run = neptune.init(\n",
    "    project=\"milestone2-california-water-shortage/deeplearning-lstm\",\n",
    "    api_token=neptune_key,\n",
    "    name=\"Advanced Model 4\",\n",
    "    tags=[\"WithDetailedWellCounts\", \"OneUnidirectionalLSTM\", \"OneRepeatVector\", \"OneUnidirectionalLSTM\", \"TimeDistributedDense\", \"Dropout\"]\n",
    ")\n",
    "neptune_cbk = NeptuneCallback(run=run, base_namespace='metrics')\n",
    "run['hyper-parameters'] = m3_hyper_parameters\n",
    "\n",
    "model3.compile(loss=\"mse\", optimizer=m3_optimizer[m3_hyper_parameters[\"optimizer\"]], metrics=[keras.metrics.RootMeanSquaredError()])\n",
    "model3.fit(X_train, y_train_3d,\n",
    "                     validation_split=m3_hyper_parameters[\"validation_split\"],\n",
    "                     batch_size=m3_hyper_parameters[\"batch_size\"],\n",
    "                     epochs=m3_hyper_parameters[\"epochs\"],\n",
    "                     shuffle=False,\n",
    "                     callbacks=[neptune_cbk])\n",
    "yhat = model3.predict(X_test, verbose=0)\n",
    "yhat_inverse = scaler.inverse_transform(yhat.squeeze(2))\n",
    "y_test_inverse = scaler.inverse_transform(y_test)\n",
    "evaluate_forecast(y_test_inverse, yhat_inverse)\n",
    "mae, mse, rmse = evaluate_forecast(y_test_inverse, yhat_inverse)\n",
    "run[\"eval/mae\"] = mae\n",
    "run[\"eval/mse\"] = mse\n",
    "run[\"eval/rmse\"] = rmse\n",
    "run.stop()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}