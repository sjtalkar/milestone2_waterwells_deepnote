{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import neptune.new as neptune\n",
    "from neptune.new.integrations.tensorflow_keras import NeptuneCallback\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, TimeDistributed, Conv1D, MaxPooling1D, Flatten, Bidirectional, Input, Flatten, Activation, Reshape, RepeatVector, Concatenate\n",
    "from keras.optimizers import RMSprop, Adam, Adamax, Adagrad\n",
    "\n",
    "import keras_tuner\n",
    "\n",
    "from lib.read_data import read_and_join_output_file\n",
    "#from lib.create_pipeline import create_transformation_pipeline\n",
    "from lib.deeplearning import create_transformation_pipelines\n",
    "from lib.transform_impute import convert_back_df\n",
    "from lib.split_data import train_test_group_time_split"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "RANDOM_SEED = 31\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tensorflow.random.set_seed(RANDOM_SEED)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tensorflow.config.list_physical_devices('GPU')))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# During experiment we can try to use neptune.ai to log all the Tensorflow experiments results\n",
    "neptune_key = pickle.load(open(\"./neptune.pkl\", \"rb\"))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preparing the Dataset\n",
    "The train and test sets are split by Township-Ranges, i.e. some Township-Ranges data are either fully in the train or test set.\n",
    "The target value is the value of that variable for 2021\n",
    "Thus train/test sets are of shape (number of Township-Ranges, 7 years (2014-2020), the number of features).\n",
    "The input of 1 data point in the model is of shape (7x81\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "                     TOTALDRILLDEPTH_AVG  WELLYIELD_AVG  STATICWATERLEVEL_AVG  \\\nTOWNSHIP_RANGE YEAR                                                             \nT01N R02E      2014             0.000000       0.000489              0.020868   \n               2015             0.000000       0.000000              0.000000   \n               2016             0.000000       0.003259              0.036728   \n               2017             0.066667       0.006410              0.025876   \n               2018             0.053651       0.000652              0.063022   \n...                                  ...            ...                   ...   \nT32S R30E      2016             0.000000       0.000000              0.000000   \n               2017             0.000000       0.000000              0.000000   \n               2018             0.000000       0.000000              0.000000   \n               2019             0.000000       0.000000              0.000000   \n               2020             0.000000       0.000000              0.000000   \n\n                     TOPOFPERFORATEDINTERVAL_AVG  \\\nTOWNSHIP_RANGE YEAR                                \nT01N R02E      2014                     0.052288   \n               2015                     0.000000   \n               2016                     0.084967   \n               2017                     0.082789   \n               2018                     0.077124   \n...                                          ...   \nT32S R30E      2016                     0.000000   \n               2017                     0.000000   \n               2018                     0.000000   \n               2019                     0.000000   \n               2020                     0.000000   \n\n                     BOTTOMOFPERFORATEDINTERVAL_AVG  TOTALCOMPLETEDDEPTH_AVG  \\\nTOWNSHIP_RANGE YEAR                                                            \nT01N R02E      2014                        0.076699                 0.127841   \n               2015                        0.000000                 0.000000   \n               2016                        0.058252                 0.056818   \n               2017                        0.064725                 0.074495   \n               2018                        0.053592                 0.064015   \n...                                             ...                      ...   \nT32S R30E      2016                        0.000000                 0.000000   \n               2017                        0.000000                 0.000000   \n               2018                        0.000000                 0.000000   \n               2019                        0.000000                 0.000000   \n               2020                        0.000000                 0.000000   \n\n                     VEGETATION_BLUE_OAK-GRAY_PINE  \\\nTOWNSHIP_RANGE YEAR                                  \nT01N R02E      2014                       0.010798   \n               2015                       0.010798   \n               2016                       0.010798   \n               2017                       0.010798   \n               2018                       0.010798   \n...                                            ...   \nT32S R30E      2016                       0.033178   \n               2017                       0.033178   \n               2018                       0.033178   \n               2019                       0.033178   \n               2020                       0.033178   \n\n                     VEGETATION_CALIFORNIA_COAST_LIVE_OAK  \\\nTOWNSHIP_RANGE YEAR                                         \nT01N R02E      2014                              0.002749   \n               2015                              0.002749   \n               2016                              0.002749   \n               2017                              0.002749   \n               2018                              0.002749   \n...                                                   ...   \nT32S R30E      2016                              0.000000   \n               2017                              0.000000   \n               2018                              0.000000   \n               2019                              0.000000   \n               2020                              0.000000   \n\n                     VEGETATION_CANYON_LIVE_OAK  VEGETATION_HARD_CHAPARRAL  \\\nTOWNSHIP_RANGE YEAR                                                          \nT01N R02E      2014                    0.000000                   0.000633   \n               2015                    0.000000                   0.000633   \n               2016                    0.000000                   0.000633   \n               2017                    0.000000                   0.000633   \n               2018                    0.000000                   0.000633   \n...                                         ...                        ...   \nT32S R30E      2016                    0.002023                   0.003535   \n               2017                    0.002023                   0.003535   \n               2018                    0.002023                   0.003535   \n               2019                    0.002023                   0.003535   \n               2020                    0.002023                   0.003535   \n\n                     ...  POPULATION_DENSITY  PCT_OF_CAPACITY  \\\nTOWNSHIP_RANGE YEAR  ...                                        \nT01N R02E      2014  ...            0.391791         0.776467   \n               2015  ...            0.394044         0.776467   \n               2016  ...            0.395968         0.776467   \n               2017  ...            0.406050         0.776467   \n               2018  ...            0.405447         0.776467   \n...                  ...                 ...              ...   \nT32S R30E      2016  ...            0.004489         0.496289   \n               2017  ...            0.004477         0.496289   \n               2018  ...            0.004494         0.496289   \n               2019  ...            0.004511         0.580893   \n               2020  ...            0.004533         0.499980   \n\n                     GROUNDSURFACEELEVATION_AVG  AVERAGE_YEARLY_PRECIPITATION  \\\nTOWNSHIP_RANGE YEAR                                                             \nT01N R02E      2014                    0.043092                      0.286941   \n               2015                    0.037376                      0.301232   \n               2016                    0.016622                      0.357881   \n               2017                    0.031660                      0.689154   \n               2018                    0.051869                      0.252603   \n...                                         ...                           ...   \nT32S R30E      2016                    0.058099                      0.118655   \n               2017                    0.058099                      0.180043   \n               2018                    0.058099                      0.084816   \n               2019                    0.058099                      0.168764   \n               2020                    0.058099                      0.166161   \n\n                     SHORTAGE_COUNT   GSE_GWE  WELL_COUNT_AGRICULTURE  \\\nTOWNSHIP_RANGE YEAR                                                     \nT01N R02E      2014             0.0  0.083371                0.021277   \n               2015             0.0  0.081869                0.000000   \n               2016             0.0  0.071257                0.000000   \n               2017             0.0  0.070044                0.000000   \n               2018             0.0  0.067062                0.021277   \n...                             ...       ...                     ...   \nT32S R30E      2016             0.0  0.667084                0.000000   \n               2017             0.0  0.566686                0.000000   \n               2018             0.0  0.597051                0.000000   \n               2019             0.0  0.608404                0.000000   \n               2020             0.0  0.595798                0.000000   \n\n                     WELL_COUNT_DOMESTIC  WELL_COUNT_INDUSTRIAL  \\\nTOWNSHIP_RANGE YEAR                                               \nT01N R02E      2014             0.013889                    0.0   \n               2015             0.000000                    0.0   \n               2016             0.013889                    0.0   \n               2017             0.041667                    0.0   \n               2018             0.013889                    0.0   \n...                                  ...                    ...   \nT32S R30E      2016             0.000000                    0.0   \n               2017             0.000000                    0.0   \n               2018             0.000000                    0.0   \n               2019             0.000000                    0.0   \n               2020             0.000000                    0.0   \n\n                     WELL_COUNT_PUBLIC  \nTOWNSHIP_RANGE YEAR                     \nT01N R02E      2014                0.0  \n               2015                0.0  \n               2016                0.0  \n               2017                0.0  \n               2018                0.0  \n...                                ...  \nT32S R30E      2016                0.0  \n               2017                0.0  \n               2018                0.0  \n               2019                0.0  \n               2020                0.0  \n\n[2842 rows x 81 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th>TOTALDRILLDEPTH_AVG</th>\n      <th>WELLYIELD_AVG</th>\n      <th>STATICWATERLEVEL_AVG</th>\n      <th>TOPOFPERFORATEDINTERVAL_AVG</th>\n      <th>BOTTOMOFPERFORATEDINTERVAL_AVG</th>\n      <th>TOTALCOMPLETEDDEPTH_AVG</th>\n      <th>VEGETATION_BLUE_OAK-GRAY_PINE</th>\n      <th>VEGETATION_CALIFORNIA_COAST_LIVE_OAK</th>\n      <th>VEGETATION_CANYON_LIVE_OAK</th>\n      <th>VEGETATION_HARD_CHAPARRAL</th>\n      <th>...</th>\n      <th>POPULATION_DENSITY</th>\n      <th>PCT_OF_CAPACITY</th>\n      <th>GROUNDSURFACEELEVATION_AVG</th>\n      <th>AVERAGE_YEARLY_PRECIPITATION</th>\n      <th>SHORTAGE_COUNT</th>\n      <th>GSE_GWE</th>\n      <th>WELL_COUNT_AGRICULTURE</th>\n      <th>WELL_COUNT_DOMESTIC</th>\n      <th>WELL_COUNT_INDUSTRIAL</th>\n      <th>WELL_COUNT_PUBLIC</th>\n    </tr>\n    <tr>\n      <th>TOWNSHIP_RANGE</th>\n      <th>YEAR</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">T01N R02E</th>\n      <th>2014</th>\n      <td>0.000000</td>\n      <td>0.000489</td>\n      <td>0.020868</td>\n      <td>0.052288</td>\n      <td>0.076699</td>\n      <td>0.127841</td>\n      <td>0.010798</td>\n      <td>0.002749</td>\n      <td>0.000000</td>\n      <td>0.000633</td>\n      <td>...</td>\n      <td>0.391791</td>\n      <td>0.776467</td>\n      <td>0.043092</td>\n      <td>0.286941</td>\n      <td>0.0</td>\n      <td>0.083371</td>\n      <td>0.021277</td>\n      <td>0.013889</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2015</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.010798</td>\n      <td>0.002749</td>\n      <td>0.000000</td>\n      <td>0.000633</td>\n      <td>...</td>\n      <td>0.394044</td>\n      <td>0.776467</td>\n      <td>0.037376</td>\n      <td>0.301232</td>\n      <td>0.0</td>\n      <td>0.081869</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2016</th>\n      <td>0.000000</td>\n      <td>0.003259</td>\n      <td>0.036728</td>\n      <td>0.084967</td>\n      <td>0.058252</td>\n      <td>0.056818</td>\n      <td>0.010798</td>\n      <td>0.002749</td>\n      <td>0.000000</td>\n      <td>0.000633</td>\n      <td>...</td>\n      <td>0.395968</td>\n      <td>0.776467</td>\n      <td>0.016622</td>\n      <td>0.357881</td>\n      <td>0.0</td>\n      <td>0.071257</td>\n      <td>0.000000</td>\n      <td>0.013889</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2017</th>\n      <td>0.066667</td>\n      <td>0.006410</td>\n      <td>0.025876</td>\n      <td>0.082789</td>\n      <td>0.064725</td>\n      <td>0.074495</td>\n      <td>0.010798</td>\n      <td>0.002749</td>\n      <td>0.000000</td>\n      <td>0.000633</td>\n      <td>...</td>\n      <td>0.406050</td>\n      <td>0.776467</td>\n      <td>0.031660</td>\n      <td>0.689154</td>\n      <td>0.0</td>\n      <td>0.070044</td>\n      <td>0.000000</td>\n      <td>0.041667</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2018</th>\n      <td>0.053651</td>\n      <td>0.000652</td>\n      <td>0.063022</td>\n      <td>0.077124</td>\n      <td>0.053592</td>\n      <td>0.064015</td>\n      <td>0.010798</td>\n      <td>0.002749</td>\n      <td>0.000000</td>\n      <td>0.000633</td>\n      <td>...</td>\n      <td>0.405447</td>\n      <td>0.776467</td>\n      <td>0.051869</td>\n      <td>0.252603</td>\n      <td>0.0</td>\n      <td>0.067062</td>\n      <td>0.021277</td>\n      <td>0.013889</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">T32S R30E</th>\n      <th>2016</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.033178</td>\n      <td>0.000000</td>\n      <td>0.002023</td>\n      <td>0.003535</td>\n      <td>...</td>\n      <td>0.004489</td>\n      <td>0.496289</td>\n      <td>0.058099</td>\n      <td>0.118655</td>\n      <td>0.0</td>\n      <td>0.667084</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2017</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.033178</td>\n      <td>0.000000</td>\n      <td>0.002023</td>\n      <td>0.003535</td>\n      <td>...</td>\n      <td>0.004477</td>\n      <td>0.496289</td>\n      <td>0.058099</td>\n      <td>0.180043</td>\n      <td>0.0</td>\n      <td>0.566686</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2018</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.033178</td>\n      <td>0.000000</td>\n      <td>0.002023</td>\n      <td>0.003535</td>\n      <td>...</td>\n      <td>0.004494</td>\n      <td>0.496289</td>\n      <td>0.058099</td>\n      <td>0.084816</td>\n      <td>0.0</td>\n      <td>0.597051</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2019</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.033178</td>\n      <td>0.000000</td>\n      <td>0.002023</td>\n      <td>0.003535</td>\n      <td>...</td>\n      <td>0.004511</td>\n      <td>0.580893</td>\n      <td>0.058099</td>\n      <td>0.168764</td>\n      <td>0.0</td>\n      <td>0.608404</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2020</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.033178</td>\n      <td>0.000000</td>\n      <td>0.002023</td>\n      <td>0.003535</td>\n      <td>...</td>\n      <td>0.004533</td>\n      <td>0.499980</td>\n      <td>0.058099</td>\n      <td>0.166161</td>\n      <td>0.0</td>\n      <td>0.595798</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>2842 rows × 81 columns</p>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_size=0.15\n",
    "# Load the data from the ETL output files\n",
    "X = read_and_join_output_file()\n",
    "#X[\"WELL_COUNT\"] = X[\"WELL_COUNT_PUBLIC\"] + X[\"WELL_COUNT_AGRICULTURE\"] + X[\"WELL_COUNT_DOMESTIC\"] + X[\"WELL_COUNT_INDUSTRIAL\"]\n",
    "#X.drop(columns=[\"WELL_COUNT_PUBLIC\", \"WELL_COUNT_AGRICULTURE\", \"WELL_COUNT_DOMESTIC\", \"WELL_COUNT_INDUSTRIAL\"], inplace=True)\n",
    "# Split the data into a training and a test set\n",
    "X_train_df, X_test_df, y_train_df, y_test_df = train_test_group_time_split(X, index=[\"TOWNSHIP_RANGE\", \"YEAR\"], group=\"TOWNSHIP_RANGE\", test_size=test_size, random_seed=RANDOM_SEED)\n",
    "# Create, fit and apply the data imputation pipeline to the training and test sets\n",
    "impute_pipeline, columns = create_transformation_pipelines(X_train_df)\n",
    "X_train_impute = impute_pipeline.fit_transform(X_train_df)\n",
    "X_test_impute = impute_pipeline.transform(X_test_df)\n",
    "# Convert the X_train and X_test back to dataframes\n",
    "X_train_impute_df = pd.DataFrame(X_train_impute, index=X_train_df.index, columns=columns)\n",
    "X_test_impute_df = pd.DataFrame(X_test_impute, index=X_test_df.index, columns=columns)\n",
    "#X_train_impute_df.drop(columns=[\"BOTTOMOFPERFORATEDINTERVAL_AVG\", \"TOPOFPERFORATEDINTERVAL_AVG\"], inplace=True)\n",
    "#X_test_impute_df.drop(columns=[\"BOTTOMOFPERFORATEDINTERVAL_AVG\", \"TOPOFPERFORATEDINTERVAL_AVG\"], inplace=True)\n",
    "# Keep only the GSE_GWE variable as the outcome variable\n",
    "scaler = MinMaxScaler()\n",
    "y_train = scaler.fit_transform(y_train_df[[\"GSE_GWE\"]])\n",
    "y_test = scaler.transform(y_test_df[[\"GSE_GWE\"]])\n",
    "y_train_3d = y_train[..., np.newaxis]\n",
    "X_train_impute_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# Change the shape of the input array to (number of Township-Ranges, 7 years (2014-2020), the number of features)\n",
    "X_train = X_train_impute_df.values.reshape(len(X_train_impute_df.index.get_level_values(0).unique()), len(X_train_impute_df.index.get_level_values(1).unique()), X_train_impute_df.shape[1])\n",
    "X_test = X_test_impute_df.values.reshape(len(X_test_impute_df.index.get_level_values(0).unique()), len(X_test_impute_df.index.get_level_values(1).unique()), X_test_impute_df.shape[1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Checking the train, validation and test input (X) datasets sizes:\n",
      "Size of the X_train dataset: (406, 7, 81)\n",
      "Size of the X_test dataset: (72, 7, 81)\n",
      "====================================================================================================\n",
      "Checking the train, validation and test output (y) datasets sizes:\n",
      "Size of the y_train dataset: (406, 1)\n",
      "Size of the y_test dataset: (72, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*100)\n",
    "print(\"Checking the train, validation and test input (X) datasets sizes:\")\n",
    "print(f\"Size of the X_train dataset: {X_train.shape}\")\n",
    "#print(f\"Size of the X_val dataset: {X_val.shape}\")\n",
    "print(f\"Size of the X_test dataset: {X_test.shape}\")\n",
    "print(\"=\"*100)\n",
    "print(\"Checking the train, validation and test output (y) datasets sizes:\")\n",
    "print(f\"Size of the y_train dataset: {y_train.shape}\")\n",
    "#print(f\"Size of the y_val dataset: {y_val_df.shape}\")\n",
    "print(f\"Size of the y_test dataset: {y_test.shape}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def evaluate_forecast(y_test_inverse, yhat_inverse):\n",
    "    mse_ = keras.metrics.MeanSquaredError()\n",
    "    mae_ = keras.metrics.MeanAbsoluteError()\n",
    "    rmse_ = keras.metrics.RootMeanSquaredError()\n",
    "    mae = mae_(y_test_inverse,yhat_inverse)\n",
    "    print('mae:', mae)\n",
    "    mse = mse_(y_test_inverse,yhat_inverse)\n",
    "    print('mse:', mse)\n",
    "    rmse = rmse_(y_test_inverse,yhat_inverse)\n",
    "    print('rmse:', rmse)\n",
    "    return mae, mse, rmse"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "nb_features = len(X_train_impute_df.columns)\n",
    "output_activation = \"linear\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Simple Model Hyper-parameter Tuning\n",
    "This model is just made of a single LSTM layer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "class Model1(keras_tuner.HyperModel):\n",
    "    def build(self, hp):\n",
    "        model = Sequential()\n",
    "        hp_units = hp.Int(\"units\", min_value=20, max_value=300, step=20)\n",
    "        hp_activ = hp.Choice(\"activation\", values=[\"relu\", \"tanh\", \"sigmoid\"])\n",
    "        model.add(LSTM(units=hp_units, activation=hp_activ, input_shape=(7, nb_features)))\n",
    "        model.add(Dense(1, activation=output_activation))\n",
    "\n",
    "        hp_learning_rate = hp.Choice(\"learning_rate\", values=[1e-2, 1e-3, 1e-4])\n",
    "        model.compile(loss=\"mse\", optimizer=Adagrad(learning_rate=hp_learning_rate), metrics=[keras.metrics.RootMeanSquaredError()])\n",
    "\n",
    "        return model\n",
    "\n",
    "    def fit(self, hp, model, *args, **kwargs):\n",
    "        return model.fit(\n",
    "            *args,\n",
    "            validation_split=hp.Float(\"validation_split\", min_value=0.05, max_value=0.2, step=0.05),\n",
    "            batch_size=hp.Int(\"batch_size\", min_value=32, max_value=192, step=32),\n",
    "            #epochs=hp.Int(\"epochs\", min_value=50, max_value=400, step=50),\n",
    "            **kwargs,\n",
    "        )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "stop_early = tensorflow.keras.callbacks.EarlyStopping(monitor='val_root_mean_squared_error', patience=7)\n",
    "tuner = keras_tuner.Hyperband(Model1(),\n",
    "                              objective=keras_tuner.Objective(\"val_root_mean_squared_error\", direction=\"min\"),\n",
    "                              max_epochs=100000,\n",
    "                              factor=1000,\n",
    "                              overwrite=True,\n",
    "                              directory=\"keras_tuner\",\n",
    "                              project_name=\"keras_tuner\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 561 Complete [00h 00m 34s]\n",
      "val_root_mean_squared_error: 0.16563788056373596\n",
      "\n",
      "Best val_root_mean_squared_error So Far: 0.08208870887756348\n",
      "Total elapsed time: 03h 55m 49s\n",
      "\n",
      "Search: Running Trial #562\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "160               |300               |units\n",
      "sigmoid           |tanh              |activation\n",
      "0.001             |0.01              |learning_rate\n",
      "0.15              |0.15              |validation_split\n",
      "192               |32                |batch_size\n",
      "100               |100               |tuner/epochs\n",
      "0                 |0                 |tuner/initial_epoch\n",
      "1                 |1                 |tuner/bracket\n",
      "0                 |0                 |tuner/round\n",
      "\n",
      "Epoch 1/100\n",
      "2/2 [==============================] - 2s 409ms/step - loss: 1.3211 - root_mean_squared_error: 1.1494 - val_loss: 1.3352 - val_root_mean_squared_error: 1.1555\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 228ms/step - loss: 1.1088 - root_mean_squared_error: 1.0530 - val_loss: 1.1745 - val_root_mean_squared_error: 1.0837\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 219ms/step - loss: 0.9706 - root_mean_squared_error: 0.9852 - val_loss: 1.0519 - val_root_mean_squared_error: 1.0256\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 221ms/step - loss: 0.8642 - root_mean_squared_error: 0.9296 - val_loss: 0.9522 - val_root_mean_squared_error: 0.9758\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 209ms/step - loss: 0.7774 - root_mean_squared_error: 0.8817 - val_loss: 0.8680 - val_root_mean_squared_error: 0.9317\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 264ms/step - loss: 0.7041 - root_mean_squared_error: 0.8391 - val_loss: 0.7954 - val_root_mean_squared_error: 0.8918\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 0s 235ms/step - loss: 0.6408 - root_mean_squared_error: 0.8005 - val_loss: 0.7315 - val_root_mean_squared_error: 0.8553\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 0s 229ms/step - loss: 0.5855 - root_mean_squared_error: 0.7652 - val_loss: 0.6752 - val_root_mean_squared_error: 0.8217\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - 0s 221ms/step - loss: 0.5367 - root_mean_squared_error: 0.7326 - val_loss: 0.6247 - val_root_mean_squared_error: 0.7904\n",
      "Epoch 10/100\n",
      "2/2 [==============================] - 0s 225ms/step - loss: 0.4932 - root_mean_squared_error: 0.7023 - val_loss: 0.5793 - val_root_mean_squared_error: 0.7611\n",
      "Epoch 11/100\n",
      "2/2 [==============================] - 0s 226ms/step - loss: 0.4545 - root_mean_squared_error: 0.6741 - val_loss: 0.5386 - val_root_mean_squared_error: 0.7339\n",
      "Epoch 12/100\n",
      "2/2 [==============================] - 0s 212ms/step - loss: 0.4196 - root_mean_squared_error: 0.6478 - val_loss: 0.5015 - val_root_mean_squared_error: 0.7081\n",
      "Epoch 13/100\n",
      "2/2 [==============================] - 0s 260ms/step - loss: 0.3881 - root_mean_squared_error: 0.6229 - val_loss: 0.4677 - val_root_mean_squared_error: 0.6839\n",
      "Epoch 14/100\n",
      "2/2 [==============================] - 0s 218ms/step - loss: 0.3596 - root_mean_squared_error: 0.5996 - val_loss: 0.4370 - val_root_mean_squared_error: 0.6611\n",
      "Epoch 15/100\n",
      "2/2 [==============================] - 0s 229ms/step - loss: 0.3338 - root_mean_squared_error: 0.5777 - val_loss: 0.4089 - val_root_mean_squared_error: 0.6394\n",
      "Epoch 16/100\n",
      "2/2 [==============================] - 0s 220ms/step - loss: 0.3103 - root_mean_squared_error: 0.5570 - val_loss: 0.3830 - val_root_mean_squared_error: 0.6189\n",
      "Epoch 17/100\n",
      "2/2 [==============================] - 0s 220ms/step - loss: 0.2888 - root_mean_squared_error: 0.5374 - val_loss: 0.3593 - val_root_mean_squared_error: 0.5994\n",
      "Epoch 18/100\n",
      "2/2 [==============================] - 0s 240ms/step - loss: 0.2692 - root_mean_squared_error: 0.5188 - val_loss: 0.3375 - val_root_mean_squared_error: 0.5809\n",
      "Epoch 19/100\n",
      "2/2 [==============================] - 0s 245ms/step - loss: 0.2513 - root_mean_squared_error: 0.5013 - val_loss: 0.3173 - val_root_mean_squared_error: 0.5633\n",
      "Epoch 20/100\n",
      "2/2 [==============================] - 0s 260ms/step - loss: 0.2349 - root_mean_squared_error: 0.4846 - val_loss: 0.2988 - val_root_mean_squared_error: 0.5466\n",
      "Epoch 21/100\n",
      "2/2 [==============================] - 0s 215ms/step - loss: 0.2199 - root_mean_squared_error: 0.4689 - val_loss: 0.2816 - val_root_mean_squared_error: 0.5307\n",
      "Epoch 22/100\n",
      "2/2 [==============================] - 0s 244ms/step - loss: 0.2061 - root_mean_squared_error: 0.4540 - val_loss: 0.2658 - val_root_mean_squared_error: 0.5156\n",
      "Epoch 23/100\n",
      "2/2 [==============================] - 0s 256ms/step - loss: 0.1934 - root_mean_squared_error: 0.4398 - val_loss: 0.2510 - val_root_mean_squared_error: 0.5010\n",
      "Epoch 24/100\n",
      "2/2 [==============================] - 0s 236ms/step - loss: 0.1817 - root_mean_squared_error: 0.4262 - val_loss: 0.2373 - val_root_mean_squared_error: 0.4872\n",
      "Epoch 25/100\n",
      "2/2 [==============================] - 0s 286ms/step - loss: 0.1709 - root_mean_squared_error: 0.4134 - val_loss: 0.2246 - val_root_mean_squared_error: 0.4740\n",
      "Epoch 26/100\n",
      "2/2 [==============================] - 0s 244ms/step - loss: 0.1610 - root_mean_squared_error: 0.4013 - val_loss: 0.2129 - val_root_mean_squared_error: 0.4614\n",
      "Epoch 27/100\n",
      "2/2 [==============================] - 0s 217ms/step - loss: 0.1520 - root_mean_squared_error: 0.3898 - val_loss: 0.2020 - val_root_mean_squared_error: 0.4495\n",
      "Epoch 28/100\n",
      "2/2 [==============================] - 0s 237ms/step - loss: 0.1436 - root_mean_squared_error: 0.3790 - val_loss: 0.1918 - val_root_mean_squared_error: 0.4380\n",
      "Epoch 29/100\n",
      "2/2 [==============================] - 0s 215ms/step - loss: 0.1359 - root_mean_squared_error: 0.3686 - val_loss: 0.1824 - val_root_mean_squared_error: 0.4271\n",
      "Epoch 30/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1207 - root_mean_squared_error: 0.3474"
     ]
    }
   ],
   "source": [
    "tuner.search(X_train, y_train, epochs=100, callbacks=[stop_early])\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(f\"\"\"\n",
    "The hyperparameter search is complete.\n",
    "lstm_units: {best_hps.get('units')}\n",
    "lstm_activation: {best_hps.get('activation')}\n",
    "learning_rate: {best_hps.get('learning_rate')}\n",
    "batch_size: {best_hps.get('batch_size')}\n",
    "validation_split: {best_hps.get('validation_split')}\n",
    "\"\"\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Simple Model Evaluation\n",
    "The results of the evaluation of the tuned model compared to the test set are storred in Neptune"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "m1_hyper_parameters = {\n",
    "    \"random_seed\": RANDOM_SEED,\n",
    "    \"test_size\": test_size,\n",
    "    \"validation_split\": 0.1,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"batch_size\": 32,\n",
    "    \"epochs\": 200,\n",
    "    \"lstm_units\": 40,\n",
    "    \"lstm_activation\": \"sigmoid\",\n",
    "    \"output_activation\": output_activation,\n",
    "    \"nb_features\": nb_features,\n",
    "    \"optimizer\": \"Adam\"\n",
    "}\n",
    "\n",
    "m1_optimizer = {\n",
    "    \"RMSprop\": RMSprop(learning_rate=m1_hyper_parameters[\"learning_rate\"]),\n",
    "    \"Adam\": Adam(learning_rate=m1_hyper_parameters[\"learning_rate\"]),\n",
    "    \"Adamax\": Adamax(learning_rate=m1_hyper_parameters[\"learning_rate\"]),\n",
    "    \"Adagrad\": Adagrad(learning_rate=m1_hyper_parameters[\"learning_rate\"])\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_1 (LSTM)               (None, 70)                42560     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 71        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 42,631\n",
      "Trainable params: 42,631\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1 = Sequential()\n",
    "model1.add(LSTM(m1_hyper_parameters[\"lstm_units\"], activation=m1_hyper_parameters[\"lstm_activation\"], input_shape=(7, nb_features)))\n",
    "model1.add(Dense(1, activation=m1_hyper_parameters[\"output_activation\"]))\n",
    "model1.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/milestone2-california-water-shortage/deeplearning-lstm/e/DEEPLSTM-87\n",
      "Remember to stop your run once you’ve finished logging your metadata (https://docs.neptune.ai/api-reference/run#.stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.\n",
      "Epoch 1/200\n",
      "12/12 [==============================] - 1s 27ms/step - loss: 0.2526 - val_loss: 0.0368\n",
      "Epoch 2/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0431 - val_loss: 0.0152\n",
      "Epoch 3/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0233 - val_loss: 0.0116\n",
      "Epoch 4/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0175 - val_loss: 0.0121\n",
      "Epoch 5/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0148 - val_loss: 0.0084\n",
      "Epoch 6/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0129 - val_loss: 0.0082\n",
      "Epoch 7/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0112 - val_loss: 0.0062\n",
      "Epoch 8/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0099 - val_loss: 0.0050\n",
      "Epoch 9/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0086 - val_loss: 0.0053\n",
      "Epoch 10/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0076 - val_loss: 0.0041\n",
      "Epoch 11/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0067 - val_loss: 0.0040\n",
      "Epoch 12/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0059 - val_loss: 0.0039\n",
      "Epoch 13/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0053 - val_loss: 0.0084\n",
      "Epoch 14/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0051 - val_loss: 0.0051\n",
      "Epoch 15/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0042 - val_loss: 0.0044\n",
      "Epoch 16/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0037 - val_loss: 0.0041\n",
      "Epoch 17/200\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.0035 - val_loss: 0.0043\n",
      "Epoch 18/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0032 - val_loss: 0.0035\n",
      "Epoch 19/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0030 - val_loss: 0.0044\n",
      "Epoch 20/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0029 - val_loss: 0.0045\n",
      "Epoch 21/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0029 - val_loss: 0.0077\n",
      "Epoch 22/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0028 - val_loss: 0.0059\n",
      "Epoch 23/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0028 - val_loss: 0.0074\n",
      "Epoch 24/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 25/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0024 - val_loss: 0.0048\n",
      "Epoch 26/200\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.0025 - val_loss: 0.0034\n",
      "Epoch 27/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0024 - val_loss: 0.0021\n",
      "Epoch 28/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0022 - val_loss: 0.0028\n",
      "Epoch 29/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0022 - val_loss: 0.0026\n",
      "Epoch 30/200\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.0022 - val_loss: 0.0022\n",
      "Epoch 31/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0021 - val_loss: 0.0015\n",
      "Epoch 32/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0022 - val_loss: 0.0035\n",
      "Epoch 33/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0021 - val_loss: 0.0028\n",
      "Epoch 34/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0021 - val_loss: 0.0014\n",
      "Epoch 35/200\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.0021 - val_loss: 0.0031\n",
      "Epoch 36/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0023 - val_loss: 0.0011\n",
      "Epoch 37/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0023 - val_loss: 0.0035\n",
      "Epoch 38/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0020 - val_loss: 0.0029\n",
      "Epoch 39/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0021 - val_loss: 9.7630e-04\n",
      "Epoch 40/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0021 - val_loss: 0.0023\n",
      "Epoch 41/200\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.0022 - val_loss: 9.6225e-04\n",
      "Epoch 42/200\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.0023 - val_loss: 0.0034\n",
      "Epoch 43/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0020 - val_loss: 0.0015\n",
      "Epoch 44/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0019 - val_loss: 0.0011\n",
      "Epoch 45/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0025 - val_loss: 0.0020\n",
      "Epoch 46/200\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.0020 - val_loss: 0.0010\n",
      "Epoch 47/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0024 - val_loss: 0.0030\n",
      "Epoch 48/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0021 - val_loss: 0.0013\n",
      "Epoch 49/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0021 - val_loss: 0.0028\n",
      "Epoch 50/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0021 - val_loss: 0.0014\n",
      "Epoch 51/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0019 - val_loss: 0.0014\n",
      "Epoch 52/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0019 - val_loss: 0.0011\n",
      "Epoch 53/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0021 - val_loss: 0.0059\n",
      "Epoch 54/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0032 - val_loss: 0.0040\n",
      "Epoch 55/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0034 - val_loss: 0.0023\n",
      "Epoch 56/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0022 - val_loss: 0.0022\n",
      "Epoch 57/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0022 - val_loss: 0.0017\n",
      "Epoch 58/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0021 - val_loss: 0.0012\n",
      "Epoch 59/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0023 - val_loss: 0.0024\n",
      "Epoch 60/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0018 - val_loss: 0.0013\n",
      "Epoch 61/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0018 - val_loss: 0.0028\n",
      "Epoch 62/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0019 - val_loss: 0.0013\n",
      "Epoch 63/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 64/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0018 - val_loss: 0.0035\n",
      "Epoch 65/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0021 - val_loss: 0.0012\n",
      "Epoch 66/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0020 - val_loss: 0.0011\n",
      "Epoch 67/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0018 - val_loss: 0.0031\n",
      "Epoch 68/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0019 - val_loss: 9.9339e-04\n",
      "Epoch 69/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0020 - val_loss: 0.0016\n",
      "Epoch 70/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0018 - val_loss: 0.0017\n",
      "Epoch 71/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0018 - val_loss: 0.0034\n",
      "Epoch 72/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0018 - val_loss: 0.0016\n",
      "Epoch 73/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0019 - val_loss: 0.0039\n",
      "Epoch 74/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0020 - val_loss: 0.0011\n",
      "Epoch 75/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0023 - val_loss: 0.0020\n",
      "Epoch 76/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0018 - val_loss: 0.0012\n",
      "Epoch 77/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0018 - val_loss: 0.0011\n",
      "Epoch 78/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0019 - val_loss: 0.0059\n",
      "Epoch 79/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0027 - val_loss: 0.0017\n",
      "Epoch 80/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0020 - val_loss: 0.0015\n",
      "Epoch 81/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0026 - val_loss: 0.0025\n",
      "Epoch 82/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0019 - val_loss: 0.0027\n",
      "Epoch 83/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0020 - val_loss: 0.0019\n",
      "Epoch 84/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0018 - val_loss: 0.0016\n",
      "Epoch 85/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0018 - val_loss: 0.0041\n",
      "Epoch 86/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0021 - val_loss: 0.0012\n",
      "Epoch 87/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0026 - val_loss: 0.0011\n",
      "Epoch 88/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0018 - val_loss: 0.0023\n",
      "Epoch 89/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0018 - val_loss: 0.0023\n",
      "Epoch 90/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0018 - val_loss: 0.0017\n",
      "Epoch 91/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0017 - val_loss: 0.0013\n",
      "Epoch 92/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0020 - val_loss: 0.0022\n",
      "Epoch 93/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 94/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0019 - val_loss: 0.0014\n",
      "Epoch 95/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0019 - val_loss: 0.0036\n",
      "Epoch 96/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0019 - val_loss: 0.0014\n",
      "Epoch 97/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0019 - val_loss: 0.0022\n",
      "Epoch 98/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0016 - val_loss: 0.0014\n",
      "Epoch 99/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0020 - val_loss: 0.0013\n",
      "Epoch 100/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0019 - val_loss: 0.0044\n",
      "Epoch 101/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0022 - val_loss: 0.0034\n",
      "Epoch 102/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0019 - val_loss: 0.0013\n",
      "Epoch 103/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0019 - val_loss: 0.0016\n",
      "Epoch 104/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0017 - val_loss: 0.0018\n",
      "Epoch 105/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0018 - val_loss: 0.0050\n",
      "Epoch 106/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0018 - val_loss: 0.0022\n",
      "Epoch 107/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0021 - val_loss: 0.0013\n",
      "Epoch 108/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0022 - val_loss: 0.0016\n",
      "Epoch 109/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0019 - val_loss: 0.0018\n",
      "Epoch 110/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0018 - val_loss: 0.0028\n",
      "Epoch 111/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0019 - val_loss: 0.0011\n",
      "Epoch 112/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0021 - val_loss: 0.0015\n",
      "Epoch 113/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0018 - val_loss: 0.0019\n",
      "Epoch 114/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0019 - val_loss: 0.0017\n",
      "Epoch 115/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0017 - val_loss: 0.0031\n",
      "Epoch 116/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0018 - val_loss: 0.0020\n",
      "Epoch 117/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0019 - val_loss: 0.0014\n",
      "Epoch 118/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0018 - val_loss: 0.0014\n",
      "Epoch 119/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0020 - val_loss: 0.0014\n",
      "Epoch 120/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0019 - val_loss: 0.0014\n",
      "Epoch 121/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0020 - val_loss: 0.0024\n",
      "Epoch 122/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0018 - val_loss: 0.0022\n",
      "Epoch 123/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0016 - val_loss: 0.0026\n",
      "Epoch 124/200\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.0016 - val_loss: 0.0017\n",
      "Epoch 125/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0016 - val_loss: 0.0023\n",
      "Epoch 126/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0017 - val_loss: 0.0028\n",
      "Epoch 127/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0021 - val_loss: 0.0013\n",
      "Epoch 128/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0026 - val_loss: 0.0014\n",
      "Epoch 129/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0019 - val_loss: 0.0031\n",
      "Epoch 130/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0017 - val_loss: 0.0023\n",
      "Epoch 131/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0016 - val_loss: 0.0013\n",
      "Epoch 132/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0016 - val_loss: 0.0021\n",
      "Epoch 133/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0017 - val_loss: 0.0037\n",
      "Epoch 134/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0018 - val_loss: 0.0029\n",
      "Epoch 135/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0016 - val_loss: 0.0023\n",
      "Epoch 136/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0015 - val_loss: 0.0019\n",
      "Epoch 137/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0017 - val_loss: 0.0021\n",
      "Epoch 138/200\n",
      "12/12 [==============================] - 0s 12ms/step - loss: 0.0019 - val_loss: 0.0018\n",
      "Epoch 139/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 140/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0015 - val_loss: 0.0025\n",
      "Epoch 141/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0017 - val_loss: 0.0015\n",
      "Epoch 142/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0017 - val_loss: 0.0016\n",
      "Epoch 143/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0023 - val_loss: 0.0015\n",
      "Epoch 144/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0017 - val_loss: 0.0039\n",
      "Epoch 145/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0017 - val_loss: 0.0019\n",
      "Epoch 146/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0017 - val_loss: 0.0025\n",
      "Epoch 147/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0016 - val_loss: 0.0014\n",
      "Epoch 148/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0017 - val_loss: 0.0015\n",
      "Epoch 149/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0016 - val_loss: 0.0028\n",
      "Epoch 150/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0015 - val_loss: 0.0021\n",
      "Epoch 151/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0016 - val_loss: 0.0022\n",
      "Epoch 152/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0015 - val_loss: 0.0026\n",
      "Epoch 153/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0017 - val_loss: 0.0044\n",
      "Epoch 154/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0018 - val_loss: 0.0020\n",
      "Epoch 155/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0016 - val_loss: 0.0025\n",
      "Epoch 156/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0019 - val_loss: 0.0014\n",
      "Epoch 157/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0016 - val_loss: 0.0012\n",
      "Epoch 158/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0017 - val_loss: 0.0019\n",
      "Epoch 159/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0019 - val_loss: 0.0015\n",
      "Epoch 160/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 161/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0014 - val_loss: 0.0024\n",
      "Epoch 162/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0014 - val_loss: 0.0041\n",
      "Epoch 163/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0017 - val_loss: 0.0020\n",
      "Epoch 164/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0014 - val_loss: 0.0020\n",
      "Epoch 165/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0015 - val_loss: 0.0019\n",
      "Epoch 166/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 167/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0016 - val_loss: 0.0013\n",
      "Epoch 168/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 169/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0015 - val_loss: 0.0020\n",
      "Epoch 170/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0013 - val_loss: 0.0015\n",
      "Epoch 171/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 172/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0014 - val_loss: 0.0018\n",
      "Epoch 173/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0013 - val_loss: 0.0018\n",
      "Epoch 174/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0016 - val_loss: 0.0023\n",
      "Epoch 175/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 176/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0018 - val_loss: 0.0016\n",
      "Epoch 177/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0015 - val_loss: 0.0018\n",
      "Epoch 178/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 179/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0013 - val_loss: 0.0021\n",
      "Epoch 180/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0017 - val_loss: 0.0018\n",
      "Epoch 181/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0016 - val_loss: 0.0019\n",
      "Epoch 182/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0016 - val_loss: 0.0018\n",
      "Epoch 183/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0013 - val_loss: 0.0039\n",
      "Epoch 184/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0015 - val_loss: 0.0038\n",
      "Epoch 185/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0017 - val_loss: 0.0020\n",
      "Epoch 186/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0016 - val_loss: 0.0027\n",
      "Epoch 187/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0015 - val_loss: 0.0023\n",
      "Epoch 188/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0012 - val_loss: 0.0021\n",
      "Epoch 189/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 190/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 191/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0019 - val_loss: 0.0014\n",
      "Epoch 192/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0022 - val_loss: 0.0019\n",
      "Epoch 193/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0017 - val_loss: 0.0016\n",
      "Epoch 194/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0021 - val_loss: 0.0023\n",
      "Epoch 195/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0024 - val_loss: 0.0044\n",
      "Epoch 196/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0020 - val_loss: 0.0046\n",
      "Epoch 197/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0014 - val_loss: 0.0043\n",
      "Epoch 198/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0013 - val_loss: 0.0016\n",
      "Epoch 199/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0012 - val_loss: 0.0026\n",
      "Epoch 200/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0014 - val_loss: 0.0016\n",
      "mae: tf.Tensor(31.096588, shape=(), dtype=float32)\n",
      "mse: tf.Tensor(2664.7837, shape=(), dtype=float32)\n",
      "rmse: tf.Tensor(51.621544, shape=(), dtype=float32)\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 193 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 193 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://app.neptune.ai/milestone2-california-water-shortage/deeplearning-lstm/e/DEEPLSTM-87\n"
     ]
    }
   ],
   "source": [
    "# Start experiment\n",
    "run = neptune.init(\n",
    "    project=\"milestone2-california-water-shortage/deeplearning-lstm\",\n",
    "    api_token=neptune_key,\n",
    "    name=\"Basic Model\",\n",
    "    tags=[\"WithDetailedWellCounts\", \"UnidirectionalLSTM\"]\n",
    ")\n",
    "neptune_cbk = NeptuneCallback(run=run, base_namespace='metrics')\n",
    "run['hyper-parameters'] = m1_hyper_parameters\n",
    "\n",
    "model1.compile(loss=\"mse\", optimizer=m1_optimizer[m1_hyper_parameters[\"optimizer\"]], metrics=[keras.metrics.RootMeanSquaredError()])\n",
    "model1.fit(X_train, y_train,\n",
    "                     validation_split=m1_hyper_parameters[\"validation_split\"],\n",
    "                     batch_size=m1_hyper_parameters[\"batch_size\"],\n",
    "                     epochs=m1_hyper_parameters[\"epochs\"],\n",
    "                     shuffle=True,\n",
    "                     callbacks=[neptune_cbk])\n",
    "yhat = model1.predict(X_test, verbose=0)\n",
    "yhat_inverse = scaler.inverse_transform(yhat)\n",
    "y_test_inverse = scaler.inverse_transform(y_test)\n",
    "mae, mse, rmse = evaluate_forecast(y_test_inverse, yhat_inverse)\n",
    "run[\"eval/mae\"] = mae\n",
    "run[\"eval/mse\"] = mse\n",
    "run[\"eval/rmse\"] = rmse\n",
    "run.stop()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model2 Hyper-parameter tuning\n",
    "This model is made of\n",
    "* a simple or bidirectional LSTM layer\n",
    "* a Dense unit\n",
    "* a Dropout Unit"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "class Model2(keras_tuner.HyperModel):\n",
    "    def build(self, hp):\n",
    "        model = Sequential()\n",
    "        lstm_units = hp.Int(\"lstm_units\", min_value=20, max_value=300, step=20)\n",
    "        lstm_activ = hp.Choice(\"lstm_activation\", values=[\"relu\", \"tanh\", \"sigmoid\"])\n",
    "        model.add(LSTM(units=lstm_units, activation=lstm_activ, input_shape=(7, nb_features)))\n",
    "        dense_units = hp.Int(\"dense_units\", min_value=11, max_value=101, step=10)\n",
    "        dense_activation = hp.Choice(\"dense_activation\", values=[\"relu\", \"tanh\", \"sigmoid\"])\n",
    "        model.add(Dense(dense_units, activation=dense_activation))\n",
    "        hp_dropout = hp.Float(\"dropout_rate\", min_value=0.05, max_value=0.25, step=0.05)\n",
    "        model.add(Dropout(hp_dropout))\n",
    "        model.add(Dense(1, activation=output_activation))\n",
    "\n",
    "        hp_learning_rate = hp.Choice(\"learning_rate\", values=[1e-2, 1e-3, 1e-4])\n",
    "        model.compile(loss=\"mse\", optimizer=RMSprop(learning_rate=hp_learning_rate), metrics=[keras.metrics.RootMeanSquaredError()])\n",
    "\n",
    "        return model\n",
    "\n",
    "    def fit(self, hp, model, *args, **kwargs):\n",
    "        return model.fit(\n",
    "            *args,\n",
    "            validation_split=hp.Float(\"validation_split\", min_value=0.05, max_value=0.2, step=0.05),\n",
    "            batch_size=hp.Int(\"batch_size\", min_value=32, max_value=192, step=32),\n",
    "            #epochs=hp.Int(\"epochs\", min_value=50, max_value=400, step=50),\n",
    "            **kwargs,\n",
    "        )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "stop_early = tensorflow.keras.callbacks.EarlyStopping(monitor=\"val_root_mean_squared_error\", patience=7)\n",
    "tuner = keras_tuner.Hyperband(Model2(),\n",
    "                              objective=keras_tuner.Objective(\"val_root_mean_squared_error\", direction=\"min\"),\n",
    "                              max_epochs=100000,\n",
    "                              factor=1000,\n",
    "                              overwrite=True,\n",
    "                              directory=\"keras_tuner\",\n",
    "                              project_name=\"keras_tuner\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1505 Complete [00h 00m 26s]\n",
      "val_root_mean_squared_error: 0.11756662279367447\n",
      "\n",
      "Best val_root_mean_squared_error So Far: 0.0819995179772377\n",
      "Total elapsed time: 03h 54m 36s\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "\n",
      "The hyperparameter search is complete.\n",
      "lstm_units: 240\n",
      "lstm_activation: tanh\n",
      "dense_units: 91\n",
      "dense_activation: tanh\n",
      "dropout_rate: 0.2\n",
      "learning_rate: 0.001\n",
      "batch_size: 128\n",
      "validation_split: 0.15000000000000002\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tuner.search(X_train, y_train, epochs=100, callbacks=[stop_early])\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(f\"\"\"\n",
    "The hyperparameter search is complete.\n",
    "lstm_units: {best_hps.get('lstm_units')}\n",
    "lstm_activation: {best_hps.get('lstm_activation')}\n",
    "dense_units: {best_hps.get('dense_units')}\n",
    "dense_activation: {best_hps.get('dense_activation')}\n",
    "dropout_rate: {best_hps.get('dropout_rate')}\n",
    "learning_rate: {best_hps.get('learning_rate')}\n",
    "batch_size: {best_hps.get('batch_size')}\n",
    "validation_split: {best_hps.get('validation_split')}\n",
    "\"\"\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model 2 Evaluation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional (Bidirectiona  (None, 480)              618240    \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 91)                43771     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 91)                0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1)                 92        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 662,103\n",
      "Trainable params: 662,103\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "m2_hyper_parameters = {\n",
    "    \"random_seed\": RANDOM_SEED,\n",
    "    \"test_size\": test_size,\n",
    "    \"validation_split\": 0.15,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"batch_size\": 128,\n",
    "    \"epochs\": 200,\n",
    "    \"lstm_units\": 240,\n",
    "    \"lstm_activation\": \"tanh\",\n",
    "    \"dense_units\": 91,\n",
    "    \"dense_activation\": \"tanh\",\n",
    "    \"dropout\": 0.2,\n",
    "    \"output_activation\": \"linear\",\n",
    "    \"nb_features\": nb_features,\n",
    "    \"optimizer\": \"RMSprop\"\n",
    "}\n",
    "\n",
    "m2_optimizer = {\n",
    "    \"RMSprop\": RMSprop(learning_rate=m2_hyper_parameters[\"learning_rate\"]),\n",
    "    \"Adam\": Adam(learning_rate=m2_hyper_parameters[\"learning_rate\"]),\n",
    "    \"Adamax\": Adamax(learning_rate=m2_hyper_parameters[\"learning_rate\"]),\n",
    "    \"Adagrad\": Adagrad(learning_rate=m2_hyper_parameters[\"learning_rate\"])\n",
    "}\n",
    "\n",
    "model2 = Sequential()\n",
    "model2.add(Bidirectional(LSTM(m2_hyper_parameters[\"lstm_units\"], activation=m2_hyper_parameters[\"lstm_activation\"]), input_shape=(7, nb_features)))\n",
    "model2.add(Dense(m2_hyper_parameters[\"dense_units\"], activation=m2_hyper_parameters[\"dense_activation\"]))\n",
    "model2.add(Dropout(m2_hyper_parameters[\"dropout\"]))\n",
    "model2.add(Dense(1, activation=m2_hyper_parameters[\"output_activation\"]))\n",
    "model2.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/milestone2-california-water-shortage/deeplearning-lstm/e/DEEPLSTM-140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Info (NVML): Not Supported. GPU usage metrics may not be reported. For more information, see https://docs-legacy.neptune.ai/logging-and-managing-experiment-results/logging-experiment-data.html#hardware-consumption \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remember to stop your run once you’ve finished logging your metadata (https://docs.neptune.ai/api-reference/run#.stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.\n",
      "Epoch 1/200\n",
      "3/3 [==============================] - 4s 375ms/step - loss: 0.4868 - root_mean_squared_error: 0.6977 - val_loss: 0.0162 - val_root_mean_squared_error: 0.1272\n",
      "Epoch 2/200\n",
      "3/3 [==============================] - 0s 63ms/step - loss: 0.0140 - root_mean_squared_error: 0.1182 - val_loss: 0.0155 - val_root_mean_squared_error: 0.1243\n",
      "Epoch 3/200\n",
      "3/3 [==============================] - 0s 67ms/step - loss: 0.0123 - root_mean_squared_error: 0.1107 - val_loss: 0.0148 - val_root_mean_squared_error: 0.1216\n",
      "Epoch 4/200\n",
      "3/3 [==============================] - 0s 64ms/step - loss: 0.0098 - root_mean_squared_error: 0.0991 - val_loss: 0.0130 - val_root_mean_squared_error: 0.1140\n",
      "Epoch 5/200\n",
      "3/3 [==============================] - 0s 66ms/step - loss: 0.0076 - root_mean_squared_error: 0.0875 - val_loss: 0.0125 - val_root_mean_squared_error: 0.1119\n",
      "Epoch 6/200\n",
      "3/3 [==============================] - 0s 69ms/step - loss: 0.0092 - root_mean_squared_error: 0.0959 - val_loss: 0.0114 - val_root_mean_squared_error: 0.1068\n",
      "Epoch 7/200\n",
      "3/3 [==============================] - 0s 65ms/step - loss: 0.0070 - root_mean_squared_error: 0.0834 - val_loss: 0.0106 - val_root_mean_squared_error: 0.1029\n",
      "Epoch 8/200\n",
      "3/3 [==============================] - 0s 63ms/step - loss: 0.0090 - root_mean_squared_error: 0.0950 - val_loss: 0.0278 - val_root_mean_squared_error: 0.1666\n",
      "Epoch 9/200\n",
      "3/3 [==============================] - 0s 61ms/step - loss: 0.0316 - root_mean_squared_error: 0.1777 - val_loss: 0.0182 - val_root_mean_squared_error: 0.1348\n",
      "Epoch 10/200\n",
      "3/3 [==============================] - 0s 66ms/step - loss: 0.0128 - root_mean_squared_error: 0.1132 - val_loss: 0.0148 - val_root_mean_squared_error: 0.1217\n",
      "Epoch 11/200\n",
      "3/3 [==============================] - 0s 62ms/step - loss: 0.0074 - root_mean_squared_error: 0.0862 - val_loss: 0.0140 - val_root_mean_squared_error: 0.1181\n",
      "Epoch 12/200\n",
      "3/3 [==============================] - 0s 64ms/step - loss: 0.0064 - root_mean_squared_error: 0.0800 - val_loss: 0.0110 - val_root_mean_squared_error: 0.1050\n",
      "Epoch 13/200\n",
      "3/3 [==============================] - 0s 61ms/step - loss: 0.0067 - root_mean_squared_error: 0.0819 - val_loss: 0.0245 - val_root_mean_squared_error: 0.1566\n",
      "Epoch 14/200\n",
      "3/3 [==============================] - 0s 64ms/step - loss: 0.0279 - root_mean_squared_error: 0.1671 - val_loss: 0.0323 - val_root_mean_squared_error: 0.1797\n",
      "Epoch 15/200\n",
      "3/3 [==============================] - 0s 65ms/step - loss: 0.0194 - root_mean_squared_error: 0.1392 - val_loss: 0.0104 - val_root_mean_squared_error: 0.1020\n",
      "Epoch 16/200\n",
      "3/3 [==============================] - 0s 64ms/step - loss: 0.0063 - root_mean_squared_error: 0.0792 - val_loss: 0.0113 - val_root_mean_squared_error: 0.1063\n",
      "Epoch 17/200\n",
      "3/3 [==============================] - 0s 64ms/step - loss: 0.0060 - root_mean_squared_error: 0.0776 - val_loss: 0.0134 - val_root_mean_squared_error: 0.1159\n",
      "Epoch 18/200\n",
      "3/3 [==============================] - 0s 63ms/step - loss: 0.0062 - root_mean_squared_error: 0.0791 - val_loss: 0.0193 - val_root_mean_squared_error: 0.1390\n",
      "Epoch 19/200\n",
      "3/3 [==============================] - 0s 64ms/step - loss: 0.0238 - root_mean_squared_error: 0.1543 - val_loss: 0.0277 - val_root_mean_squared_error: 0.1664\n",
      "Epoch 20/200\n",
      "3/3 [==============================] - 0s 69ms/step - loss: 0.0130 - root_mean_squared_error: 0.1142 - val_loss: 0.0111 - val_root_mean_squared_error: 0.1051\n",
      "Epoch 21/200\n",
      "3/3 [==============================] - 0s 65ms/step - loss: 0.0071 - root_mean_squared_error: 0.0841 - val_loss: 0.0127 - val_root_mean_squared_error: 0.1125\n",
      "Epoch 22/200\n",
      "3/3 [==============================] - 0s 64ms/step - loss: 0.0065 - root_mean_squared_error: 0.0807 - val_loss: 0.0114 - val_root_mean_squared_error: 0.1067\n",
      "Epoch 23/200\n",
      "3/3 [==============================] - 0s 66ms/step - loss: 0.0055 - root_mean_squared_error: 0.0745 - val_loss: 0.0161 - val_root_mean_squared_error: 0.1268\n",
      "Epoch 24/200\n",
      "3/3 [==============================] - 0s 66ms/step - loss: 0.0078 - root_mean_squared_error: 0.0883 - val_loss: 0.0281 - val_root_mean_squared_error: 0.1677\n",
      "Epoch 25/200\n",
      "3/3 [==============================] - 0s 63ms/step - loss: 0.0213 - root_mean_squared_error: 0.1461 - val_loss: 0.0187 - val_root_mean_squared_error: 0.1368\n",
      "Epoch 26/200\n",
      "3/3 [==============================] - 0s 65ms/step - loss: 0.0073 - root_mean_squared_error: 0.0854 - val_loss: 0.0129 - val_root_mean_squared_error: 0.1136\n",
      "Epoch 27/200\n",
      "3/3 [==============================] - 0s 63ms/step - loss: 0.0067 - root_mean_squared_error: 0.0817 - val_loss: 0.0186 - val_root_mean_squared_error: 0.1365\n",
      "Epoch 28/200\n",
      "3/3 [==============================] - 0s 66ms/step - loss: 0.0122 - root_mean_squared_error: 0.1103 - val_loss: 0.0168 - val_root_mean_squared_error: 0.1297\n",
      "Epoch 29/200\n",
      "3/3 [==============================] - 0s 66ms/step - loss: 0.0119 - root_mean_squared_error: 0.1089 - val_loss: 0.0207 - val_root_mean_squared_error: 0.1439\n",
      "Epoch 30/200\n",
      "3/3 [==============================] - 0s 65ms/step - loss: 0.0085 - root_mean_squared_error: 0.0924 - val_loss: 0.0136 - val_root_mean_squared_error: 0.1166\n",
      "Epoch 31/200\n",
      "3/3 [==============================] - 0s 68ms/step - loss: 0.0054 - root_mean_squared_error: 0.0734 - val_loss: 0.0130 - val_root_mean_squared_error: 0.1141\n",
      "Epoch 32/200\n",
      "3/3 [==============================] - 0s 65ms/step - loss: 0.0058 - root_mean_squared_error: 0.0762 - val_loss: 0.0141 - val_root_mean_squared_error: 0.1186\n",
      "Epoch 33/200\n",
      "3/3 [==============================] - 0s 67ms/step - loss: 0.0100 - root_mean_squared_error: 0.1000 - val_loss: 0.0363 - val_root_mean_squared_error: 0.1906\n",
      "Epoch 34/200\n",
      "3/3 [==============================] - 0s 67ms/step - loss: 0.0154 - root_mean_squared_error: 0.1241 - val_loss: 0.0123 - val_root_mean_squared_error: 0.1109\n",
      "Epoch 35/200\n",
      "3/3 [==============================] - 0s 65ms/step - loss: 0.0066 - root_mean_squared_error: 0.0814 - val_loss: 0.0148 - val_root_mean_squared_error: 0.1218\n",
      "Epoch 36/200\n",
      "3/3 [==============================] - 0s 63ms/step - loss: 0.0061 - root_mean_squared_error: 0.0781 - val_loss: 0.0129 - val_root_mean_squared_error: 0.1134\n",
      "Epoch 37/200\n",
      "3/3 [==============================] - 0s 68ms/step - loss: 0.0096 - root_mean_squared_error: 0.0978 - val_loss: 0.0227 - val_root_mean_squared_error: 0.1508\n",
      "Epoch 38/200\n",
      "3/3 [==============================] - 0s 65ms/step - loss: 0.0083 - root_mean_squared_error: 0.0909 - val_loss: 0.0120 - val_root_mean_squared_error: 0.1093\n",
      "Epoch 39/200\n",
      "3/3 [==============================] - 0s 66ms/step - loss: 0.0072 - root_mean_squared_error: 0.0850 - val_loss: 0.0155 - val_root_mean_squared_error: 0.1245\n",
      "Epoch 40/200\n",
      "3/3 [==============================] - 0s 67ms/step - loss: 0.0055 - root_mean_squared_error: 0.0741 - val_loss: 0.0159 - val_root_mean_squared_error: 0.1259\n",
      "Epoch 41/200\n",
      "3/3 [==============================] - 0s 66ms/step - loss: 0.0132 - root_mean_squared_error: 0.1150 - val_loss: 0.0143 - val_root_mean_squared_error: 0.1196\n",
      "Epoch 42/200\n",
      "3/3 [==============================] - 0s 70ms/step - loss: 0.0076 - root_mean_squared_error: 0.0875 - val_loss: 0.0129 - val_root_mean_squared_error: 0.1137\n",
      "Epoch 43/200\n",
      "3/3 [==============================] - 0s 63ms/step - loss: 0.0077 - root_mean_squared_error: 0.0879 - val_loss: 0.0176 - val_root_mean_squared_error: 0.1327\n",
      "Epoch 44/200\n",
      "3/3 [==============================] - 0s 64ms/step - loss: 0.0077 - root_mean_squared_error: 0.0879 - val_loss: 0.0125 - val_root_mean_squared_error: 0.1119\n",
      "Epoch 45/200\n",
      "3/3 [==============================] - 0s 63ms/step - loss: 0.0076 - root_mean_squared_error: 0.0869 - val_loss: 0.0176 - val_root_mean_squared_error: 0.1326\n",
      "Epoch 46/200\n",
      "3/3 [==============================] - 0s 63ms/step - loss: 0.0075 - root_mean_squared_error: 0.0868 - val_loss: 0.0150 - val_root_mean_squared_error: 0.1227\n",
      "Epoch 47/200\n",
      "3/3 [==============================] - 0s 61ms/step - loss: 0.0079 - root_mean_squared_error: 0.0888 - val_loss: 0.0163 - val_root_mean_squared_error: 0.1275\n",
      "Epoch 48/200\n",
      "3/3 [==============================] - 0s 65ms/step - loss: 0.0049 - root_mean_squared_error: 0.0702 - val_loss: 0.0135 - val_root_mean_squared_error: 0.1161\n",
      "Epoch 49/200\n",
      "3/3 [==============================] - 0s 63ms/step - loss: 0.0087 - root_mean_squared_error: 0.0931 - val_loss: 0.0169 - val_root_mean_squared_error: 0.1298\n",
      "Epoch 50/200\n",
      "3/3 [==============================] - 0s 63ms/step - loss: 0.0060 - root_mean_squared_error: 0.0774 - val_loss: 0.0143 - val_root_mean_squared_error: 0.1197\n",
      "Epoch 51/200\n",
      "3/3 [==============================] - 0s 64ms/step - loss: 0.0038 - root_mean_squared_error: 0.0613 - val_loss: 0.0185 - val_root_mean_squared_error: 0.1359\n",
      "Epoch 52/200\n",
      "3/3 [==============================] - 0s 62ms/step - loss: 0.0053 - root_mean_squared_error: 0.0725 - val_loss: 0.0167 - val_root_mean_squared_error: 0.1291\n",
      "Epoch 53/200\n",
      "3/3 [==============================] - 0s 62ms/step - loss: 0.0131 - root_mean_squared_error: 0.1142 - val_loss: 0.0178 - val_root_mean_squared_error: 0.1334\n",
      "Epoch 54/200\n",
      "3/3 [==============================] - 0s 64ms/step - loss: 0.0058 - root_mean_squared_error: 0.0759 - val_loss: 0.0126 - val_root_mean_squared_error: 0.1122\n",
      "Epoch 55/200\n",
      "3/3 [==============================] - 0s 61ms/step - loss: 0.0065 - root_mean_squared_error: 0.0804 - val_loss: 0.0173 - val_root_mean_squared_error: 0.1315\n",
      "Epoch 56/200\n",
      "3/3 [==============================] - 0s 61ms/step - loss: 0.0056 - root_mean_squared_error: 0.0748 - val_loss: 0.0143 - val_root_mean_squared_error: 0.1197\n",
      "Epoch 57/200\n",
      "3/3 [==============================] - 0s 63ms/step - loss: 0.0042 - root_mean_squared_error: 0.0651 - val_loss: 0.0162 - val_root_mean_squared_error: 0.1272\n",
      "Epoch 58/200\n",
      "3/3 [==============================] - 0s 61ms/step - loss: 0.0057 - root_mean_squared_error: 0.0754 - val_loss: 0.0229 - val_root_mean_squared_error: 0.1513\n",
      "Epoch 59/200\n",
      "3/3 [==============================] - 0s 63ms/step - loss: 0.0103 - root_mean_squared_error: 0.1015 - val_loss: 0.0292 - val_root_mean_squared_error: 0.1708\n",
      "Epoch 60/200\n",
      "3/3 [==============================] - 0s 64ms/step - loss: 0.0074 - root_mean_squared_error: 0.0862 - val_loss: 0.0135 - val_root_mean_squared_error: 0.1163\n",
      "Epoch 61/200\n",
      "3/3 [==============================] - 0s 62ms/step - loss: 0.0053 - root_mean_squared_error: 0.0727 - val_loss: 0.0180 - val_root_mean_squared_error: 0.1342\n",
      "Epoch 62/200\n",
      "3/3 [==============================] - 0s 61ms/step - loss: 0.0069 - root_mean_squared_error: 0.0828 - val_loss: 0.0161 - val_root_mean_squared_error: 0.1270\n",
      "Epoch 63/200\n",
      "3/3 [==============================] - 0s 63ms/step - loss: 0.0080 - root_mean_squared_error: 0.0895 - val_loss: 0.0156 - val_root_mean_squared_error: 0.1248\n",
      "Epoch 64/200\n",
      "3/3 [==============================] - 0s 61ms/step - loss: 0.0046 - root_mean_squared_error: 0.0680 - val_loss: 0.0130 - val_root_mean_squared_error: 0.1140\n",
      "Epoch 65/200\n",
      "3/3 [==============================] - 0s 63ms/step - loss: 0.0060 - root_mean_squared_error: 0.0776 - val_loss: 0.0176 - val_root_mean_squared_error: 0.1327\n",
      "Epoch 66/200\n",
      "3/3 [==============================] - 0s 64ms/step - loss: 0.0056 - root_mean_squared_error: 0.0746 - val_loss: 0.0179 - val_root_mean_squared_error: 0.1337\n",
      "Epoch 67/200\n",
      "3/3 [==============================] - 0s 65ms/step - loss: 0.0086 - root_mean_squared_error: 0.0927 - val_loss: 0.0191 - val_root_mean_squared_error: 0.1382\n",
      "Epoch 68/200\n",
      "3/3 [==============================] - 0s 69ms/step - loss: 0.0058 - root_mean_squared_error: 0.0763 - val_loss: 0.0151 - val_root_mean_squared_error: 0.1231\n",
      "Epoch 69/200\n",
      "3/3 [==============================] - 0s 66ms/step - loss: 0.0067 - root_mean_squared_error: 0.0819 - val_loss: 0.0180 - val_root_mean_squared_error: 0.1343\n",
      "Epoch 70/200\n",
      "3/3 [==============================] - 0s 63ms/step - loss: 0.0038 - root_mean_squared_error: 0.0614 - val_loss: 0.0175 - val_root_mean_squared_error: 0.1322\n",
      "Epoch 71/200\n",
      "3/3 [==============================] - 0s 71ms/step - loss: 0.0035 - root_mean_squared_error: 0.0595 - val_loss: 0.0281 - val_root_mean_squared_error: 0.1676\n",
      "Epoch 72/200\n",
      "3/3 [==============================] - 0s 66ms/step - loss: 0.0072 - root_mean_squared_error: 0.0848 - val_loss: 0.0137 - val_root_mean_squared_error: 0.1171\n",
      "Epoch 73/200\n",
      "3/3 [==============================] - 0s 63ms/step - loss: 0.0072 - root_mean_squared_error: 0.0847 - val_loss: 0.0129 - val_root_mean_squared_error: 0.1134\n",
      "Epoch 74/200\n",
      "3/3 [==============================] - 0s 65ms/step - loss: 0.0047 - root_mean_squared_error: 0.0683 - val_loss: 0.0132 - val_root_mean_squared_error: 0.1148\n",
      "Epoch 75/200\n",
      "3/3 [==============================] - 0s 62ms/step - loss: 0.0037 - root_mean_squared_error: 0.0607 - val_loss: 0.0157 - val_root_mean_squared_error: 0.1252\n",
      "Epoch 76/200\n",
      "3/3 [==============================] - 0s 66ms/step - loss: 0.0033 - root_mean_squared_error: 0.0572 - val_loss: 0.0147 - val_root_mean_squared_error: 0.1213\n",
      "Epoch 77/200\n",
      "3/3 [==============================] - 0s 64ms/step - loss: 0.0056 - root_mean_squared_error: 0.0750 - val_loss: 0.0287 - val_root_mean_squared_error: 0.1694\n",
      "Epoch 78/200\n",
      "3/3 [==============================] - 0s 63ms/step - loss: 0.0083 - root_mean_squared_error: 0.0908 - val_loss: 0.0108 - val_root_mean_squared_error: 0.1037\n",
      "Epoch 79/200\n",
      "3/3 [==============================] - 0s 65ms/step - loss: 0.0055 - root_mean_squared_error: 0.0740 - val_loss: 0.0232 - val_root_mean_squared_error: 0.1523\n",
      "Epoch 80/200\n",
      "3/3 [==============================] - 0s 65ms/step - loss: 0.0059 - root_mean_squared_error: 0.0769 - val_loss: 0.0166 - val_root_mean_squared_error: 0.1287\n",
      "Epoch 81/200\n",
      "3/3 [==============================] - 0s 66ms/step - loss: 0.0054 - root_mean_squared_error: 0.0731 - val_loss: 0.0199 - val_root_mean_squared_error: 0.1411\n",
      "Epoch 82/200\n",
      "3/3 [==============================] - 0s 63ms/step - loss: 0.0047 - root_mean_squared_error: 0.0683 - val_loss: 0.0130 - val_root_mean_squared_error: 0.1141\n",
      "Epoch 83/200\n",
      "3/3 [==============================] - 0s 66ms/step - loss: 0.0040 - root_mean_squared_error: 0.0631 - val_loss: 0.0169 - val_root_mean_squared_error: 0.1302\n",
      "Epoch 84/200\n",
      "3/3 [==============================] - 0s 62ms/step - loss: 0.0043 - root_mean_squared_error: 0.0654 - val_loss: 0.0143 - val_root_mean_squared_error: 0.1195\n",
      "Epoch 85/200\n",
      "3/3 [==============================] - 0s 63ms/step - loss: 0.0093 - root_mean_squared_error: 0.0965 - val_loss: 0.0290 - val_root_mean_squared_error: 0.1702\n",
      "Epoch 86/200\n",
      "3/3 [==============================] - 0s 64ms/step - loss: 0.0069 - root_mean_squared_error: 0.0830 - val_loss: 0.0146 - val_root_mean_squared_error: 0.1207\n",
      "Epoch 87/200\n",
      "3/3 [==============================] - 0s 63ms/step - loss: 0.0030 - root_mean_squared_error: 0.0552 - val_loss: 0.0204 - val_root_mean_squared_error: 0.1428\n",
      "Epoch 88/200\n",
      "3/3 [==============================] - 0s 61ms/step - loss: 0.0050 - root_mean_squared_error: 0.0707 - val_loss: 0.0134 - val_root_mean_squared_error: 0.1159\n",
      "Epoch 89/200\n",
      "3/3 [==============================] - 0s 65ms/step - loss: 0.0039 - root_mean_squared_error: 0.0628 - val_loss: 0.0314 - val_root_mean_squared_error: 0.1771\n",
      "Epoch 90/200\n",
      "3/3 [==============================] - 0s 62ms/step - loss: 0.0069 - root_mean_squared_error: 0.0829 - val_loss: 0.0162 - val_root_mean_squared_error: 0.1273\n",
      "Epoch 91/200\n",
      "3/3 [==============================] - 0s 62ms/step - loss: 0.0069 - root_mean_squared_error: 0.0829 - val_loss: 0.0208 - val_root_mean_squared_error: 0.1441\n",
      "Epoch 92/200\n",
      "3/3 [==============================] - 0s 63ms/step - loss: 0.0045 - root_mean_squared_error: 0.0671 - val_loss: 0.0130 - val_root_mean_squared_error: 0.1142\n",
      "Epoch 93/200\n",
      "3/3 [==============================] - 0s 66ms/step - loss: 0.0035 - root_mean_squared_error: 0.0590 - val_loss: 0.0177 - val_root_mean_squared_error: 0.1329\n",
      "Epoch 94/200\n",
      "3/3 [==============================] - 0s 62ms/step - loss: 0.0034 - root_mean_squared_error: 0.0581 - val_loss: 0.0187 - val_root_mean_squared_error: 0.1366\n",
      "Epoch 95/200\n",
      "3/3 [==============================] - 0s 63ms/step - loss: 0.0046 - root_mean_squared_error: 0.0680 - val_loss: 0.0174 - val_root_mean_squared_error: 0.1319\n",
      "Epoch 96/200\n",
      "3/3 [==============================] - 0s 60ms/step - loss: 0.0056 - root_mean_squared_error: 0.0749 - val_loss: 0.0163 - val_root_mean_squared_error: 0.1275\n",
      "Epoch 97/200\n",
      "3/3 [==============================] - 0s 63ms/step - loss: 0.0056 - root_mean_squared_error: 0.0750 - val_loss: 0.0146 - val_root_mean_squared_error: 0.1207\n",
      "Epoch 98/200\n",
      "3/3 [==============================] - 0s 61ms/step - loss: 0.0033 - root_mean_squared_error: 0.0570 - val_loss: 0.0129 - val_root_mean_squared_error: 0.1137\n",
      "Epoch 99/200\n",
      "3/3 [==============================] - 0s 61ms/step - loss: 0.0059 - root_mean_squared_error: 0.0771 - val_loss: 0.0255 - val_root_mean_squared_error: 0.1598\n",
      "Epoch 100/200\n",
      "3/3 [==============================] - 0s 59ms/step - loss: 0.0057 - root_mean_squared_error: 0.0753 - val_loss: 0.0140 - val_root_mean_squared_error: 0.1181\n",
      "Epoch 101/200\n",
      "3/3 [==============================] - 0s 64ms/step - loss: 0.0041 - root_mean_squared_error: 0.0637 - val_loss: 0.0141 - val_root_mean_squared_error: 0.1189\n",
      "Epoch 102/200\n",
      "3/3 [==============================] - 0s 60ms/step - loss: 0.0031 - root_mean_squared_error: 0.0557 - val_loss: 0.0167 - val_root_mean_squared_error: 0.1293\n",
      "Epoch 103/200\n",
      "3/3 [==============================] - 0s 59ms/step - loss: 0.0030 - root_mean_squared_error: 0.0543 - val_loss: 0.0269 - val_root_mean_squared_error: 0.1640\n",
      "Epoch 104/200\n",
      "3/3 [==============================] - 0s 64ms/step - loss: 0.0075 - root_mean_squared_error: 0.0866 - val_loss: 0.0158 - val_root_mean_squared_error: 0.1256\n",
      "Epoch 105/200\n",
      "3/3 [==============================] - 0s 64ms/step - loss: 0.0059 - root_mean_squared_error: 0.0770 - val_loss: 0.0160 - val_root_mean_squared_error: 0.1266\n",
      "Epoch 106/200\n",
      "3/3 [==============================] - 0s 64ms/step - loss: 0.0037 - root_mean_squared_error: 0.0606 - val_loss: 0.0134 - val_root_mean_squared_error: 0.1156\n",
      "Epoch 107/200\n",
      "3/3 [==============================] - 0s 62ms/step - loss: 0.0032 - root_mean_squared_error: 0.0564 - val_loss: 0.0124 - val_root_mean_squared_error: 0.1114\n",
      "Epoch 108/200\n",
      "3/3 [==============================] - 0s 60ms/step - loss: 0.0048 - root_mean_squared_error: 0.0695 - val_loss: 0.0205 - val_root_mean_squared_error: 0.1431\n",
      "Epoch 109/200\n",
      "3/3 [==============================] - 0s 60ms/step - loss: 0.0062 - root_mean_squared_error: 0.0787 - val_loss: 0.0143 - val_root_mean_squared_error: 0.1194\n",
      "Epoch 110/200\n",
      "3/3 [==============================] - 0s 61ms/step - loss: 0.0046 - root_mean_squared_error: 0.0678 - val_loss: 0.0164 - val_root_mean_squared_error: 0.1281\n",
      "Epoch 111/200\n",
      "3/3 [==============================] - 0s 60ms/step - loss: 0.0035 - root_mean_squared_error: 0.0594 - val_loss: 0.0152 - val_root_mean_squared_error: 0.1232\n",
      "Epoch 112/200\n",
      "3/3 [==============================] - 0s 60ms/step - loss: 0.0031 - root_mean_squared_error: 0.0559 - val_loss: 0.0249 - val_root_mean_squared_error: 0.1576\n",
      "Epoch 113/200\n",
      "3/3 [==============================] - 0s 62ms/step - loss: 0.0032 - root_mean_squared_error: 0.0566 - val_loss: 0.0150 - val_root_mean_squared_error: 0.1224\n",
      "Epoch 114/200\n",
      "3/3 [==============================] - 0s 60ms/step - loss: 0.0049 - root_mean_squared_error: 0.0700 - val_loss: 0.0144 - val_root_mean_squared_error: 0.1202\n",
      "Epoch 115/200\n",
      "3/3 [==============================] - 0s 61ms/step - loss: 0.0060 - root_mean_squared_error: 0.0778 - val_loss: 0.0156 - val_root_mean_squared_error: 0.1251\n",
      "Epoch 116/200\n",
      "3/3 [==============================] - 0s 63ms/step - loss: 0.0039 - root_mean_squared_error: 0.0628 - val_loss: 0.0203 - val_root_mean_squared_error: 0.1424\n",
      "Epoch 117/200\n",
      "3/3 [==============================] - 0s 63ms/step - loss: 0.0046 - root_mean_squared_error: 0.0681 - val_loss: 0.0152 - val_root_mean_squared_error: 0.1231\n",
      "Epoch 118/200\n",
      "3/3 [==============================] - 0s 62ms/step - loss: 0.0039 - root_mean_squared_error: 0.0626 - val_loss: 0.0162 - val_root_mean_squared_error: 0.1273\n",
      "Epoch 119/200\n",
      "3/3 [==============================] - 0s 65ms/step - loss: 0.0025 - root_mean_squared_error: 0.0496 - val_loss: 0.0122 - val_root_mean_squared_error: 0.1107\n",
      "Epoch 120/200\n",
      "3/3 [==============================] - 0s 63ms/step - loss: 0.0036 - root_mean_squared_error: 0.0597 - val_loss: 0.0201 - val_root_mean_squared_error: 0.1417\n",
      "Epoch 121/200\n",
      "3/3 [==============================] - 0s 64ms/step - loss: 0.0065 - root_mean_squared_error: 0.0804 - val_loss: 0.0134 - val_root_mean_squared_error: 0.1160\n",
      "Epoch 122/200\n",
      "3/3 [==============================] - 0s 64ms/step - loss: 0.0035 - root_mean_squared_error: 0.0595 - val_loss: 0.0204 - val_root_mean_squared_error: 0.1429\n",
      "Epoch 123/200\n",
      "3/3 [==============================] - 0s 64ms/step - loss: 0.0029 - root_mean_squared_error: 0.0540 - val_loss: 0.0136 - val_root_mean_squared_error: 0.1168\n",
      "Epoch 124/200\n",
      "3/3 [==============================] - 0s 67ms/step - loss: 0.0024 - root_mean_squared_error: 0.0489 - val_loss: 0.0137 - val_root_mean_squared_error: 0.1170\n",
      "Epoch 125/200\n",
      "3/3 [==============================] - 0s 66ms/step - loss: 0.0032 - root_mean_squared_error: 0.0570 - val_loss: 0.0223 - val_root_mean_squared_error: 0.1494\n",
      "Epoch 126/200\n",
      "3/3 [==============================] - 0s 63ms/step - loss: 0.0065 - root_mean_squared_error: 0.0806 - val_loss: 0.0157 - val_root_mean_squared_error: 0.1252\n",
      "Epoch 127/200\n",
      "3/3 [==============================] - 0s 68ms/step - loss: 0.0046 - root_mean_squared_error: 0.0678 - val_loss: 0.0144 - val_root_mean_squared_error: 0.1199\n",
      "Epoch 128/200\n",
      "3/3 [==============================] - 0s 64ms/step - loss: 0.0030 - root_mean_squared_error: 0.0552 - val_loss: 0.0135 - val_root_mean_squared_error: 0.1163\n",
      "Epoch 129/200\n",
      "3/3 [==============================] - 0s 64ms/step - loss: 0.0047 - root_mean_squared_error: 0.0684 - val_loss: 0.0170 - val_root_mean_squared_error: 0.1303\n",
      "Epoch 130/200\n",
      "3/3 [==============================] - 0s 66ms/step - loss: 0.0036 - root_mean_squared_error: 0.0598 - val_loss: 0.0149 - val_root_mean_squared_error: 0.1221\n",
      "Epoch 131/200\n",
      "3/3 [==============================] - 0s 64ms/step - loss: 0.0032 - root_mean_squared_error: 0.0563 - val_loss: 0.0164 - val_root_mean_squared_error: 0.1280\n",
      "Epoch 132/200\n",
      "3/3 [==============================] - 0s 64ms/step - loss: 0.0028 - root_mean_squared_error: 0.0526 - val_loss: 0.0213 - val_root_mean_squared_error: 0.1459\n",
      "Epoch 133/200\n",
      "3/3 [==============================] - 0s 65ms/step - loss: 0.0029 - root_mean_squared_error: 0.0537 - val_loss: 0.0155 - val_root_mean_squared_error: 0.1244\n",
      "Epoch 134/200\n",
      "3/3 [==============================] - 0s 61ms/step - loss: 0.0056 - root_mean_squared_error: 0.0751 - val_loss: 0.0259 - val_root_mean_squared_error: 0.1611\n",
      "Epoch 135/200\n",
      "3/3 [==============================] - 0s 61ms/step - loss: 0.0059 - root_mean_squared_error: 0.0767 - val_loss: 0.0141 - val_root_mean_squared_error: 0.1187\n",
      "Epoch 136/200\n",
      "3/3 [==============================] - 0s 61ms/step - loss: 0.0036 - root_mean_squared_error: 0.0604 - val_loss: 0.0143 - val_root_mean_squared_error: 0.1197\n",
      "Epoch 137/200\n",
      "3/3 [==============================] - 0s 62ms/step - loss: 0.0020 - root_mean_squared_error: 0.0450 - val_loss: 0.0149 - val_root_mean_squared_error: 0.1222\n",
      "Epoch 138/200\n",
      "3/3 [==============================] - 0s 61ms/step - loss: 0.0024 - root_mean_squared_error: 0.0494 - val_loss: 0.0128 - val_root_mean_squared_error: 0.1133\n",
      "Epoch 139/200\n",
      "3/3 [==============================] - 0s 63ms/step - loss: 0.0039 - root_mean_squared_error: 0.0627 - val_loss: 0.0231 - val_root_mean_squared_error: 0.1519\n",
      "Epoch 140/200\n",
      "3/3 [==============================] - 0s 62ms/step - loss: 0.0067 - root_mean_squared_error: 0.0820 - val_loss: 0.0141 - val_root_mean_squared_error: 0.1189\n",
      "Epoch 141/200\n",
      "3/3 [==============================] - 0s 62ms/step - loss: 0.0040 - root_mean_squared_error: 0.0635 - val_loss: 0.0138 - val_root_mean_squared_error: 0.1176\n",
      "Epoch 142/200\n",
      "3/3 [==============================] - 0s 62ms/step - loss: 0.0021 - root_mean_squared_error: 0.0457 - val_loss: 0.0137 - val_root_mean_squared_error: 0.1172\n",
      "Epoch 143/200\n",
      "3/3 [==============================] - 0s 61ms/step - loss: 0.0022 - root_mean_squared_error: 0.0468 - val_loss: 0.0171 - val_root_mean_squared_error: 0.1307\n",
      "Epoch 144/200\n",
      "3/3 [==============================] - 0s 60ms/step - loss: 0.0040 - root_mean_squared_error: 0.0636 - val_loss: 0.0179 - val_root_mean_squared_error: 0.1340\n",
      "Epoch 145/200\n",
      "3/3 [==============================] - 0s 61ms/step - loss: 0.0054 - root_mean_squared_error: 0.0736 - val_loss: 0.0188 - val_root_mean_squared_error: 0.1370\n",
      "Epoch 146/200\n",
      "3/3 [==============================] - 0s 62ms/step - loss: 0.0030 - root_mean_squared_error: 0.0550 - val_loss: 0.0155 - val_root_mean_squared_error: 0.1246\n",
      "Epoch 147/200\n",
      "3/3 [==============================] - 0s 60ms/step - loss: 0.0025 - root_mean_squared_error: 0.0498 - val_loss: 0.0138 - val_root_mean_squared_error: 0.1173\n",
      "Epoch 148/200\n",
      "3/3 [==============================] - 0s 63ms/step - loss: 0.0017 - root_mean_squared_error: 0.0411 - val_loss: 0.0134 - val_root_mean_squared_error: 0.1159\n",
      "Epoch 149/200\n",
      "3/3 [==============================] - 0s 61ms/step - loss: 0.0026 - root_mean_squared_error: 0.0505 - val_loss: 0.0188 - val_root_mean_squared_error: 0.1371\n",
      "Epoch 150/200\n",
      "3/3 [==============================] - 0s 61ms/step - loss: 0.0079 - root_mean_squared_error: 0.0888 - val_loss: 0.0152 - val_root_mean_squared_error: 0.1232\n",
      "Epoch 151/200\n",
      "3/3 [==============================] - 0s 63ms/step - loss: 0.0043 - root_mean_squared_error: 0.0657 - val_loss: 0.0166 - val_root_mean_squared_error: 0.1287\n",
      "Epoch 152/200\n",
      "3/3 [==============================] - 0s 63ms/step - loss: 0.0022 - root_mean_squared_error: 0.0469 - val_loss: 0.0154 - val_root_mean_squared_error: 0.1240\n",
      "Epoch 153/200\n",
      "3/3 [==============================] - 0s 63ms/step - loss: 0.0019 - root_mean_squared_error: 0.0433 - val_loss: 0.0145 - val_root_mean_squared_error: 0.1204\n",
      "Epoch 154/200\n",
      "3/3 [==============================] - 0s 61ms/step - loss: 0.0025 - root_mean_squared_error: 0.0498 - val_loss: 0.0200 - val_root_mean_squared_error: 0.1413\n",
      "Epoch 155/200\n",
      "3/3 [==============================] - 0s 61ms/step - loss: 0.0058 - root_mean_squared_error: 0.0759 - val_loss: 0.0131 - val_root_mean_squared_error: 0.1145\n",
      "Epoch 156/200\n",
      "3/3 [==============================] - 0s 63ms/step - loss: 0.0045 - root_mean_squared_error: 0.0668 - val_loss: 0.0135 - val_root_mean_squared_error: 0.1163\n",
      "Epoch 157/200\n",
      "3/3 [==============================] - 0s 63ms/step - loss: 0.0031 - root_mean_squared_error: 0.0555 - val_loss: 0.0148 - val_root_mean_squared_error: 0.1216\n",
      "Epoch 158/200\n",
      "3/3 [==============================] - 0s 63ms/step - loss: 0.0034 - root_mean_squared_error: 0.0582 - val_loss: 0.0165 - val_root_mean_squared_error: 0.1286\n",
      "Epoch 159/200\n",
      "3/3 [==============================] - 0s 64ms/step - loss: 0.0026 - root_mean_squared_error: 0.0508 - val_loss: 0.0157 - val_root_mean_squared_error: 0.1254\n",
      "Epoch 160/200\n",
      "3/3 [==============================] - 0s 63ms/step - loss: 0.0025 - root_mean_squared_error: 0.0497 - val_loss: 0.0158 - val_root_mean_squared_error: 0.1256\n",
      "Epoch 161/200\n",
      "3/3 [==============================] - 0s 64ms/step - loss: 0.0024 - root_mean_squared_error: 0.0491 - val_loss: 0.0143 - val_root_mean_squared_error: 0.1197\n",
      "Epoch 162/200\n",
      "3/3 [==============================] - 0s 66ms/step - loss: 0.0047 - root_mean_squared_error: 0.0686 - val_loss: 0.0265 - val_root_mean_squared_error: 0.1628\n",
      "Epoch 163/200\n",
      "3/3 [==============================] - 0s 65ms/step - loss: 0.0071 - root_mean_squared_error: 0.0844 - val_loss: 0.0142 - val_root_mean_squared_error: 0.1190\n",
      "Epoch 164/200\n",
      "3/3 [==============================] - 0s 62ms/step - loss: 0.0023 - root_mean_squared_error: 0.0475 - val_loss: 0.0189 - val_root_mean_squared_error: 0.1374\n",
      "Epoch 165/200\n",
      "3/3 [==============================] - 0s 64ms/step - loss: 0.0028 - root_mean_squared_error: 0.0525 - val_loss: 0.0136 - val_root_mean_squared_error: 0.1166\n",
      "Epoch 166/200\n",
      "3/3 [==============================] - 0s 65ms/step - loss: 0.0022 - root_mean_squared_error: 0.0474 - val_loss: 0.0228 - val_root_mean_squared_error: 0.1510\n",
      "Epoch 167/200\n",
      "3/3 [==============================] - 0s 62ms/step - loss: 0.0033 - root_mean_squared_error: 0.0578 - val_loss: 0.0135 - val_root_mean_squared_error: 0.1160\n",
      "Epoch 168/200\n",
      "3/3 [==============================] - 0s 65ms/step - loss: 0.0026 - root_mean_squared_error: 0.0510 - val_loss: 0.0180 - val_root_mean_squared_error: 0.1342\n",
      "Epoch 169/200\n",
      "3/3 [==============================] - 0s 62ms/step - loss: 0.0029 - root_mean_squared_error: 0.0539 - val_loss: 0.0139 - val_root_mean_squared_error: 0.1181\n",
      "Epoch 170/200\n",
      "3/3 [==============================] - 0s 63ms/step - loss: 0.0032 - root_mean_squared_error: 0.0562 - val_loss: 0.0152 - val_root_mean_squared_error: 0.1233\n",
      "Epoch 171/200\n",
      "3/3 [==============================] - 0s 63ms/step - loss: 0.0018 - root_mean_squared_error: 0.0422 - val_loss: 0.0159 - val_root_mean_squared_error: 0.1263\n",
      "Epoch 172/200\n",
      "3/3 [==============================] - 0s 61ms/step - loss: 0.0027 - root_mean_squared_error: 0.0520 - val_loss: 0.0219 - val_root_mean_squared_error: 0.1480\n",
      "Epoch 173/200\n",
      "3/3 [==============================] - 0s 62ms/step - loss: 0.0045 - root_mean_squared_error: 0.0672 - val_loss: 0.0173 - val_root_mean_squared_error: 0.1316\n",
      "Epoch 174/200\n",
      "3/3 [==============================] - 0s 62ms/step - loss: 0.0050 - root_mean_squared_error: 0.0706 - val_loss: 0.0161 - val_root_mean_squared_error: 0.1269\n",
      "Epoch 175/200\n",
      "3/3 [==============================] - 0s 63ms/step - loss: 0.0032 - root_mean_squared_error: 0.0568 - val_loss: 0.0149 - val_root_mean_squared_error: 0.1221\n",
      "Epoch 176/200\n",
      "3/3 [==============================] - 0s 63ms/step - loss: 0.0018 - root_mean_squared_error: 0.0422 - val_loss: 0.0168 - val_root_mean_squared_error: 0.1294\n",
      "Epoch 177/200\n",
      "3/3 [==============================] - 0s 62ms/step - loss: 0.0033 - root_mean_squared_error: 0.0578 - val_loss: 0.0163 - val_root_mean_squared_error: 0.1275\n",
      "Epoch 178/200\n",
      "3/3 [==============================] - 0s 61ms/step - loss: 0.0042 - root_mean_squared_error: 0.0647 - val_loss: 0.0175 - val_root_mean_squared_error: 0.1321\n",
      "Epoch 179/200\n",
      "3/3 [==============================] - 0s 60ms/step - loss: 0.0020 - root_mean_squared_error: 0.0443 - val_loss: 0.0140 - val_root_mean_squared_error: 0.1185\n",
      "Epoch 180/200\n",
      "3/3 [==============================] - 0s 62ms/step - loss: 0.0018 - root_mean_squared_error: 0.0423 - val_loss: 0.0144 - val_root_mean_squared_error: 0.1200\n",
      "Epoch 181/200\n",
      "3/3 [==============================] - 0s 60ms/step - loss: 0.0020 - root_mean_squared_error: 0.0443 - val_loss: 0.0145 - val_root_mean_squared_error: 0.1204\n",
      "Epoch 182/200\n",
      "3/3 [==============================] - 0s 61ms/step - loss: 0.0035 - root_mean_squared_error: 0.0594 - val_loss: 0.0140 - val_root_mean_squared_error: 0.1182\n",
      "Epoch 183/200\n",
      "3/3 [==============================] - 0s 64ms/step - loss: 0.0042 - root_mean_squared_error: 0.0651 - val_loss: 0.0149 - val_root_mean_squared_error: 0.1221\n",
      "Epoch 184/200\n",
      "3/3 [==============================] - 0s 62ms/step - loss: 0.0033 - root_mean_squared_error: 0.0571 - val_loss: 0.0138 - val_root_mean_squared_error: 0.1175\n",
      "Epoch 185/200\n",
      "3/3 [==============================] - 0s 61ms/step - loss: 0.0019 - root_mean_squared_error: 0.0438 - val_loss: 0.0130 - val_root_mean_squared_error: 0.1141\n",
      "Epoch 186/200\n",
      "3/3 [==============================] - 0s 62ms/step - loss: 0.0016 - root_mean_squared_error: 0.0405 - val_loss: 0.0139 - val_root_mean_squared_error: 0.1180\n",
      "Epoch 187/200\n",
      "3/3 [==============================] - 0s 61ms/step - loss: 0.0022 - root_mean_squared_error: 0.0466 - val_loss: 0.0154 - val_root_mean_squared_error: 0.1242\n",
      "Epoch 188/200\n",
      "3/3 [==============================] - 0s 62ms/step - loss: 0.0021 - root_mean_squared_error: 0.0461 - val_loss: 0.0143 - val_root_mean_squared_error: 0.1195\n",
      "Epoch 189/200\n",
      "3/3 [==============================] - 0s 64ms/step - loss: 0.0048 - root_mean_squared_error: 0.0693 - val_loss: 0.0190 - val_root_mean_squared_error: 0.1379\n",
      "Epoch 190/200\n",
      "3/3 [==============================] - 0s 60ms/step - loss: 0.0051 - root_mean_squared_error: 0.0711 - val_loss: 0.0172 - val_root_mean_squared_error: 0.1313\n",
      "Epoch 191/200\n",
      "3/3 [==============================] - 0s 60ms/step - loss: 0.0017 - root_mean_squared_error: 0.0412 - val_loss: 0.0183 - val_root_mean_squared_error: 0.1352\n",
      "Epoch 192/200\n",
      "3/3 [==============================] - 0s 60ms/step - loss: 0.0020 - root_mean_squared_error: 0.0445 - val_loss: 0.0149 - val_root_mean_squared_error: 0.1219\n",
      "Epoch 193/200\n",
      "3/3 [==============================] - 0s 60ms/step - loss: 0.0016 - root_mean_squared_error: 0.0395 - val_loss: 0.0171 - val_root_mean_squared_error: 0.1308\n",
      "Epoch 194/200\n",
      "3/3 [==============================] - 0s 60ms/step - loss: 0.0025 - root_mean_squared_error: 0.0501 - val_loss: 0.0157 - val_root_mean_squared_error: 0.1252\n",
      "Epoch 195/200\n",
      "3/3 [==============================] - 0s 60ms/step - loss: 0.0024 - root_mean_squared_error: 0.0486 - val_loss: 0.0160 - val_root_mean_squared_error: 0.1265\n",
      "Epoch 196/200\n",
      "3/3 [==============================] - 0s 60ms/step - loss: 0.0037 - root_mean_squared_error: 0.0608 - val_loss: 0.0134 - val_root_mean_squared_error: 0.1158\n",
      "Epoch 197/200\n",
      "3/3 [==============================] - 0s 61ms/step - loss: 0.0026 - root_mean_squared_error: 0.0512 - val_loss: 0.0137 - val_root_mean_squared_error: 0.1170\n",
      "Epoch 198/200\n",
      "3/3 [==============================] - 0s 62ms/step - loss: 0.0021 - root_mean_squared_error: 0.0458 - val_loss: 0.0230 - val_root_mean_squared_error: 0.1516\n",
      "Epoch 199/200\n",
      "3/3 [==============================] - 0s 64ms/step - loss: 0.0056 - root_mean_squared_error: 0.0751 - val_loss: 0.0130 - val_root_mean_squared_error: 0.1140\n",
      "Epoch 200/200\n",
      "3/3 [==============================] - 0s 60ms/step - loss: 0.0021 - root_mean_squared_error: 0.0461 - val_loss: 0.0139 - val_root_mean_squared_error: 0.1180\n",
      "mae: tf.Tensor(31.689932, shape=(), dtype=float32)\n",
      "mse: tf.Tensor(1850.0625, shape=(), dtype=float32)\n",
      "rmse: tf.Tensor(43.012352, shape=(), dtype=float32)\n",
      "mae: tf.Tensor(31.689932, shape=(), dtype=float32)\n",
      "mse: tf.Tensor(1850.0625, shape=(), dtype=float32)\n",
      "rmse: tf.Tensor(43.012352, shape=(), dtype=float32)\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 102 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 102 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://app.neptune.ai/milestone2-california-water-shortage/deeplearning-lstm/e/DEEPLSTM-140\n"
     ]
    }
   ],
   "source": [
    "run = neptune.init(\n",
    "    project=\"milestone2-california-water-shortage/deeplearning-lstm\",\n",
    "    api_token=neptune_key,\n",
    "    name=\"Advanced Model 1\",\n",
    "    tags=[\"WithDetailedWellCounts\", \"BidirectionalLSTM\"]\n",
    ")\n",
    "neptune_cbk = NeptuneCallback(run=run, base_namespace='metrics')\n",
    "run['hyper-parameters'] = m2_hyper_parameters\n",
    "\n",
    "model2.compile(loss=\"mse\", optimizer=m2_optimizer[m2_hyper_parameters[\"optimizer\"]], metrics=[keras.metrics.RootMeanSquaredError()])\n",
    "model2.fit(X_train, y_train,\n",
    "                     validation_split=m2_hyper_parameters[\"validation_split\"],\n",
    "                     batch_size=m2_hyper_parameters[\"batch_size\"],\n",
    "                     epochs=m2_hyper_parameters[\"epochs\"],\n",
    "                     shuffle=True,\n",
    "                     callbacks=[neptune_cbk])\n",
    "yhat = model2.predict(X_test, verbose=0)\n",
    "yhat_inverse = scaler.inverse_transform(yhat)\n",
    "y_test_inverse = scaler.inverse_transform(y_test)\n",
    "evaluate_forecast(y_test_inverse, yhat_inverse)\n",
    "mae, mse, rmse = evaluate_forecast(y_test_inverse, yhat_inverse)\n",
    "run[\"eval/mae\"] = mae\n",
    "run[\"eval/mse\"] = mse\n",
    "run[\"eval/rmse\"] = rmse\n",
    "run.stop()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model3 Hyper-parameter tuning\n",
    "* a simple or bidirectional encoding LSTM layer\n",
    "* a simple or bidirectional decoding LSTM layer\n",
    "* a Dense unit\n",
    "* a Dropout Unit"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Model3(keras_tuner.HyperModel):\n",
    "    def build(self, hp):\n",
    "        model = Sequential()\n",
    "        lstm_units = hp.Int(\"lstm_units\", min_value=20, max_value=300, step=20)\n",
    "        lstm_activ = hp.Choice(\"lstm_activation\", values=[\"relu\", \"tanh\", \"sigmoid\"])\n",
    "        model.add(LSTM(units=lstm_units, activation=lstm_activ, input_shape=(7, nb_features)))\n",
    "        model3.add(RepeatVector(1))\n",
    "        lstm_units_2 = hp.Int(\"lstm_units\", min_value=20, max_value=300, step=20)\n",
    "        model.add(LSTM(units=lstm_units_2, activation=lstm_activ, return_sequences=True))\n",
    "        dense_units = hp.Int(\"2nd_lstm_units\", min_value=11, max_value=101, step=10)\n",
    "        dense_activation = hp.Choice(\"dense_activation\", values=[\"relu\", \"tanh\", \"sigmoid\"])\n",
    "        model.add(TimeDistributed(Dense(dense_units, activation=dense_activation)))\n",
    "        hp_dropout = hp.Float(\"dropout_rate\", min_value=0.05, max_value=0.25, step=0.05)\n",
    "        model.add(Dropout(hp_dropout))\n",
    "        model.add(Dense(1, activation=output_activation))\n",
    "\n",
    "        hp_learning_rate = hp.Choice(\"learning_rate\", values=[1e-2, 1e-3, 1e-4])\n",
    "        model.compile(loss=\"mse\", optimizer=RMSprop(learning_rate=hp_learning_rate), metrics=[keras.metrics.RootMeanSquaredError()])\n",
    "\n",
    "        return model\n",
    "\n",
    "    def fit(self, hp, model, *args, **kwargs):\n",
    "        return model.fit(\n",
    "            *args,\n",
    "            validation_split=hp.Float(\"validation_split\", min_value=0.05, max_value=0.2, step=0.05),\n",
    "            batch_size=hp.Int(\"batch_size\", min_value=32, max_value=192, step=32),\n",
    "            #epochs=hp.Int(\"epochs\", min_value=50, max_value=400, step=50),\n",
    "            **kwargs,\n",
    "        )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "stop_early = tensorflow.keras.callbacks.EarlyStopping(monitor=\"val_root_mean_squared_error\", patience=7)\n",
    "tuner = keras_tuner.Hyperband(Model3(),\n",
    "                              objective=keras_tuner.Objective(\"val_root_mean_squared_error\", direction=\"min\"),\n",
    "                              max_epochs=100000,\n",
    "                              factor=1000,\n",
    "                              overwrite=True,\n",
    "                              directory=\"keras_tuner\",\n",
    "                              project_name=\"keras_tuner\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tuner.search(X_train, y_train, epochs=100, callbacks=[stop_early])\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(f\"\"\"\n",
    "The hyperparameter search is complete.\n",
    "lstm_units: {best_hps.get('lstm_units')}\n",
    "2nd_lstm_units: {best_hps.get('2nd_lstm_units')}\n",
    "lstm_activation: {best_hps.get('lstm_activation')}\n",
    "dense_units: {best_hps.get('dense_units')}\n",
    "dense_activation: {best_hps.get('dense_activation')}\n",
    "dropout_rate: {best_hps.get('dropout_rate')}\n",
    "learning_rate: {best_hps.get('learning_rate')}\n",
    "batch_size: {best_hps.get('batch_size')}\n",
    "validation_split: {best_hps.get('validation_split')}\n",
    "\"\"\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model 3 Evaluation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_18 (LSTM)              (None, 100)               72800     \n",
      "                                                                 \n",
      " repeat_vector_9 (RepeatVect  (None, 1, 100)           0         \n",
      " or)                                                             \n",
      "                                                                 \n",
      " lstm_19 (LSTM)              (None, 1, 100)            80400     \n",
      "                                                                 \n",
      " time_distributed_9 (TimeDis  (None, 1, 81)            8181      \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 1, 81)             0         \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 1, 1)              82        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 161,463\n",
      "Trainable params: 161,463\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "m3_hyper_parameters = {\n",
    "    \"random_seed\": RANDOM_SEED,\n",
    "    \"test_size\": test_size,\n",
    "    \"validation_split\": 0.1,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"batch_size\": 64,\n",
    "    \"epochs\": 200,\n",
    "    \"lstm_units\": 200,\n",
    "    \"2nd_lstm_units\": 100,\n",
    "    \"lstm_activation\": \"sigmoid\",\n",
    "    \"dense_units\": 81,\n",
    "    \"dense_activation\": \"tanh\",\n",
    "    \"dropout\": 0.2,\n",
    "    \"output_activation\": \"linear\",\n",
    "    \"nb_features\": nb_features,\n",
    "    \"optimizer\": \"RMSprop\"\n",
    "}\n",
    "\n",
    "m3_optimizer = {\n",
    "    \"RMSprop\": RMSprop(learning_rate=m3_hyper_parameters[\"learning_rate\"]),\n",
    "    \"Adam\": Adam(learning_rate=m3_hyper_parameters[\"learning_rate\"]),\n",
    "    \"Adamax\": Adamax(learning_rate=m3_hyper_parameters[\"learning_rate\"]),\n",
    "    \"Adagrad\": Adagrad(learning_rate=m3_hyper_parameters[\"learning_rate\"])\n",
    "}\n",
    "\n",
    "model3 = Sequential()\n",
    "model3.add(LSTM(m3_hyper_parameters[\"lstm_units\"], activation=m3_hyper_parameters[\"lstm_activation\"], input_shape=(7, nb_features)))\n",
    "model3.add(RepeatVector(1))\n",
    "model3.add(LSTM(m3_hyper_parameters[\"2nd_lstm_units\"], activation=m3_hyper_parameters[\"lstm_activation\"], return_sequences=True))\n",
    "model3.add(TimeDistributed(Dense(m3_hyper_parameters[\"dense_units\"], activation=m3_hyper_parameters[\"dense_activation\"])))\n",
    "model3.add(Dropout(m3_hyper_parameters[\"dropout\"]))\n",
    "model3.add(Dense(1, activation=m3_hyper_parameters[\"output_activation\"]))\n",
    "model3.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/milestone2-california-water-shortage/deeplearning-lstm/e/DEEPLSTM-80\n",
      "Remember to stop your run once you’ve finished logging your metadata (https://docs.neptune.ai/api-reference/run#.stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.\n",
      "Epoch 1/200\n",
      "2/2 [==============================] - 4s 750ms/step - loss: 0.0954 - val_loss: 0.0624\n",
      "Epoch 2/200\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.0453 - val_loss: 0.0285\n",
      "Epoch 3/200\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.0536 - val_loss: 0.0317\n",
      "Epoch 4/200\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.0327 - val_loss: 0.0286\n",
      "Epoch 5/200\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.0293 - val_loss: 0.0291\n",
      "Epoch 6/200\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.0234 - val_loss: 0.0296\n",
      "Epoch 7/200\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.0180 - val_loss: 0.0287\n",
      "Epoch 8/200\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.0142 - val_loss: 0.0247\n",
      "Epoch 9/200\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.0111 - val_loss: 0.0206\n",
      "Epoch 10/200\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.0092 - val_loss: 0.0201\n",
      "Epoch 11/200\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.0087 - val_loss: 0.0198\n",
      "Epoch 12/200\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.0071 - val_loss: 0.0166\n",
      "Epoch 13/200\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.0129 - val_loss: 0.0498\n",
      "Epoch 14/200\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.0197 - val_loss: 0.0202\n",
      "Epoch 15/200\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.0071 - val_loss: 0.0203\n",
      "Epoch 16/200\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.0072 - val_loss: 0.0153\n",
      "Epoch 17/200\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.0048 - val_loss: 0.0150\n",
      "Epoch 18/200\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.0044 - val_loss: 0.0153\n",
      "Epoch 19/200\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.0043 - val_loss: 0.0161\n",
      "Epoch 20/200\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.0043 - val_loss: 0.0181\n",
      "Epoch 21/200\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.0050 - val_loss: 0.0258\n",
      "Epoch 22/200\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.0078 - val_loss: 0.0323\n",
      "Epoch 23/200\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.0063 - val_loss: 0.0184\n",
      "Epoch 24/200\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.0095 - val_loss: 0.0403\n",
      "Epoch 25/200\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.0154 - val_loss: 0.0201\n",
      "Epoch 26/200\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.0061 - val_loss: 0.0184\n",
      "Epoch 27/200\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.0039 - val_loss: 0.0163\n",
      "Epoch 28/200\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.0044 - val_loss: 0.0170\n",
      "Epoch 29/200\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.0042 - val_loss: 0.0149\n",
      "Epoch 30/200\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.0037 - val_loss: 0.0219\n",
      "Epoch 31/200\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.0073 - val_loss: 0.0309\n",
      "Epoch 32/200\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.0060 - val_loss: 0.0191\n",
      "Epoch 33/200\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.0064 - val_loss: 0.0326\n",
      "Epoch 34/200\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.0103 - val_loss: 0.0206\n",
      "Epoch 35/200\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.0055 - val_loss: 0.0208\n",
      "Epoch 36/200\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.0036 - val_loss: 0.0185\n",
      "Epoch 37/200\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.0047 - val_loss: 0.0195\n",
      "Epoch 38/200\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.0044 - val_loss: 0.0170\n",
      "Epoch 39/200\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.0047 - val_loss: 0.0282\n",
      "Epoch 40/200\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.0072 - val_loss: 0.0253\n",
      "Epoch 41/200\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.0042 - val_loss: 0.0228\n",
      "Epoch 42/200\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.0063 - val_loss: 0.0190\n",
      "Epoch 43/200\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.0039 - val_loss: 0.0168\n",
      "Epoch 44/200\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.0035 - val_loss: 0.0235\n",
      "Epoch 45/200\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.0052 - val_loss: 0.0263\n",
      "Epoch 46/200\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.0038 - val_loss: 0.0199\n",
      "Epoch 47/200\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.0061 - val_loss: 0.0312\n",
      "Epoch 48/200\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.0083 - val_loss: 0.0252\n",
      "Epoch 49/200\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.0078 - val_loss: 0.0247\n",
      "Epoch 50/200\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.0036 - val_loss: 0.0200\n",
      "Epoch 51/200\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.0041 - val_loss: 0.0211\n",
      "Epoch 52/200\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.0040 - val_loss: 0.0188\n",
      "Epoch 53/200\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.0031 - val_loss: 0.0217\n",
      "Epoch 54/200\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.0033 - val_loss: 0.0214\n",
      "Epoch 55/200\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.0028 - val_loss: 0.0199\n",
      "Epoch 56/200\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.0043 - val_loss: 0.0285\n",
      "Epoch 57/200\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.0061 - val_loss: 0.0257\n",
      "Epoch 58/200\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.0097 - val_loss: 0.0286\n",
      "Epoch 59/200\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.0041 - val_loss: 0.0204\n",
      "Epoch 60/200\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.0041 - val_loss: 0.0248\n",
      "Epoch 61/200\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.0049 - val_loss: 0.0203\n",
      "Epoch 62/200\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.0032 - val_loss: 0.0221\n",
      "Epoch 63/200\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.0027 - val_loss: 0.0201\n",
      "Epoch 64/200\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.0032 - val_loss: 0.0231\n",
      "Epoch 65/200\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.0037 - val_loss: 0.0193\n",
      "Epoch 66/200\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.0032 - val_loss: 0.0302\n",
      "Epoch 67/200\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.0071 - val_loss: 0.0282\n",
      "Epoch 68/200\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.0041 - val_loss: 0.0280\n",
      "Epoch 69/200\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.0060 - val_loss: 0.0202\n",
      "Epoch 70/200\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.0024 - val_loss: 0.0201\n",
      "Epoch 71/200\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.0024 - val_loss: 0.0203\n",
      "Epoch 72/200\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.0024 - val_loss: 0.0201\n",
      "Epoch 73/200\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.0021 - val_loss: 0.0208\n",
      "Epoch 74/200\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.0026 - val_loss: 0.0279\n",
      "Epoch 75/200\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.0059 - val_loss: 0.0337\n",
      "Epoch 76/200\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.0041 - val_loss: 0.0260\n",
      "Epoch 77/200\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 0.0062 - val_loss: 0.0285\n",
      "Epoch 78/200\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.0048 - val_loss: 0.0230\n",
      "Epoch 79/200\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.0033 - val_loss: 0.0242\n",
      "Epoch 80/200\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.0024 - val_loss: 0.0241\n",
      "Epoch 81/200\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.0036 - val_loss: 0.0235\n",
      "Epoch 82/200\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.0028 - val_loss: 0.0211\n",
      "Epoch 83/200\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.0027 - val_loss: 0.0293\n",
      "Epoch 84/200\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.0050 - val_loss: 0.0288\n",
      "Epoch 85/200\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.0033 - val_loss: 0.0288\n",
      "Epoch 86/200\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.0055 - val_loss: 0.0219\n",
      "Epoch 87/200\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.0022 - val_loss: 0.0214\n",
      "Epoch 88/200\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.0021 - val_loss: 0.0220\n",
      "Epoch 89/200\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.0019 - val_loss: 0.0220\n",
      "Epoch 90/200\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.0019 - val_loss: 0.0230\n",
      "Epoch 91/200\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.0021 - val_loss: 0.0257\n",
      "Epoch 92/200\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.0031 - val_loss: 0.0347\n",
      "Epoch 93/200\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.0042 - val_loss: 0.0268\n",
      "Epoch 94/200\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.0062 - val_loss: 0.0401\n",
      "Epoch 95/200\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.0108 - val_loss: 0.0239\n",
      "Epoch 96/200\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.0031 - val_loss: 0.0237\n",
      "Epoch 97/200\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.0021 - val_loss: 0.0246\n",
      "Epoch 98/200\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.0026 - val_loss: 0.0226\n",
      "Epoch 99/200\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.0019 - val_loss: 0.0231\n",
      "Epoch 100/200\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.0019 - val_loss: 0.0242\n",
      "Epoch 101/200\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.0017 - val_loss: 0.0234\n",
      "Epoch 102/200\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.0024 - val_loss: 0.0299\n",
      "Epoch 103/200\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.0040 - val_loss: 0.0241\n",
      "Epoch 104/200\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.0046 - val_loss: 0.0340\n",
      "Epoch 105/200\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 0.0038 - val_loss: 0.0253\n",
      "Epoch 106/200\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.0038 - val_loss: 0.0327\n",
      "Epoch 107/200\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.0058 - val_loss: 0.0256\n",
      "Epoch 108/200\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.0028 - val_loss: 0.0249\n",
      "Epoch 109/200\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.0019 - val_loss: 0.0255\n",
      "Epoch 110/200\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.0025 - val_loss: 0.0233\n",
      "Epoch 111/200\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.0017 - val_loss: 0.0238\n",
      "Epoch 112/200\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.0018 - val_loss: 0.0240\n",
      "Epoch 113/200\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.0017 - val_loss: 0.0236\n",
      "Epoch 114/200\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.0025 - val_loss: 0.0310\n",
      "Epoch 115/200\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.0040 - val_loss: 0.0298\n",
      "Epoch 116/200\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.0081 - val_loss: 0.0309\n",
      "Epoch 117/200\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.0027 - val_loss: 0.0263\n",
      "Epoch 118/200\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.0028 - val_loss: 0.0243\n",
      "Epoch 119/200\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.0019 - val_loss: 0.0229\n",
      "Epoch 120/200\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.0015 - val_loss: 0.0235\n",
      "Epoch 121/200\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.0015 - val_loss: 0.0231\n",
      "Epoch 122/200\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.0015 - val_loss: 0.0239\n",
      "Epoch 123/200\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.0016 - val_loss: 0.0242\n",
      "Epoch 124/200\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.0017 - val_loss: 0.0235\n",
      "Epoch 125/200\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.0017 - val_loss: 0.0307\n",
      "Epoch 126/200\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.0076 - val_loss: 0.0337\n",
      "Epoch 127/200\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.0035 - val_loss: 0.0315\n",
      "Epoch 128/200\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.0047 - val_loss: 0.0236\n",
      "Epoch 129/200\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.0016 - val_loss: 0.0240\n",
      "Epoch 130/200\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.0018 - val_loss: 0.0240\n",
      "Epoch 131/200\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.0016 - val_loss: 0.0232\n",
      "Epoch 132/200\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.0016 - val_loss: 0.0263\n",
      "Epoch 133/200\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.0020 - val_loss: 0.0261\n",
      "Epoch 134/200\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.0018 - val_loss: 0.0293\n",
      "Epoch 135/200\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.0038 - val_loss: 0.0255\n",
      "Epoch 136/200\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.0020 - val_loss: 0.0251\n",
      "Epoch 137/200\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.0034 - val_loss: 0.0344\n",
      "Epoch 138/200\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.0029 - val_loss: 0.0257\n",
      "Epoch 139/200\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.0037 - val_loss: 0.0332\n",
      "Epoch 140/200\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.0054 - val_loss: 0.0276\n",
      "Epoch 141/200\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.0032 - val_loss: 0.0258\n",
      "Epoch 142/200\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.0019 - val_loss: 0.0275\n",
      "Epoch 143/200\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.0025 - val_loss: 0.0240\n",
      "Epoch 144/200\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 0.0016 - val_loss: 0.0240\n",
      "Epoch 145/200\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.0013 - val_loss: 0.0246\n",
      "Epoch 146/200\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.0016 - val_loss: 0.0246\n",
      "Epoch 147/200\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.0015 - val_loss: 0.0254\n",
      "Epoch 148/200\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.0024 - val_loss: 0.0309\n",
      "Epoch 149/200\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.0028 - val_loss: 0.0261\n",
      "Epoch 150/200\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.0034 - val_loss: 0.0353\n",
      "Epoch 151/200\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.0065 - val_loss: 0.0266\n",
      "Epoch 152/200\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.0030 - val_loss: 0.0253\n",
      "Epoch 153/200\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.0017 - val_loss: 0.0267\n",
      "Epoch 154/200\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.0021 - val_loss: 0.0238\n",
      "Epoch 155/200\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.0012 - val_loss: 0.0238\n",
      "Epoch 156/200\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.0012 - val_loss: 0.0240\n",
      "Epoch 157/200\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.0013 - val_loss: 0.0250\n",
      "Epoch 158/200\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.0015 - val_loss: 0.0244\n",
      "Epoch 159/200\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.0017 - val_loss: 0.0298\n",
      "Epoch 160/200\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.0043 - val_loss: 0.0302\n",
      "Epoch 161/200\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.0033 - val_loss: 0.0348\n",
      "Epoch 162/200\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.0055 - val_loss: 0.0241\n",
      "Epoch 163/200\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.0017 - val_loss: 0.0239\n",
      "Epoch 164/200\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.0015 - val_loss: 0.0257\n",
      "Epoch 165/200\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.0017 - val_loss: 0.0232\n",
      "Epoch 166/200\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.0012 - val_loss: 0.0246\n",
      "Epoch 167/200\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.0014 - val_loss: 0.0247\n",
      "Epoch 168/200\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.0012 - val_loss: 0.0268\n",
      "Epoch 169/200\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.0023 - val_loss: 0.0279\n",
      "Epoch 170/200\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.0020 - val_loss: 0.0275\n",
      "Epoch 171/200\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.0050 - val_loss: 0.0314\n",
      "Epoch 172/200\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.0024 - val_loss: 0.0287\n",
      "Epoch 173/200\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.0029 - val_loss: 0.0245\n",
      "Epoch 174/200\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.0013 - val_loss: 0.0232\n",
      "Epoch 175/200\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.0011 - val_loss: 0.0241\n",
      "Epoch 176/200\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.0010 - val_loss: 0.0241\n",
      "Epoch 177/200\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 9.6527e-04 - val_loss: 0.0245\n",
      "Epoch 178/200\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 9.6664e-04 - val_loss: 0.0242\n",
      "Epoch 179/200\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.0014 - val_loss: 0.0295\n",
      "Epoch 180/200\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.0023 - val_loss: 0.0245\n",
      "Epoch 181/200\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.0038 - val_loss: 0.0394\n",
      "Epoch 182/200\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.0045 - val_loss: 0.0255\n",
      "Epoch 183/200\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.0033 - val_loss: 0.0318\n",
      "Epoch 184/200\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.0043 - val_loss: 0.0248\n",
      "Epoch 185/200\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.0018 - val_loss: 0.0244\n",
      "Epoch 186/200\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.0013 - val_loss: 0.0255\n",
      "Epoch 187/200\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.0013 - val_loss: 0.0237\n",
      "Epoch 188/200\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 9.9264e-04 - val_loss: 0.0236\n",
      "Epoch 189/200\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 9.7352e-04 - val_loss: 0.0244\n",
      "Epoch 190/200\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.0011 - val_loss: 0.0247\n",
      "Epoch 191/200\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.0010 - val_loss: 0.0238\n",
      "Epoch 192/200\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 9.9793e-04 - val_loss: 0.0261\n",
      "Epoch 193/200\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.0020 - val_loss: 0.0287\n",
      "Epoch 194/200\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.0018 - val_loss: 0.0289\n",
      "Epoch 195/200\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.0033 - val_loss: 0.0289\n",
      "Epoch 196/200\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.0023 - val_loss: 0.0270\n",
      "Epoch 197/200\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.0031 - val_loss: 0.0274\n",
      "Epoch 198/200\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.0019 - val_loss: 0.0303\n",
      "Epoch 199/200\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.0029 - val_loss: 0.0243\n",
      "Epoch 200/200\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.0013 - val_loss: 0.0235\n",
      "mae: tf.Tensor(32.583473, shape=(), dtype=float32)\n",
      "mse: tf.Tensor(2451.6838, shape=(), dtype=float32)\n",
      "rmse: tf.Tensor(49.51448, shape=(), dtype=float32)\n",
      "mae: tf.Tensor(32.583473, shape=(), dtype=float32)\n",
      "mse: tf.Tensor(2451.6838, shape=(), dtype=float32)\n",
      "rmse: tf.Tensor(49.51448, shape=(), dtype=float32)\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 43 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 43 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://app.neptune.ai/milestone2-california-water-shortage/deeplearning-lstm/e/DEEPLSTM-80\n"
     ]
    }
   ],
   "source": [
    "run = neptune.init(\n",
    "    project=\"milestone2-california-water-shortage/deeplearning-lstm\",\n",
    "    api_token=neptune_key,\n",
    "    name=\"Advanced Model 4\",\n",
    "    tags=[\"WithDetailedWellCounts\", \"OneUnidirectionalLSTM\", \"OneRepeatVector\", \"OneUnidirectionalLSTM\", \"TimeDistributedDense\", \"Dropout\"]\n",
    ")\n",
    "neptune_cbk = NeptuneCallback(run=run, base_namespace='metrics')\n",
    "run['hyper-parameters'] = m3_hyper_parameters\n",
    "\n",
    "model3.compile(loss=\"mse\", optimizer=m3_optimizer[m3_hyper_parameters[\"optimizer\"]], metrics=[keras.metrics.RootMeanSquaredError()])\n",
    "model3.fit(X_train, y_train_3d,\n",
    "                     validation_split=m3_hyper_parameters[\"validation_split\"],\n",
    "                     batch_size=m3_hyper_parameters[\"batch_size\"],\n",
    "                     epochs=m3_hyper_parameters[\"epochs\"],\n",
    "                     shuffle=False,\n",
    "                     callbacks=[neptune_cbk])\n",
    "yhat = model3.predict(X_test, verbose=0)\n",
    "yhat_inverse = scaler.inverse_transform(yhat.squeeze(2))\n",
    "y_test_inverse = scaler.inverse_transform(y_test)\n",
    "evaluate_forecast(y_test_inverse, yhat_inverse)\n",
    "mae, mse, rmse = evaluate_forecast(y_test_inverse, yhat_inverse)\n",
    "run[\"eval/mae\"] = mae\n",
    "run[\"eval/mse\"] = mse\n",
    "run[\"eval/rmse\"] = rmse\n",
    "run.stop()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}