{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, TimeDistributed, RepeatVector\n",
    "from keras.optimizers import RMSprop, Adam, Adamax, Adagrad\n",
    "\n",
    "from lib.read_data import read_and_join_output_file\n",
    "#from lib.create_pipeline import create_transformation_pipeline\n",
    "from lib.deeplearning import create_transformation_pipelines, evaluate_forecast\n",
    "from lib.transform_impute import convert_back_df\n",
    "from lib.split_data import train_test_group_time_split"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "RANDOM_SEED = 31\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tensorflow.random.set_seed(RANDOM_SEED)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tensorflow.config.list_physical_devices('GPU')))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preparing the Dataset\n",
    "### The Train-Test Split\n",
    "The dataset is made of 478 Township-Ranges, each containing a multivariate (81 features) time series (data between 2014 to 2021). This dataset can thus be seen as a 3 dimensional dataset of\n",
    "$478 TownshipRanges * 8 time stamps * 81 features$\n",
    "The objective is to predict the 2022 target value of GSE_GWE (Ground Surface Elevation to Groundwater Water Elevation - Depth to groundwater elevation in feet below ground surface) for each Township-Range.\n",
    "\n",
    "LSTM neural networks can be used for time series forecasting and take inputs of the shape *[samples, time series steps, features]*. This perfectly fits our dataset.\n",
    "To fit our dataset and objective, as well as LSTM neural networks architecture we will thus perform the train test split as follow:\n",
    "* Training and Test sets will be split by Township-Ranges. I.e., some Township-Ranges will have all their 2014-2021 data points in the training set, some others will be in the the test set.\n",
    "* The model will be trained based on the 2014-2020 data for all features - including the target feature - and will be trained and tested on the 2021 value of the target feature.\n",
    "\n",
    "With such a method, unlike a simple time series forecasting where the target feature is forecasted only based on its past value, we allow past value of other features (in our case cultivated crops, precipitations, population density, number of wells drilled) to influence the future value of the target feature.\n",
    "\n",
    "![Train-Test Split](../doc/images/deeplearning-train-test-split.jpg)\n",
    "### Data Imputation and Scaling\n",
    "For neural network we use a MinMax scaler to scale all values between 0 and 1.\n",
    "We do not need to do any data imputation on the training and test sets *y* target feature since it does not have any missing data point"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "test_size=0.15\n",
    "# Load the data from the ETL output files\n",
    "X = read_and_join_output_file()\n",
    "# Split the data into a training and a test set\n",
    "X_train_df, X_test_df, y_train_df, y_test_df = train_test_group_time_split(X, index=[\"TOWNSHIP_RANGE\", \"YEAR\"], group=\"TOWNSHIP_RANGE\", test_size=test_size, random_seed=RANDOM_SEED)\n",
    "# Create, fit and apply the data imputation pipeline to the training and test sets\n",
    "impute_pipeline, columns = create_transformation_pipelines(X_train_df)\n",
    "X_train_impute = impute_pipeline.fit_transform(X_train_df)\n",
    "X_test_impute = impute_pipeline.transform(X_test_df)\n",
    "# Convert the X_train and X_test back to dataframes\n",
    "X_train_impute_df = pd.DataFrame(X_train_impute, index=X_train_df.index, columns=columns)\n",
    "X_test_impute_df = pd.DataFrame(X_test_impute, index=X_test_df.index, columns=columns)\n",
    "X_train_impute_df[\"GSE_GWE\"] = np.sqrt(X_train_impute_df[\"GSE_GWE\"])\n",
    "X_test_impute_df[\"GSE_GWE\"] = np.sqrt(X_test_impute_df[\"GSE_GWE\"])\n",
    "# Keep only the GSE_GWE variable as the outcome variable\n",
    "scaler = MinMaxScaler()\n",
    "y_train = scaler.fit_transform(y_train_df[[\"GSE_GWE\"]])\n",
    "#y_train = np.sqrt(y_train)\n",
    "y_test = scaler.transform(y_test_df[[\"GSE_GWE\"]])\n",
    "y_train_3d = y_train[..., np.newaxis]\n",
    "X_train_impute_df\n",
    "\n",
    "nb_features = len(X_train_impute_df.columns)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "# Change the shape of the input array to (number of Township-Ranges, 7 years (2014-2020), the number of features)\n",
    "X_train = X_train_impute_df.values.reshape(len(X_train_impute_df.index.get_level_values(0).unique()), len(X_train_impute_df.index.get_level_values(1).unique()), X_train_impute_df.shape[1])\n",
    "X_test = X_test_impute_df.values.reshape(len(X_test_impute_df.index.get_level_values(0).unique()), len(X_test_impute_df.index.get_level_values(1).unique()), X_test_impute_df.shape[1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Checking the train, validation and test input (X) datasets sizes:\n",
      "Size of the X_train dataset: (406, 7, 81)\n",
      "Size of the X_test dataset: (72, 7, 81)\n",
      "====================================================================================================\n",
      "Checking the train, validation and test output (y) datasets sizes:\n",
      "Size of the y_train dataset: (406, 1)\n",
      "Size of the y_test dataset: (72, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*100)\n",
    "print(\"Checking the train, validation and test input (X) datasets sizes:\")\n",
    "print(f\"Size of the X_train dataset: {X_train.shape}\")\n",
    "#print(f\"Size of the X_val dataset: {X_val.shape}\")\n",
    "print(f\"Size of the X_test dataset: {X_test.shape}\")\n",
    "print(\"=\"*100)\n",
    "print(\"Checking the train, validation and test output (y) datasets sizes:\")\n",
    "print(f\"Size of the y_train dataset: {y_train.shape}\")\n",
    "#print(f\"Size of the y_val dataset: {y_val_df.shape}\")\n",
    "print(f\"Size of the y_test dataset: {y_test.shape}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_test_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[1;32mIn [1]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[1;32m----> 1\u001B[0m results_df \u001B[38;5;241m=\u001B[39m \u001B[43my_test_df\u001B[49m[[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mGSE_GWE\u001B[39m\u001B[38;5;124m\"\u001B[39m]]\u001B[38;5;241m.\u001B[39mcopy()\n",
      "\u001B[1;31mNameError\u001B[0m: name 'y_test_df' is not defined"
     ]
    }
   ],
   "source": [
    "results_df = y_test_df[[\"GSE_GWE\"]].copy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training Different Models\n",
    "We tried 3 different LSTM models:\n",
    "* A simple model made of a single *LSTM* layer and an output *Dense* layer\n",
    "* A model made of a *LSTM* layer followed by a *Dense* and *Dropout* layers before the output layer\n",
    "* An Encoder-Decoder model made of 2 LSTM models\n",
    "\n",
    "### The Simple LSTM Model\n",
    "![Simple LSTM Model](../doc/images/deeplearning-architecture-1.jpg)\n",
    "\n",
    "### LSTM Model With a Dense Layer\n",
    "![LSTM Model With Dense Layer](../doc/images/deeplearning-architecture-2.jpg)\n",
    "\n",
    "### Encoder-Decoder LSTM Model\n",
    "Encoder-decoder architectures are more common for sequence to sequence learning e.g., when forecast of the next 3 days based on the past months or years data. In our case we only predict data for 1 time step in the feature. The output sequence being of length 1 this architecture might seem superfluous but has been tested anyway. This architecture was inspired by the Encoder-Decoder architecture in this article: *[CNN-LSTM-Based Models for Multiple Parallel Input and Multi-Step Forecast](https://towardsdatascience.com/cnn-lstm-based-models-for-multiple-parallel-input-and-multi-step-forecast-6fe2172f7668)*.\n",
    "\n",
    "The model is made of\n",
    "* an encoding *LSTM* layer\n",
    "* a *RepeatVector* Layer. The role of this layer is simply to repeat the output of the encoding LSTM layers for the number of time steps in the output sequence (in our case 1).\n",
    "* a decoding *LSTM* layer\n",
    "* a fully connected *Dense* layer is applied to each time step using the *TimeDistributed* wrapper\n",
    "* a *Dropout* layer\n",
    "\n",
    "As such models are made for sequence to sequence learning and forecasting, the output os such a model is different from the previous ones. It has an output of size *[samples, forcasting sequence length, target features]*. In our case the forecasting sequence length and number of target features are both 1.\n",
    "\n",
    "![Encoder-Decoder LSTM Model](../doc/images/deeplearning-architecture-3.jpg)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Simple Model\n",
    "This model is just made of a single LSTM layer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_1 (LSTM)               (None, 40)                19520     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 41        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 19,561\n",
      "Trainable params: 19,561\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "m1_hyper_parameters = {\n",
    "    \"random_seed\": RANDOM_SEED,\n",
    "    \"test_size\": test_size,\n",
    "    \"validation_split\": 0.05,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"batch_size\": 32,\n",
    "    \"epochs\": 200,\n",
    "    \"lstm_units\": 80,\n",
    "    \"lstm_activation\": \"sigmoid\",\n",
    "    \"output_activation\": \"linear\",\n",
    "    \"nb_features\": nb_features,\n",
    "    \"optimizer\": \"Adam\"\n",
    "}\n",
    "\n",
    "m1_hyper_parameters = {\n",
    "    \"random_seed\": RANDOM_SEED,\n",
    "    \"test_size\": test_size,\n",
    "    \"validation_split\": 0.1,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"batch_size\": 32,\n",
    "    \"epochs\": 200,\n",
    "    \"lstm_units\": 40,\n",
    "    \"lstm_activation\": \"sigmoid\",\n",
    "    \"output_activation\": \"linear\",\n",
    "    \"nb_features\": nb_features,\n",
    "    \"optimizer\": \"Adam\"\n",
    "}\n",
    "\n",
    "m1_optimizer = {\n",
    "    \"RMSprop\": RMSprop(learning_rate=m1_hyper_parameters[\"learning_rate\"]),\n",
    "    \"Adam\": Adam(learning_rate=m1_hyper_parameters[\"learning_rate\"]),\n",
    "    \"Adamax\": Adamax(learning_rate=m1_hyper_parameters[\"learning_rate\"]),\n",
    "    \"Adagrad\": Adagrad(learning_rate=m1_hyper_parameters[\"learning_rate\"])\n",
    "}\n",
    "\n",
    "model1 = Sequential()\n",
    "model1.add(LSTM(m1_hyper_parameters[\"lstm_units\"], activation=m1_hyper_parameters[\"lstm_activation\"], input_shape=(7, nb_features)))\n",
    "model1.add(Dense(1, activation=m1_hyper_parameters[\"output_activation\"]))\n",
    "model1.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [28]\u001B[0m, in \u001B[0;36m<cell line: 2>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m model1\u001B[38;5;241m.\u001B[39mcompile(loss\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmse\u001B[39m\u001B[38;5;124m\"\u001B[39m, optimizer\u001B[38;5;241m=\u001B[39mm1_optimizer[m1_hyper_parameters[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moptimizer\u001B[39m\u001B[38;5;124m\"\u001B[39m]], metrics\u001B[38;5;241m=\u001B[39m[keras\u001B[38;5;241m.\u001B[39mmetrics\u001B[38;5;241m.\u001B[39mRootMeanSquaredError()])\n\u001B[1;32m----> 2\u001B[0m \u001B[43mmodel1\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      3\u001B[0m \u001B[43m                     \u001B[49m\u001B[43mvalidation_split\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mm1_hyper_parameters\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mvalidation_split\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      4\u001B[0m \u001B[43m                     \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mm1_hyper_parameters\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mbatch_size\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      5\u001B[0m \u001B[43m                     \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mm1_hyper_parameters\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mepochs\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      6\u001B[0m \u001B[43m                     \u001B[49m\u001B[43mshuffle\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m      7\u001B[0m yhat \u001B[38;5;241m=\u001B[39m model1\u001B[38;5;241m.\u001B[39mpredict(X_test, verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n\u001B[0;32m      8\u001B[0m yhat_inverse \u001B[38;5;241m=\u001B[39m scaler\u001B[38;5;241m.\u001B[39minverse_transform(yhat)\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\milestone2\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     62\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m     63\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 64\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m     65\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:  \u001B[38;5;66;03m# pylint: disable=broad-except\u001B[39;00m\n\u001B[0;32m     66\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\milestone2\\lib\\site-packages\\keras\\engine\\training.py:1409\u001B[0m, in \u001B[0;36mModel.fit\u001B[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[0;32m   1402\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m tf\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mexperimental\u001B[38;5;241m.\u001B[39mTrace(\n\u001B[0;32m   1403\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m   1404\u001B[0m     epoch_num\u001B[38;5;241m=\u001B[39mepoch,\n\u001B[0;32m   1405\u001B[0m     step_num\u001B[38;5;241m=\u001B[39mstep,\n\u001B[0;32m   1406\u001B[0m     batch_size\u001B[38;5;241m=\u001B[39mbatch_size,\n\u001B[0;32m   1407\u001B[0m     _r\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m):\n\u001B[0;32m   1408\u001B[0m   callbacks\u001B[38;5;241m.\u001B[39mon_train_batch_begin(step)\n\u001B[1;32m-> 1409\u001B[0m   tmp_logs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43miterator\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1410\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m data_handler\u001B[38;5;241m.\u001B[39mshould_sync:\n\u001B[0;32m   1411\u001B[0m     context\u001B[38;5;241m.\u001B[39masync_wait()\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\milestone2\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    148\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    149\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 150\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    151\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    152\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\milestone2\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001B[0m, in \u001B[0;36mFunction.__call__\u001B[1;34m(self, *args, **kwds)\u001B[0m\n\u001B[0;32m    912\u001B[0m compiler \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mxla\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jit_compile \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnonXla\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    914\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m OptionalXlaContext(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jit_compile):\n\u001B[1;32m--> 915\u001B[0m   result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)\n\u001B[0;32m    917\u001B[0m new_tracing_count \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexperimental_get_tracing_count()\n\u001B[0;32m    918\u001B[0m without_tracing \u001B[38;5;241m=\u001B[39m (tracing_count \u001B[38;5;241m==\u001B[39m new_tracing_count)\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\milestone2\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:980\u001B[0m, in \u001B[0;36mFunction._call\u001B[1;34m(self, *args, **kwds)\u001B[0m\n\u001B[0;32m    976\u001B[0m     \u001B[38;5;28;01mpass\u001B[39;00m  \u001B[38;5;66;03m# Fall through to cond-based initialization.\u001B[39;00m\n\u001B[0;32m    977\u001B[0m   \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    978\u001B[0m     \u001B[38;5;66;03m# Lifting succeeded, so variables are initialized and we can run the\u001B[39;00m\n\u001B[0;32m    979\u001B[0m     \u001B[38;5;66;03m# stateless function.\u001B[39;00m\n\u001B[1;32m--> 980\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_stateless_fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)\n\u001B[0;32m    981\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    982\u001B[0m   _, _, filtered_flat_args \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m    983\u001B[0m       \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_stateful_fn\u001B[38;5;241m.\u001B[39m_function_spec\u001B[38;5;241m.\u001B[39mcanonicalize_function_inputs(  \u001B[38;5;66;03m# pylint: disable=protected-access\u001B[39;00m\n\u001B[0;32m    984\u001B[0m           \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds))\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\milestone2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2452\u001B[0m, in \u001B[0;36mFunction.__call__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   2449\u001B[0m \u001B[38;5;124;03m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001B[39;00m\n\u001B[0;32m   2450\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock:\n\u001B[0;32m   2451\u001B[0m   (graph_function,\n\u001B[1;32m-> 2452\u001B[0m    filtered_flat_args) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_maybe_define_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2453\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m graph_function\u001B[38;5;241m.\u001B[39m_call_flat(\n\u001B[0;32m   2454\u001B[0m     filtered_flat_args, captured_inputs\u001B[38;5;241m=\u001B[39mgraph_function\u001B[38;5;241m.\u001B[39mcaptured_inputs)\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\milestone2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2711\u001B[0m, in \u001B[0;36mFunction._maybe_define_function\u001B[1;34m(self, args, kwargs)\u001B[0m\n\u001B[0;32m   2708\u001B[0m   cache_key \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_function_cache\u001B[38;5;241m.\u001B[39mgeneralize(cache_key)\n\u001B[0;32m   2709\u001B[0m   (args, kwargs) \u001B[38;5;241m=\u001B[39m cache_key\u001B[38;5;241m.\u001B[39m_placeholder_value()  \u001B[38;5;66;03m# pylint: disable=protected-access\u001B[39;00m\n\u001B[1;32m-> 2711\u001B[0m graph_function \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_create_graph_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2712\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_function_cache\u001B[38;5;241m.\u001B[39madd(cache_key, cache_key_deletion_observer,\n\u001B[0;32m   2713\u001B[0m                          graph_function)\n\u001B[0;32m   2715\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m graph_function, filtered_flat_args\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\milestone2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2627\u001B[0m, in \u001B[0;36mFunction._create_graph_function\u001B[1;34m(self, args, kwargs)\u001B[0m\n\u001B[0;32m   2622\u001B[0m missing_arg_names \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m   2623\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m_\u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m (arg, i) \u001B[38;5;28;01mfor\u001B[39;00m i, arg \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(missing_arg_names)\n\u001B[0;32m   2624\u001B[0m ]\n\u001B[0;32m   2625\u001B[0m arg_names \u001B[38;5;241m=\u001B[39m base_arg_names \u001B[38;5;241m+\u001B[39m missing_arg_names\n\u001B[0;32m   2626\u001B[0m graph_function \u001B[38;5;241m=\u001B[39m ConcreteFunction(\n\u001B[1;32m-> 2627\u001B[0m     \u001B[43mfunc_graph_module\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfunc_graph_from_py_func\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   2628\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2629\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_python_function\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2630\u001B[0m \u001B[43m        \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2631\u001B[0m \u001B[43m        \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2632\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minput_signature\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2633\u001B[0m \u001B[43m        \u001B[49m\u001B[43mautograph\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_autograph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2634\u001B[0m \u001B[43m        \u001B[49m\u001B[43mautograph_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_autograph_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2635\u001B[0m \u001B[43m        \u001B[49m\u001B[43marg_names\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43marg_names\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2636\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcapture_by_value\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_capture_by_value\u001B[49m\u001B[43m)\u001B[49m,\n\u001B[0;32m   2637\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_function_attributes,\n\u001B[0;32m   2638\u001B[0m     spec\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfunction_spec,\n\u001B[0;32m   2639\u001B[0m     \u001B[38;5;66;03m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001B[39;00m\n\u001B[0;32m   2640\u001B[0m     \u001B[38;5;66;03m# scope. This is not the default behavior since it gets used in some\u001B[39;00m\n\u001B[0;32m   2641\u001B[0m     \u001B[38;5;66;03m# places (like Keras) where the FuncGraph lives longer than the\u001B[39;00m\n\u001B[0;32m   2642\u001B[0m     \u001B[38;5;66;03m# ConcreteFunction.\u001B[39;00m\n\u001B[0;32m   2643\u001B[0m     shared_func_graph\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m   2644\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m graph_function\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\milestone2\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:1141\u001B[0m, in \u001B[0;36mfunc_graph_from_py_func\u001B[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, acd_record_initial_resource_uses)\u001B[0m\n\u001B[0;32m   1138\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1139\u001B[0m   _, original_func \u001B[38;5;241m=\u001B[39m tf_decorator\u001B[38;5;241m.\u001B[39munwrap(python_func)\n\u001B[1;32m-> 1141\u001B[0m func_outputs \u001B[38;5;241m=\u001B[39m python_func(\u001B[38;5;241m*\u001B[39mfunc_args, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfunc_kwargs)\n\u001B[0;32m   1143\u001B[0m \u001B[38;5;66;03m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001B[39;00m\n\u001B[0;32m   1144\u001B[0m \u001B[38;5;66;03m# TensorArrays and `None`s.\u001B[39;00m\n\u001B[0;32m   1145\u001B[0m func_outputs \u001B[38;5;241m=\u001B[39m nest\u001B[38;5;241m.\u001B[39mmap_structure(\n\u001B[0;32m   1146\u001B[0m     convert, func_outputs, expand_composites\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\milestone2\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:677\u001B[0m, in \u001B[0;36mFunction._defun_with_scope.<locals>.wrapped_fn\u001B[1;34m(*args, **kwds)\u001B[0m\n\u001B[0;32m    673\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m default_graph\u001B[38;5;241m.\u001B[39m_variable_creator_scope(scope, priority\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m50\u001B[39m):  \u001B[38;5;66;03m# pylint: disable=protected-access\u001B[39;00m\n\u001B[0;32m    674\u001B[0m   \u001B[38;5;66;03m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001B[39;00m\n\u001B[0;32m    675\u001B[0m   \u001B[38;5;66;03m# the function a weak reference to itself to avoid a reference cycle.\u001B[39;00m\n\u001B[0;32m    676\u001B[0m   \u001B[38;5;28;01mwith\u001B[39;00m OptionalXlaContext(compile_with_xla):\n\u001B[1;32m--> 677\u001B[0m     out \u001B[38;5;241m=\u001B[39m weak_wrapped_fn()\u001B[38;5;241m.\u001B[39m__wrapped__(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)\n\u001B[0;32m    678\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m out\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\milestone2\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:1116\u001B[0m, in \u001B[0;36mfunc_graph_from_py_func.<locals>.autograph_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m   1114\u001B[0m \u001B[38;5;66;03m# TODO(mdan): Push this block higher in tf.function's call stack.\u001B[39;00m\n\u001B[0;32m   1115\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1116\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mautograph\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconverted_call\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1117\u001B[0m \u001B[43m      \u001B[49m\u001B[43moriginal_func\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1118\u001B[0m \u001B[43m      \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1119\u001B[0m \u001B[43m      \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1120\u001B[0m \u001B[43m      \u001B[49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mautograph\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mConversionOptions\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1121\u001B[0m \u001B[43m          \u001B[49m\u001B[43mrecursive\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m   1122\u001B[0m \u001B[43m          \u001B[49m\u001B[43moptional_features\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mautograph_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1123\u001B[0m \u001B[43m          \u001B[49m\u001B[43muser_requested\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m   1124\u001B[0m \u001B[43m      \u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1125\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:  \u001B[38;5;66;03m# pylint:disable=broad-except\u001B[39;00m\n\u001B[0;32m   1126\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(e, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mag_error_metadata\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\milestone2\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:439\u001B[0m, in \u001B[0;36mconverted_call\u001B[1;34m(f, args, kwargs, caller_fn_scope, options)\u001B[0m\n\u001B[0;32m    437\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    438\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m kwargs \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 439\u001B[0m     result \u001B[38;5;241m=\u001B[39m converted_f(\u001B[38;5;241m*\u001B[39meffective_args, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    440\u001B[0m   \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    441\u001B[0m     result \u001B[38;5;241m=\u001B[39m converted_f(\u001B[38;5;241m*\u001B[39meffective_args)\n",
      "File \u001B[1;32mD:\\Users\\Matthieu\\AppData\\Local\\Temp\\__autograph_generated_filehp7mfz6u.py:15\u001B[0m, in \u001B[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001B[1;34m(iterator)\u001B[0m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m     14\u001B[0m     do_return \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m---> 15\u001B[0m     retval_ \u001B[38;5;241m=\u001B[39m \u001B[43mag__\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconverted_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43mag__\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mld\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstep_function\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mag__\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mld\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mag__\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mld\u001B[49m\u001B[43m(\u001B[49m\u001B[43miterator\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfscope\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     16\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m:\n\u001B[0;32m     17\u001B[0m     do_return \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\milestone2\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:331\u001B[0m, in \u001B[0;36mconverted_call\u001B[1;34m(f, args, kwargs, caller_fn_scope, options)\u001B[0m\n\u001B[0;32m    329\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m conversion\u001B[38;5;241m.\u001B[39mis_in_allowlist_cache(f, options):\n\u001B[0;32m    330\u001B[0m   logging\u001B[38;5;241m.\u001B[39mlog(\u001B[38;5;241m2\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mAllowlisted \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m: from cache\u001B[39m\u001B[38;5;124m'\u001B[39m, f)\n\u001B[1;32m--> 331\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_call_unconverted\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptions\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m    333\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m ag_ctx\u001B[38;5;241m.\u001B[39mcontrol_status_ctx()\u001B[38;5;241m.\u001B[39mstatus \u001B[38;5;241m==\u001B[39m ag_ctx\u001B[38;5;241m.\u001B[39mStatus\u001B[38;5;241m.\u001B[39mDISABLED:\n\u001B[0;32m    334\u001B[0m   logging\u001B[38;5;241m.\u001B[39mlog(\u001B[38;5;241m2\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mAllowlisted: \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m: AutoGraph is disabled in context\u001B[39m\u001B[38;5;124m'\u001B[39m, f)\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\milestone2\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:459\u001B[0m, in \u001B[0;36m_call_unconverted\u001B[1;34m(f, args, kwargs, options, update_cache)\u001B[0m\n\u001B[0;32m    457\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m kwargs \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    458\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m f(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m--> 459\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\milestone2\\lib\\site-packages\\keras\\engine\\training.py:1040\u001B[0m, in \u001B[0;36mModel.make_train_function.<locals>.step_function\u001B[1;34m(model, iterator)\u001B[0m\n\u001B[0;32m   1037\u001B[0m   run_step \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mfunction(\n\u001B[0;32m   1038\u001B[0m       run_step, jit_compile\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, reduce_retracing\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m   1039\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mnext\u001B[39m(iterator)\n\u001B[1;32m-> 1040\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdistribute_strategy\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrun_step\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1041\u001B[0m outputs \u001B[38;5;241m=\u001B[39m reduce_per_replica(\n\u001B[0;32m   1042\u001B[0m     outputs, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdistribute_strategy, reduction\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfirst\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m   1043\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m outputs\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\milestone2\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1312\u001B[0m, in \u001B[0;36mStrategyBase.run\u001B[1;34m(***failed resolving arguments***)\u001B[0m\n\u001B[0;32m   1307\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mscope():\n\u001B[0;32m   1308\u001B[0m   \u001B[38;5;66;03m# tf.distribute supports Eager functions, so AutoGraph should not be\u001B[39;00m\n\u001B[0;32m   1309\u001B[0m   \u001B[38;5;66;03m# applied when the caller is also in Eager mode.\u001B[39;00m\n\u001B[0;32m   1310\u001B[0m   fn \u001B[38;5;241m=\u001B[39m autograph\u001B[38;5;241m.\u001B[39mtf_convert(\n\u001B[0;32m   1311\u001B[0m       fn, autograph_ctx\u001B[38;5;241m.\u001B[39mcontrol_status_ctx(), convert_by_default\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m-> 1312\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_extended\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcall_for_each_replica\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\milestone2\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2888\u001B[0m, in \u001B[0;36mStrategyExtendedV1.call_for_each_replica\u001B[1;34m(self, fn, args, kwargs)\u001B[0m\n\u001B[0;32m   2886\u001B[0m   kwargs \u001B[38;5;241m=\u001B[39m {}\n\u001B[0;32m   2887\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_container_strategy()\u001B[38;5;241m.\u001B[39mscope():\n\u001B[1;32m-> 2888\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_for_each_replica\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\milestone2\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3689\u001B[0m, in \u001B[0;36m_DefaultDistributionExtended._call_for_each_replica\u001B[1;34m(self, fn, args, kwargs)\u001B[0m\n\u001B[0;32m   3687\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_call_for_each_replica\u001B[39m(\u001B[38;5;28mself\u001B[39m, fn, args, kwargs):\n\u001B[0;32m   3688\u001B[0m   \u001B[38;5;28;01mwith\u001B[39;00m ReplicaContext(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_container_strategy(), replica_id_in_sync_group\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m):\n\u001B[1;32m-> 3689\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\milestone2\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:689\u001B[0m, in \u001B[0;36mconvert.<locals>.decorator.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    687\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    688\u001B[0m   \u001B[38;5;28;01mwith\u001B[39;00m conversion_ctx:\n\u001B[1;32m--> 689\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mconverted_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    690\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:  \u001B[38;5;66;03m# pylint:disable=broad-except\u001B[39;00m\n\u001B[0;32m    691\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(e, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mag_error_metadata\u001B[39m\u001B[38;5;124m'\u001B[39m):\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\milestone2\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:377\u001B[0m, in \u001B[0;36mconverted_call\u001B[1;34m(f, args, kwargs, caller_fn_scope, options)\u001B[0m\n\u001B[0;32m    374\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m _call_unconverted(f, args, kwargs, options)\n\u001B[0;32m    376\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m options\u001B[38;5;241m.\u001B[39muser_requested \u001B[38;5;129;01mand\u001B[39;00m conversion\u001B[38;5;241m.\u001B[39mis_allowlisted(f):\n\u001B[1;32m--> 377\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_call_unconverted\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptions\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    379\u001B[0m \u001B[38;5;66;03m# internal_convert_user_code is for example turned off when issuing a dynamic\u001B[39;00m\n\u001B[0;32m    380\u001B[0m \u001B[38;5;66;03m# call conversion from generated code while in nonrecursive mode. In that\u001B[39;00m\n\u001B[0;32m    381\u001B[0m \u001B[38;5;66;03m# case we evidently don't want to recurse, but we still have to convert\u001B[39;00m\n\u001B[0;32m    382\u001B[0m \u001B[38;5;66;03m# things like builtins.\u001B[39;00m\n\u001B[0;32m    383\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m options\u001B[38;5;241m.\u001B[39minternal_convert_user_code:\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\milestone2\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:458\u001B[0m, in \u001B[0;36m_call_unconverted\u001B[1;34m(f, args, kwargs, options, update_cache)\u001B[0m\n\u001B[0;32m    455\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m f\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__self__\u001B[39m\u001B[38;5;241m.\u001B[39mcall(args, kwargs)\n\u001B[0;32m    457\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m kwargs \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 458\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m f(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    459\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m f(\u001B[38;5;241m*\u001B[39margs)\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\milestone2\\lib\\site-packages\\keras\\engine\\training.py:1030\u001B[0m, in \u001B[0;36mModel.make_train_function.<locals>.step_function.<locals>.run_step\u001B[1;34m(data)\u001B[0m\n\u001B[0;32m   1029\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrun_step\u001B[39m(data):\n\u001B[1;32m-> 1030\u001B[0m   outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1031\u001B[0m   \u001B[38;5;66;03m# Ensure counter is updated only if `train_step` succeeds.\u001B[39;00m\n\u001B[0;32m   1032\u001B[0m   \u001B[38;5;28;01mwith\u001B[39;00m tf\u001B[38;5;241m.\u001B[39mcontrol_dependencies(_minimum_control_deps(outputs)):\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\milestone2\\lib\\site-packages\\keras\\engine\\training.py:893\u001B[0m, in \u001B[0;36mModel.train_step\u001B[1;34m(self, data)\u001B[0m\n\u001B[0;32m    891\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_validate_target_and_loss(y, loss)\n\u001B[0;32m    892\u001B[0m \u001B[38;5;66;03m# Run backwards pass.\u001B[39;00m\n\u001B[1;32m--> 893\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mminimize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mloss\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrainable_variables\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtape\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtape\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    894\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompute_metrics(x, y, y_pred, sample_weight)\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\milestone2\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py:537\u001B[0m, in \u001B[0;36mOptimizerV2.minimize\u001B[1;34m(self, loss, var_list, grad_loss, name, tape)\u001B[0m\n\u001B[0;32m    506\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mminimize\u001B[39m(\u001B[38;5;28mself\u001B[39m, loss, var_list, grad_loss\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, name\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, tape\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m    507\u001B[0m   \u001B[38;5;124;03m\"\"\"Minimize `loss` by updating `var_list`.\u001B[39;00m\n\u001B[0;32m    508\u001B[0m \n\u001B[0;32m    509\u001B[0m \u001B[38;5;124;03m  This method simply computes gradient using `tf.GradientTape` and calls\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    535\u001B[0m \n\u001B[0;32m    536\u001B[0m \u001B[38;5;124;03m  \"\"\"\u001B[39;00m\n\u001B[1;32m--> 537\u001B[0m   grads_and_vars \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_compute_gradients\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    538\u001B[0m \u001B[43m      \u001B[49m\u001B[43mloss\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvar_list\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvar_list\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_loss\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgrad_loss\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtape\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtape\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    539\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapply_gradients(grads_and_vars, name\u001B[38;5;241m=\u001B[39mname)\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\milestone2\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py:590\u001B[0m, in \u001B[0;36mOptimizerV2._compute_gradients\u001B[1;34m(self, loss, var_list, grad_loss, tape)\u001B[0m\n\u001B[0;32m    588\u001B[0m var_list \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mnest\u001B[38;5;241m.\u001B[39mflatten(var_list)\n\u001B[0;32m    589\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m tf\u001B[38;5;241m.\u001B[39mname_scope(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_name \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/gradients\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m--> 590\u001B[0m   grads_and_vars \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_gradients\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtape\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloss\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvar_list\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_loss\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    592\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_assert_valid_dtypes([\n\u001B[0;32m    593\u001B[0m     v \u001B[38;5;28;01mfor\u001B[39;00m g, v \u001B[38;5;129;01min\u001B[39;00m grads_and_vars\n\u001B[0;32m    594\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m g \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m v\u001B[38;5;241m.\u001B[39mdtype \u001B[38;5;241m!=\u001B[39m tf\u001B[38;5;241m.\u001B[39mresource\n\u001B[0;32m    595\u001B[0m ])\n\u001B[0;32m    597\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m grads_and_vars\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\milestone2\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py:471\u001B[0m, in \u001B[0;36mOptimizerV2._get_gradients\u001B[1;34m(self, tape, loss, var_list, grad_loss)\u001B[0m\n\u001B[0;32m    469\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_get_gradients\u001B[39m(\u001B[38;5;28mself\u001B[39m, tape, loss, var_list, grad_loss\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m    470\u001B[0m   \u001B[38;5;124;03m\"\"\"Called in `minimize` to compute gradients from loss.\"\"\"\u001B[39;00m\n\u001B[1;32m--> 471\u001B[0m   grads \u001B[38;5;241m=\u001B[39m \u001B[43mtape\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgradient\u001B[49m\u001B[43m(\u001B[49m\u001B[43mloss\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvar_list\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_loss\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    472\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mzip\u001B[39m(grads, var_list))\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\milestone2\\lib\\site-packages\\tensorflow\\python\\eager\\backprop.py:1100\u001B[0m, in \u001B[0;36mGradientTape.gradient\u001B[1;34m(self, target, sources, output_gradients, unconnected_gradients)\u001B[0m\n\u001B[0;32m   1094\u001B[0m   output_gradients \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m   1095\u001B[0m       composite_tensor_gradient\u001B[38;5;241m.\u001B[39mget_flat_tensors_for_gradients(\n\u001B[0;32m   1096\u001B[0m           output_gradients))\n\u001B[0;32m   1097\u001B[0m   output_gradients \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01mif\u001B[39;00m x \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m ops\u001B[38;5;241m.\u001B[39mconvert_to_tensor(x)\n\u001B[0;32m   1098\u001B[0m                       \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m output_gradients]\n\u001B[1;32m-> 1100\u001B[0m flat_grad \u001B[38;5;241m=\u001B[39m \u001B[43mimperative_grad\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mimperative_grad\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1101\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_tape\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1102\u001B[0m \u001B[43m    \u001B[49m\u001B[43mflat_targets\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1103\u001B[0m \u001B[43m    \u001B[49m\u001B[43mflat_sources\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1104\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_gradients\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_gradients\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1105\u001B[0m \u001B[43m    \u001B[49m\u001B[43msources_raw\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mflat_sources_raw\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1106\u001B[0m \u001B[43m    \u001B[49m\u001B[43munconnected_gradients\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43munconnected_gradients\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1108\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_persistent:\n\u001B[0;32m   1109\u001B[0m   \u001B[38;5;66;03m# Keep track of watched variables before setting tape to None\u001B[39;00m\n\u001B[0;32m   1110\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_watched_variables \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tape\u001B[38;5;241m.\u001B[39mwatched_variables()\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\milestone2\\lib\\site-packages\\tensorflow\\python\\eager\\imperative_grad.py:67\u001B[0m, in \u001B[0;36mimperative_grad\u001B[1;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001B[0m\n\u001B[0;32m     63\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m:\n\u001B[0;32m     64\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m     65\u001B[0m       \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnknown value for unconnected_gradients: \u001B[39m\u001B[38;5;132;01m%r\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m unconnected_gradients)\n\u001B[1;32m---> 67\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mpywrap_tfe\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTFE_Py_TapeGradient\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     68\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtape\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_tape\u001B[49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# pylint: disable=protected-access\u001B[39;49;00m\n\u001B[0;32m     69\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     70\u001B[0m \u001B[43m    \u001B[49m\u001B[43msources\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     71\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_gradients\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     72\u001B[0m \u001B[43m    \u001B[49m\u001B[43msources_raw\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     73\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcompat\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mas_str\u001B[49m\u001B[43m(\u001B[49m\u001B[43munconnected_gradients\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvalue\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\milestone2\\lib\\site-packages\\tensorflow\\python\\eager\\backprop.py:157\u001B[0m, in \u001B[0;36m_gradient_function\u001B[1;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001B[0m\n\u001B[0;32m    155\u001B[0m     gradient_name_scope \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m forward_pass_name_scope \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    156\u001B[0m   \u001B[38;5;28;01mwith\u001B[39;00m ops\u001B[38;5;241m.\u001B[39mname_scope(gradient_name_scope):\n\u001B[1;32m--> 157\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mgrad_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmock_op\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mout_grads\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    158\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    159\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m grad_fn(mock_op, \u001B[38;5;241m*\u001B[39mout_grads)\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\milestone2\\lib\\site-packages\\tensorflow\\python\\ops\\while_v2.py:455\u001B[0m, in \u001B[0;36m_WhileGrad\u001B[1;34m(op, *grads)\u001B[0m\n\u001B[0;32m    444\u001B[0m outputs \u001B[38;5;241m=\u001B[39m _build_while_op(\n\u001B[0;32m    445\u001B[0m     loop_vars,\n\u001B[0;32m    446\u001B[0m     cond_grad_graph,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    451\u001B[0m     num_original_outputs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mlen\u001B[39m(body_grad_graph\u001B[38;5;241m.\u001B[39moutputs),\n\u001B[0;32m    452\u001B[0m     stateful_parallelism\u001B[38;5;241m=\u001B[39mstateful_parallelism)\n\u001B[0;32m    454\u001B[0m \u001B[38;5;66;03m# See comment in while_loop.\u001B[39;00m\n\u001B[1;32m--> 455\u001B[0m outputs \u001B[38;5;241m=\u001B[39m [array_ops\u001B[38;5;241m.\u001B[39midentity(t) \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m outputs]\n\u001B[0;32m    456\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _get_structured_grad_output(outputs, grads, body_grad_graph)\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\milestone2\\lib\\site-packages\\tensorflow\\python\\ops\\while_v2.py:455\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m    444\u001B[0m outputs \u001B[38;5;241m=\u001B[39m _build_while_op(\n\u001B[0;32m    445\u001B[0m     loop_vars,\n\u001B[0;32m    446\u001B[0m     cond_grad_graph,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    451\u001B[0m     num_original_outputs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mlen\u001B[39m(body_grad_graph\u001B[38;5;241m.\u001B[39moutputs),\n\u001B[0;32m    452\u001B[0m     stateful_parallelism\u001B[38;5;241m=\u001B[39mstateful_parallelism)\n\u001B[0;32m    454\u001B[0m \u001B[38;5;66;03m# See comment in while_loop.\u001B[39;00m\n\u001B[1;32m--> 455\u001B[0m outputs \u001B[38;5;241m=\u001B[39m [\u001B[43marray_ops\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43midentity\u001B[49m\u001B[43m(\u001B[49m\u001B[43mt\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m outputs]\n\u001B[0;32m    456\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _get_structured_grad_output(outputs, grads, body_grad_graph)\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\milestone2\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    148\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    149\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 150\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    151\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    152\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\milestone2\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1082\u001B[0m, in \u001B[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m   1080\u001B[0m \u001B[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001B[39;00m\n\u001B[0;32m   1081\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1082\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m dispatch_target(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1083\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (\u001B[38;5;167;01mTypeError\u001B[39;00m, \u001B[38;5;167;01mValueError\u001B[39;00m):\n\u001B[0;32m   1084\u001B[0m   \u001B[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001B[39;00m\n\u001B[0;32m   1085\u001B[0m   \u001B[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001B[39;00m\n\u001B[0;32m   1086\u001B[0m   result \u001B[38;5;241m=\u001B[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\milestone2\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:295\u001B[0m, in \u001B[0;36midentity\u001B[1;34m(input, name)\u001B[0m\n\u001B[0;32m    291\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m context\u001B[38;5;241m.\u001B[39mexecuting_eagerly() \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgraph\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m    292\u001B[0m   \u001B[38;5;66;03m# Make sure we get an input with handle data attached from resource\u001B[39;00m\n\u001B[0;32m    293\u001B[0m   \u001B[38;5;66;03m# variables. Variables have correct handle data when graph building.\u001B[39;00m\n\u001B[0;32m    294\u001B[0m   \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m ops\u001B[38;5;241m.\u001B[39mconvert_to_tensor(\u001B[38;5;28minput\u001B[39m)\n\u001B[1;32m--> 295\u001B[0m ret \u001B[38;5;241m=\u001B[39m \u001B[43mgen_array_ops\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43midentity\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    296\u001B[0m \u001B[38;5;66;03m# Propagate handle data for happier shape inference for resource variables.\u001B[39;00m\n\u001B[0;32m    297\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_handle_data\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\milestone2\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py:4076\u001B[0m, in \u001B[0;36midentity\u001B[1;34m(input, name)\u001B[0m\n\u001B[0;32m   4074\u001B[0m     \u001B[38;5;28;01mpass\u001B[39;00m  \u001B[38;5;66;03m# Add nodes to the TensorFlow graph.\u001B[39;00m\n\u001B[0;32m   4075\u001B[0m \u001B[38;5;66;03m# Add nodes to the TensorFlow graph.\u001B[39;00m\n\u001B[1;32m-> 4076\u001B[0m _, _, _op, _outputs \u001B[38;5;241m=\u001B[39m \u001B[43m_op_def_library\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply_op_helper\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   4077\u001B[0m \u001B[43m      \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mIdentity\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   4078\u001B[0m _result \u001B[38;5;241m=\u001B[39m _outputs[:]\n\u001B[0;32m   4079\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _execute\u001B[38;5;241m.\u001B[39mmust_record_gradient():\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\milestone2\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:797\u001B[0m, in \u001B[0;36m_apply_op_helper\u001B[1;34m(op_type_name, name, **keywords)\u001B[0m\n\u001B[0;32m    792\u001B[0m must_colocate_inputs \u001B[38;5;241m=\u001B[39m [val \u001B[38;5;28;01mfor\u001B[39;00m arg, val \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(op_def\u001B[38;5;241m.\u001B[39minput_arg, inputs)\n\u001B[0;32m    793\u001B[0m                         \u001B[38;5;28;01mif\u001B[39;00m arg\u001B[38;5;241m.\u001B[39mis_ref]\n\u001B[0;32m    794\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m _MaybeColocateWith(must_colocate_inputs):\n\u001B[0;32m    795\u001B[0m   \u001B[38;5;66;03m# Add Op to graph\u001B[39;00m\n\u001B[0;32m    796\u001B[0m   \u001B[38;5;66;03m# pylint: disable=protected-access\u001B[39;00m\n\u001B[1;32m--> 797\u001B[0m   op \u001B[38;5;241m=\u001B[39m \u001B[43mg\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_create_op_internal\u001B[49m\u001B[43m(\u001B[49m\u001B[43mop_type_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtypes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    798\u001B[0m \u001B[43m                             \u001B[49m\u001B[43mname\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mscope\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minput_types\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_types\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    799\u001B[0m \u001B[43m                             \u001B[49m\u001B[43mattrs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattr_protos\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mop_def\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mop_def\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    801\u001B[0m \u001B[38;5;66;03m# `outputs` is returned as a separate return value so that the output\u001B[39;00m\n\u001B[0;32m    802\u001B[0m \u001B[38;5;66;03m# tensors can the `op` per se can be decoupled so that the\u001B[39;00m\n\u001B[0;32m    803\u001B[0m \u001B[38;5;66;03m# `op_callbacks` can function properly. See framework/op_callbacks.py\u001B[39;00m\n\u001B[0;32m    804\u001B[0m \u001B[38;5;66;03m# for more details.\u001B[39;00m\n\u001B[0;32m    805\u001B[0m outputs \u001B[38;5;241m=\u001B[39m op\u001B[38;5;241m.\u001B[39moutputs\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\milestone2\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:694\u001B[0m, in \u001B[0;36mFuncGraph._create_op_internal\u001B[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001B[0m\n\u001B[0;32m    692\u001B[0m   inp \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcapture(inp)\n\u001B[0;32m    693\u001B[0m   captured_inputs\u001B[38;5;241m.\u001B[39mappend(inp)\n\u001B[1;32m--> 694\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mFuncGraph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_create_op_internal\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# pylint: disable=protected-access\u001B[39;49;00m\n\u001B[0;32m    695\u001B[0m \u001B[43m    \u001B[49m\u001B[43mop_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcaptured_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtypes\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minput_types\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattrs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mop_def\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    696\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcompute_device\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\milestone2\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:3754\u001B[0m, in \u001B[0;36mGraph._create_op_internal\u001B[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001B[0m\n\u001B[0;32m   3751\u001B[0m \u001B[38;5;66;03m# _create_op_helper mutates the new Operation. `_mutation_lock` ensures a\u001B[39;00m\n\u001B[0;32m   3752\u001B[0m \u001B[38;5;66;03m# Session.run call cannot occur between creating and mutating the op.\u001B[39;00m\n\u001B[0;32m   3753\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_mutation_lock():\n\u001B[1;32m-> 3754\u001B[0m   ret \u001B[38;5;241m=\u001B[39m \u001B[43mOperation\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   3755\u001B[0m \u001B[43m      \u001B[49m\u001B[43mnode_def\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3756\u001B[0m \u001B[43m      \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3757\u001B[0m \u001B[43m      \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3758\u001B[0m \u001B[43m      \u001B[49m\u001B[43moutput_types\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdtypes\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3759\u001B[0m \u001B[43m      \u001B[49m\u001B[43mcontrol_inputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcontrol_inputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3760\u001B[0m \u001B[43m      \u001B[49m\u001B[43minput_types\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_types\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3761\u001B[0m \u001B[43m      \u001B[49m\u001B[43moriginal_op\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_default_original_op\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3762\u001B[0m \u001B[43m      \u001B[49m\u001B[43mop_def\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mop_def\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3763\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_op_helper(ret, compute_device\u001B[38;5;241m=\u001B[39mcompute_device)\n\u001B[0;32m   3764\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m ret\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\milestone2\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:2129\u001B[0m, in \u001B[0;36mOperation.__init__\u001B[1;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001B[0m\n\u001B[0;32m   2127\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m op_def \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   2128\u001B[0m     op_def \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_graph\u001B[38;5;241m.\u001B[39m_get_op_def(node_def\u001B[38;5;241m.\u001B[39mop)\n\u001B[1;32m-> 2129\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_c_op \u001B[38;5;241m=\u001B[39m \u001B[43m_create_c_op\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnode_def\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2130\u001B[0m \u001B[43m                            \u001B[49m\u001B[43mcontrol_input_ops\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mop_def\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2131\u001B[0m   name \u001B[38;5;241m=\u001B[39m compat\u001B[38;5;241m.\u001B[39mas_str(node_def\u001B[38;5;241m.\u001B[39mname)\n\u001B[0;32m   2133\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_traceback \u001B[38;5;241m=\u001B[39m tf_stack\u001B[38;5;241m.\u001B[39mextract_stack_for_node(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_c_op)\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\milestone2\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    148\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    149\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 150\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    151\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    152\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\milestone2\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1960\u001B[0m, in \u001B[0;36m_create_c_op\u001B[1;34m(graph, node_def, inputs, control_inputs, op_def)\u001B[0m\n\u001B[0;32m   1956\u001B[0m   pywrap_tf_session\u001B[38;5;241m.\u001B[39mTF_SetAttrValueProto(op_desc, compat\u001B[38;5;241m.\u001B[39mas_str(name),\n\u001B[0;32m   1957\u001B[0m                                          serialized)\n\u001B[0;32m   1959\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1960\u001B[0m   c_op \u001B[38;5;241m=\u001B[39m \u001B[43mpywrap_tf_session\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTF_FinishOperation\u001B[49m\u001B[43m(\u001B[49m\u001B[43mop_desc\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1961\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m errors\u001B[38;5;241m.\u001B[39mInvalidArgumentError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m   1962\u001B[0m   \u001B[38;5;66;03m# Convert to ValueError for backwards compatibility.\u001B[39;00m\n\u001B[0;32m   1963\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(e\u001B[38;5;241m.\u001B[39mmessage)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "model1.compile(loss=\"mse\", optimizer=m1_optimizer[m1_hyper_parameters[\"optimizer\"]], metrics=[keras.metrics.RootMeanSquaredError()])\n",
    "model1.fit(X_train, y_train,\n",
    "                     validation_split=m1_hyper_parameters[\"validation_split\"],\n",
    "                     batch_size=m1_hyper_parameters[\"batch_size\"],\n",
    "                     epochs=m1_hyper_parameters[\"epochs\"],\n",
    "                     shuffle=True)\n",
    "yhat = model1.predict(X_test, verbose=0)\n",
    "yhat_inverse = scaler.inverse_transform(yhat)\n",
    "y_test_inverse = scaler.inverse_transform(y_test)\n",
    "results_df[\"experiment_1_prediction\"] = yhat_inverse\n",
    "mae, mse, rmse = evaluate_forecast(y_test_inverse, yhat_inverse)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model 2\n",
    "This model is made of\n",
    "* a simple or bidirectional LSTM layer\n",
    "* a Dense unit\n",
    "* a Dropout Unit"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "m2_hyper_parameters = {\n",
    "    \"random_seed\": RANDOM_SEED,\n",
    "    \"test_size\": test_size,\n",
    "    \"validation_split\": 0.1,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"batch_size\": 64,\n",
    "    \"epochs\": 200,\n",
    "    \"lstm_units\": 20,\n",
    "    \"lstm_activation\": \"sigmoid\",\n",
    "    \"dense_units\": 61,\n",
    "    \"dense_activation\": \"tanh\",\n",
    "    \"dropout\": 0.2,\n",
    "    \"output_activation\": \"linear\",\n",
    "    \"nb_features\": nb_features,\n",
    "    \"optimizer\": \"RMSprop\"\n",
    "}\n",
    "\n",
    "m2_optimizer = {\n",
    "    \"RMSprop\": RMSprop(learning_rate=m2_hyper_parameters[\"learning_rate\"]),\n",
    "    \"Adam\": Adam(learning_rate=m2_hyper_parameters[\"learning_rate\"]),\n",
    "    \"Adamax\": Adamax(learning_rate=m2_hyper_parameters[\"learning_rate\"]),\n",
    "    \"Adagrad\": Adagrad(learning_rate=m2_hyper_parameters[\"learning_rate\"])\n",
    "}\n",
    "\n",
    "model2 = Sequential()\n",
    "model2.add(LSTM(m2_hyper_parameters[\"lstm_units\"], activation=m2_hyper_parameters[\"lstm_activation\"], input_shape=(7, nb_features)))\n",
    "model2.add(Dense(m2_hyper_parameters[\"dense_units\"], activation=m2_hyper_parameters[\"dense_activation\"]))\n",
    "model2.add(Dropout(m2_hyper_parameters[\"dropout\"]))\n",
    "model2.add(Dense(1, activation=m2_hyper_parameters[\"output_activation\"]))\n",
    "model2.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model2.compile(loss=\"mse\", optimizer=m2_optimizer[m2_hyper_parameters[\"optimizer\"]], metrics=[keras.metrics.RootMeanSquaredError()])\n",
    "model2.fit(X_train, y_train,\n",
    "                     validation_split=m2_hyper_parameters[\"validation_split\"],\n",
    "                     batch_size=m2_hyper_parameters[\"batch_size\"],\n",
    "                     epochs=m2_hyper_parameters[\"epochs\"],\n",
    "                     shuffle=True)\n",
    "yhat = model2.predict(X_test, verbose=0)\n",
    "yhat_inverse = scaler.inverse_transform(yhat)\n",
    "y_test_inverse = scaler.inverse_transform(y_test)\n",
    "results_df[\"experiment_2_prediction\"] = yhat_inverse\n",
    "evaluate_forecast(y_test_inverse, yhat_inverse)\n",
    "mae, mse, rmse = evaluate_forecast(y_test_inverse, yhat_inverse)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model 3 : Encoder-Decoder\n",
    "This model is made of\n",
    "* a simple or bidirectional encoding LSTM layer\n",
    "* a simple or bidirectional decoding LSTM layer\n",
    "* a Dense unit\n",
    "* a Dropout Unit"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "m3_hyper_parameters = {\n",
    "    \"random_seed\": RANDOM_SEED,\n",
    "    \"test_size\": test_size,\n",
    "    \"validation_split\": 0.1,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"batch_size\": 64,\n",
    "    \"epochs\": 200,\n",
    "    \"lstm_units\": 200,\n",
    "    \"2nd_lstm_units\": 100,\n",
    "    \"lstm_activation\": \"sigmoid\",\n",
    "    \"dense_units\": 81,\n",
    "    \"dense_activation\": \"tanh\",\n",
    "    \"dropout\": 0.2,\n",
    "    \"output_activation\": \"linear\",\n",
    "    \"nb_features\": nb_features,\n",
    "    \"optimizer\": \"RMSprop\"\n",
    "}\n",
    "\n",
    "m3_optimizer = {\n",
    "    \"RMSprop\": RMSprop(learning_rate=m3_hyper_parameters[\"learning_rate\"]),\n",
    "    \"Adam\": Adam(learning_rate=m3_hyper_parameters[\"learning_rate\"]),\n",
    "    \"Adamax\": Adamax(learning_rate=m3_hyper_parameters[\"learning_rate\"]),\n",
    "    \"Adagrad\": Adagrad(learning_rate=m3_hyper_parameters[\"learning_rate\"])\n",
    "}\n",
    "\n",
    "model3 = Sequential()\n",
    "model3.add(LSTM(m3_hyper_parameters[\"lstm_units\"], activation=m3_hyper_parameters[\"lstm_activation\"], input_shape=(7, nb_features)))\n",
    "model3.add(RepeatVector(1))\n",
    "model3.add(LSTM(m3_hyper_parameters[\"2nd_lstm_units\"], activation=m3_hyper_parameters[\"lstm_activation\"], return_sequences=True))\n",
    "model3.add(TimeDistributed(Dense(m3_hyper_parameters[\"dense_units\"], activation=m3_hyper_parameters[\"dense_activation\"])))\n",
    "model3.add(Dropout(m3_hyper_parameters[\"dropout\"]))\n",
    "model3.add(Dense(1, activation=m3_hyper_parameters[\"output_activation\"]))\n",
    "model3.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model3.compile(loss=\"mse\", optimizer=m3_optimizer[m3_hyper_parameters[\"optimizer\"]], metrics=[keras.metrics.RootMeanSquaredError()])\n",
    "model3.fit(X_train, y_train_3d,\n",
    "                     validation_split=m3_hyper_parameters[\"validation_split\"],\n",
    "                     batch_size=m3_hyper_parameters[\"batch_size\"],\n",
    "                     epochs=m3_hyper_parameters[\"epochs\"],\n",
    "                     shuffle=False)\n",
    "yhat = model3.predict(X_test, verbose=0)\n",
    "yhat_inverse = scaler.inverse_transform(yhat.squeeze(2))\n",
    "y_test_inverse = scaler.inverse_transform(y_test)\n",
    "results_df[\"experiment_2_prediction\"] = yhat_inverse\n",
    "evaluate_forecast(y_test_inverse, yhat_inverse)\n",
    "mae, mse, rmse = evaluate_forecast(y_test_inverse, yhat_inverse)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "results_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}