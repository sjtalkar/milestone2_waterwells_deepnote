{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import neptune.new as neptune\n",
    "from neptune.new.integrations.tensorflow_keras import NeptuneCallback\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, TimeDistributed, Conv1D, MaxPooling1D, Flatten, Bidirectional, Input, Flatten, Activation, Reshape, RepeatVector, Concatenate\n",
    "from keras.optimizers import RMSprop, Adam, Adamax\n",
    "\n",
    "from lib.read_data import read_and_join_output_file\n",
    "from lib.create_pipeline import create_transformation_pipeline\n",
    "from lib.transform_impute import convert_back_df\n",
    "from lib.split_data import train_test_group_time_split"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tensorflow.config.list_physical_devices('GPU')))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "# During experiment we can try to use neptune.ai to log all the Tensorflow experiments results\n",
    "neptune_key = pickle.load(open(\"./neptune.pkl\", \"rb\"))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preparing the Dataset\n",
    "The train and test sets are split by Township-Ranges, i.e. some Township-Ranges data are either fully in the train or test set.\n",
    "The target value is the value of that variable for 2021\n",
    "Thus train/test sets are of shape (number of Township-Ranges, 7 years (2014-2020), the number of features).\n",
    "The input of 1 data point in the model is of shape (7x81\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "data": {
      "text/plain": "                     TOTALDRILLDEPTH_AVG  WELLYIELD_AVG  STATICWATERLEVEL_AVG  \\\nTOWNSHIP_RANGE YEAR                                                             \nT01N R03E      2014             0.097778       0.018246              0.037145   \n               2015             0.095238       0.021053              0.025042   \n               2016             0.114286       0.007916              0.022398   \n               2017             0.000000       0.013684              0.030885   \n               2018             0.083873       0.002474              0.034558   \n...                                  ...            ...                   ...   \nT32S R30E      2016             0.000000       0.000000              0.000000   \n               2017             0.000000       0.000000              0.000000   \n               2018             0.000000       0.000000              0.000000   \n               2019             0.000000       0.000000              0.000000   \n               2020             0.000000       0.000000              0.000000   \n\n                     TOPOFPERFORATEDINTERVAL_AVG  \\\nTOWNSHIP_RANGE YEAR                                \nT01N R03E      2014                     0.098039   \n               2015                     0.117647   \n               2016                     0.152614   \n               2017                     0.127451   \n               2018                     0.148257   \n...                                          ...   \nT32S R30E      2016                     0.000000   \n               2017                     0.000000   \n               2018                     0.000000   \n               2019                     0.000000   \n               2020                     0.000000   \n\n                     BOTTOMOFPERFORATEDINTERVAL_AVG  TOTALCOMPLETEDDEPTH_AVG  \\\nTOWNSHIP_RANGE YEAR                                                            \nT01N R03E      2014                        0.111111                 0.105856   \n               2015                        0.080460                 0.079848   \n               2016                        0.103768                 0.104880   \n               2017                        0.082375                 0.081749   \n               2018                        0.093934                 0.107605   \n...                                             ...                      ...   \nT32S R30E      2016                        0.000000                 0.000000   \n               2017                        0.000000                 0.000000   \n               2018                        0.000000                 0.000000   \n               2019                        0.000000                 0.000000   \n               2020                        0.000000                 0.000000   \n\n                     VEGETATION_BLUE_OAK-GRAY_PINE  \\\nTOWNSHIP_RANGE YEAR                                  \nT01N R03E      2014                       0.000037   \n               2015                       0.000037   \n               2016                       0.000037   \n               2017                       0.000037   \n               2018                       0.000037   \n...                                            ...   \nT32S R30E      2016                       0.033178   \n               2017                       0.033178   \n               2018                       0.033178   \n               2019                       0.033178   \n               2020                       0.033178   \n\n                     VEGETATION_CALIFORNIA_COAST_LIVE_OAK  \\\nTOWNSHIP_RANGE YEAR                                         \nT01N R03E      2014                              0.000137   \n               2015                              0.000137   \n               2016                              0.000137   \n               2017                              0.000137   \n               2018                              0.000137   \n...                                                   ...   \nT32S R30E      2016                              0.000000   \n               2017                              0.000000   \n               2018                              0.000000   \n               2019                              0.000000   \n               2020                              0.000000   \n\n                     VEGETATION_CANYON_LIVE_OAK  VEGETATION_HARD_CHAPARRAL  \\\nTOWNSHIP_RANGE YEAR                                                          \nT01N R03E      2014                    0.000000                   0.000386   \n               2015                    0.000000                   0.000386   \n               2016                    0.000000                   0.000386   \n               2017                    0.000000                   0.000386   \n               2018                    0.000000                   0.000386   \n...                                         ...                        ...   \nT32S R30E      2016                    0.002023                   0.003535   \n               2017                    0.002023                   0.003535   \n               2018                    0.002023                   0.003535   \n               2019                    0.002023                   0.003535   \n               2020                    0.002023                   0.003535   \n\n                     ...  POPULATION_DENSITY  PCT_OF_CAPACITY  \\\nTOWNSHIP_RANGE YEAR  ...                                        \nT01N R03E      2014  ...            0.252900         0.717075   \n               2015  ...            0.252799         0.717075   \n               2016  ...            0.250621         0.717075   \n               2017  ...            0.254669         0.717075   \n               2018  ...            0.256461         0.800728   \n...                  ...                 ...              ...   \nT32S R30E      2016  ...            0.004469         0.496289   \n               2017  ...            0.004457         0.496289   \n               2018  ...            0.004474         0.496289   \n               2019  ...            0.004491         0.580893   \n               2020  ...            0.004512         0.499980   \n\n                     GROUNDSURFACEELEVATION_AVG  AVERAGE_YEARLY_PRECIPITATION  \\\nTOWNSHIP_RANGE YEAR                                                             \nT01N R03E      2014                    0.023626                      0.163573   \n               2015                    0.018249                      0.217900   \n               2016                    0.024153                      0.209056   \n               2017                    0.023541                      0.213645   \n               2018                    0.020523                      0.181012   \n...                                         ...                           ...   \nT32S R30E      2016                    0.139007                      0.111564   \n               2017                    0.139007                      0.169284   \n               2018                    0.139007                      0.079747   \n               2019                    0.139007                      0.158678   \n               2020                    0.139007                      0.156231   \n\n                     SHORTAGE_COUNT   GSE_GWE  WELL_COUNT_AGRICULTURE  \\\nTOWNSHIP_RANGE YEAR                                                     \nT01N R03E      2014             0.0  0.043005                0.029412   \n               2015             0.0  0.050637                0.000000   \n               2016             0.0  0.035780                0.029412   \n               2017             0.0  0.033202                0.000000   \n               2018             0.0  0.030798                0.000000   \n...                             ...       ...                     ...   \nT32S R30E      2016             0.0  0.621728                0.000000   \n               2017             0.0  0.527907                0.000000   \n               2018             0.0  0.556283                0.000000   \n               2019             0.0  0.566892                0.000000   \n               2020             0.0  0.555112                0.000000   \n\n                     WELL_COUNT_DOMESTIC  WELL_COUNT_INDUSTRIAL  \\\nTOWNSHIP_RANGE YEAR                                               \nT01N R03E      2014             0.041667                    0.0   \n               2015             0.027778                    0.0   \n               2016             0.055556                    0.0   \n               2017             0.027778                    0.0   \n               2018             0.097222                    0.0   \n...                                  ...                    ...   \nT32S R30E      2016             0.000000                    0.0   \n               2017             0.000000                    0.0   \n               2018             0.000000                    0.0   \n               2019             0.000000                    0.0   \n               2020             0.000000                    0.0   \n\n                     WELL_COUNT_PUBLIC  \nTOWNSHIP_RANGE YEAR                     \nT01N R03E      2014                0.0  \n               2015                0.0  \n               2016                0.0  \n               2017                0.0  \n               2018                0.0  \n...                                ...  \nT32S R30E      2016                0.0  \n               2017                0.0  \n               2018                0.0  \n               2019                0.0  \n               2020                0.0  \n\n[2674 rows x 81 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th>TOTALDRILLDEPTH_AVG</th>\n      <th>WELLYIELD_AVG</th>\n      <th>STATICWATERLEVEL_AVG</th>\n      <th>TOPOFPERFORATEDINTERVAL_AVG</th>\n      <th>BOTTOMOFPERFORATEDINTERVAL_AVG</th>\n      <th>TOTALCOMPLETEDDEPTH_AVG</th>\n      <th>VEGETATION_BLUE_OAK-GRAY_PINE</th>\n      <th>VEGETATION_CALIFORNIA_COAST_LIVE_OAK</th>\n      <th>VEGETATION_CANYON_LIVE_OAK</th>\n      <th>VEGETATION_HARD_CHAPARRAL</th>\n      <th>...</th>\n      <th>POPULATION_DENSITY</th>\n      <th>PCT_OF_CAPACITY</th>\n      <th>GROUNDSURFACEELEVATION_AVG</th>\n      <th>AVERAGE_YEARLY_PRECIPITATION</th>\n      <th>SHORTAGE_COUNT</th>\n      <th>GSE_GWE</th>\n      <th>WELL_COUNT_AGRICULTURE</th>\n      <th>WELL_COUNT_DOMESTIC</th>\n      <th>WELL_COUNT_INDUSTRIAL</th>\n      <th>WELL_COUNT_PUBLIC</th>\n    </tr>\n    <tr>\n      <th>TOWNSHIP_RANGE</th>\n      <th>YEAR</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">T01N R03E</th>\n      <th>2014</th>\n      <td>0.097778</td>\n      <td>0.018246</td>\n      <td>0.037145</td>\n      <td>0.098039</td>\n      <td>0.111111</td>\n      <td>0.105856</td>\n      <td>0.000037</td>\n      <td>0.000137</td>\n      <td>0.000000</td>\n      <td>0.000386</td>\n      <td>...</td>\n      <td>0.252900</td>\n      <td>0.717075</td>\n      <td>0.023626</td>\n      <td>0.163573</td>\n      <td>0.0</td>\n      <td>0.043005</td>\n      <td>0.029412</td>\n      <td>0.041667</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2015</th>\n      <td>0.095238</td>\n      <td>0.021053</td>\n      <td>0.025042</td>\n      <td>0.117647</td>\n      <td>0.080460</td>\n      <td>0.079848</td>\n      <td>0.000037</td>\n      <td>0.000137</td>\n      <td>0.000000</td>\n      <td>0.000386</td>\n      <td>...</td>\n      <td>0.252799</td>\n      <td>0.717075</td>\n      <td>0.018249</td>\n      <td>0.217900</td>\n      <td>0.0</td>\n      <td>0.050637</td>\n      <td>0.000000</td>\n      <td>0.027778</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2016</th>\n      <td>0.114286</td>\n      <td>0.007916</td>\n      <td>0.022398</td>\n      <td>0.152614</td>\n      <td>0.103768</td>\n      <td>0.104880</td>\n      <td>0.000037</td>\n      <td>0.000137</td>\n      <td>0.000000</td>\n      <td>0.000386</td>\n      <td>...</td>\n      <td>0.250621</td>\n      <td>0.717075</td>\n      <td>0.024153</td>\n      <td>0.209056</td>\n      <td>0.0</td>\n      <td>0.035780</td>\n      <td>0.029412</td>\n      <td>0.055556</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2017</th>\n      <td>0.000000</td>\n      <td>0.013684</td>\n      <td>0.030885</td>\n      <td>0.127451</td>\n      <td>0.082375</td>\n      <td>0.081749</td>\n      <td>0.000037</td>\n      <td>0.000137</td>\n      <td>0.000000</td>\n      <td>0.000386</td>\n      <td>...</td>\n      <td>0.254669</td>\n      <td>0.717075</td>\n      <td>0.023541</td>\n      <td>0.213645</td>\n      <td>0.0</td>\n      <td>0.033202</td>\n      <td>0.000000</td>\n      <td>0.027778</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2018</th>\n      <td>0.083873</td>\n      <td>0.002474</td>\n      <td>0.034558</td>\n      <td>0.148257</td>\n      <td>0.093934</td>\n      <td>0.107605</td>\n      <td>0.000037</td>\n      <td>0.000137</td>\n      <td>0.000000</td>\n      <td>0.000386</td>\n      <td>...</td>\n      <td>0.256461</td>\n      <td>0.800728</td>\n      <td>0.020523</td>\n      <td>0.181012</td>\n      <td>0.0</td>\n      <td>0.030798</td>\n      <td>0.000000</td>\n      <td>0.097222</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">T32S R30E</th>\n      <th>2016</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.033178</td>\n      <td>0.000000</td>\n      <td>0.002023</td>\n      <td>0.003535</td>\n      <td>...</td>\n      <td>0.004469</td>\n      <td>0.496289</td>\n      <td>0.139007</td>\n      <td>0.111564</td>\n      <td>0.0</td>\n      <td>0.621728</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2017</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.033178</td>\n      <td>0.000000</td>\n      <td>0.002023</td>\n      <td>0.003535</td>\n      <td>...</td>\n      <td>0.004457</td>\n      <td>0.496289</td>\n      <td>0.139007</td>\n      <td>0.169284</td>\n      <td>0.0</td>\n      <td>0.527907</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2018</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.033178</td>\n      <td>0.000000</td>\n      <td>0.002023</td>\n      <td>0.003535</td>\n      <td>...</td>\n      <td>0.004474</td>\n      <td>0.496289</td>\n      <td>0.139007</td>\n      <td>0.079747</td>\n      <td>0.0</td>\n      <td>0.556283</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2019</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.033178</td>\n      <td>0.000000</td>\n      <td>0.002023</td>\n      <td>0.003535</td>\n      <td>...</td>\n      <td>0.004491</td>\n      <td>0.580893</td>\n      <td>0.139007</td>\n      <td>0.158678</td>\n      <td>0.0</td>\n      <td>0.566892</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2020</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.033178</td>\n      <td>0.000000</td>\n      <td>0.002023</td>\n      <td>0.003535</td>\n      <td>...</td>\n      <td>0.004512</td>\n      <td>0.499980</td>\n      <td>0.139007</td>\n      <td>0.156231</td>\n      <td>0.0</td>\n      <td>0.555112</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>2674 rows × 81 columns</p>\n</div>"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RANDOM_SEED = 42\n",
    "# Load the data from the ETL output files\n",
    "X = read_and_join_output_file()\n",
    "#X[\"WELL_COUNT\"] = X[\"WELL_COUNT_PUBLIC\"] + X[\"WELL_COUNT_AGRICULTURE\"] + X[\"WELL_COUNT_DOMESTIC\"] + X[\"WELL_COUNT_INDUSTRIAL\"]\n",
    "#X.drop(columns=[\"WELL_COUNT_PUBLIC\", \"WELL_COUNT_AGRICULTURE\", \"WELL_COUNT_DOMESTIC\", \"WELL_COUNT_INDUSTRIAL\"], inplace=True)\n",
    "# Split the data into a training and a test set\n",
    "X_train_df, X_test_df, y_train_df, y_test_df = train_test_group_time_split(X, index=[\"TOWNSHIP_RANGE\", \"YEAR\"], group=\"TOWNSHIP_RANGE\", random_seed=RANDOM_SEED)\n",
    "# Create, fit and apply the data imputation pipeline to the training and test sets\n",
    "impute_pipeline = create_transformation_pipeline(X_train_df, scaler = MinMaxScaler())\n",
    "X_train_impute = impute_pipeline.fit_transform(X_train_df)\n",
    "X_test_impute = impute_pipeline.fit_transform(X_test_df)\n",
    "# Convert the X_train and X_test back to dataframes\n",
    "X_train_impute_df = convert_back_df(X_train_impute, impute_pipeline, X_train_df)\n",
    "X_test_impute_df = convert_back_df(X_test_impute, impute_pipeline, X_test_df)\n",
    "# Keep only the GSE_GWE variable as the outcome variable\n",
    "scaler = MinMaxScaler()\n",
    "y_train = scaler.fit_transform(y_train_df[[\"GSE_GWE\"]])\n",
    "y_test = scaler.transform(y_test_df[[\"GSE_GWE\"]])\n",
    "X_train_impute_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "# Change the shape of the input array to (number of Township-Ranges, 7 years (2014-2020), the number of features)\n",
    "X_train = X_train_impute_df.values.reshape(len(X_train_impute_df.index.get_level_values(0).unique()), len(X_train_impute_df.index.get_level_values(1).unique()), X_train_impute_df.shape[1])\n",
    "X_test = X_test_impute_df.values.reshape(len(X_test_impute_df.index.get_level_values(0).unique()), len(X_test_impute_df.index.get_level_values(1).unique()), X_test_impute_df.shape[1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Checking the train, validation and test input (X) datasets sizes:\n",
      "Size of the X_train dataset: (382, 7, 81)\n",
      "Size of the X_test dataset: (96, 7, 81)\n",
      "====================================================================================================\n",
      "Checking the train, validation and test output (y) datasets sizes:\n",
      "Size of the y_train dataset: (382, 1)\n",
      "Size of the y_test dataset: (96, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*100)\n",
    "print(\"Checking the train, validation and test input (X) datasets sizes:\")\n",
    "print(f\"Size of the X_train dataset: {X_train.shape}\")\n",
    "#print(f\"Size of the X_val dataset: {X_val.shape}\")\n",
    "print(f\"Size of the X_test dataset: {X_test.shape}\")\n",
    "print(\"=\"*100)\n",
    "print(\"Checking the train, validation and test output (y) datasets sizes:\")\n",
    "print(f\"Size of the y_train dataset: {y_train.shape}\")\n",
    "#print(f\"Size of the y_val dataset: {y_val_df.shape}\")\n",
    "print(f\"Size of the y_test dataset: {y_test.shape}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "results_df = y_test_df[[\"GSE_GWE\"]].copy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "nb_features = len(X_train_impute_df.columns)\n",
    "\n",
    "adam_optimizer = Adam(learning_rate=hyper_parameters[\"learning_rate\"])\n",
    "rms_optimizer = RMSprop(learning_rate=hyper_parameters[\"learning_rate\"])\n",
    "adamax_optimizer = Adamax(learning_rate=hyper_parameters[\"learning_rate\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "def evaluate_forecast(y_test_inverse, yhat_inverse):\n",
    "    mse_ = keras.metrics.MeanSquaredError()\n",
    "    mae_ = keras.metrics.MeanAbsoluteError()\n",
    "    rmse_ = keras.metrics.RootMeanSquaredError()\n",
    "    mae = mae_(y_test_inverse,yhat_inverse)\n",
    "    print('mae:', mae)\n",
    "    mse = mse_(y_test_inverse,yhat_inverse)\n",
    "    print('mse:', mse)\n",
    "    rmse = rmse_(y_test_inverse,yhat_inverse)\n",
    "    print('rmse:', rmse)\n",
    "    return mae, mse, rmse"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_4 (LSTM)               (None, 200)               225600    \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 1)                 201       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 225,801\n",
      "Trainable params: 225,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "hyper_parameters = {\n",
    "    \"validation_split\": 0.05,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"batch_size\": 32,\n",
    "    \"epochs\": 100,\n",
    "    \"lstm_units\": 200,\n",
    "    \"output_activation\": \"linear\",\n",
    "}\n",
    "\n",
    "model1 = Sequential()\n",
    "model1.add(LSTM(hyper_parameters[\"lstm_units\"], input_shape=(7, nb_features)))\n",
    "model1.add(Dense(1, activation=hyper_parameters[\"output_activation\"]))\n",
    "model1.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/mlienart/milestone2/e/MIL06KDIEQ-10\n",
      "Remember to stop your run once you’ve finished logging your metadata (https://docs.neptune.ai/api-reference/run#.stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.\n",
      "Epoch 1/100\n",
      "12/12 [==============================] - 2s 47ms/step - loss: 0.0220 - val_loss: 0.0054\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.0086 - val_loss: 0.0073\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.0059 - val_loss: 0.0157\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.0042 - val_loss: 0.0081\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.0061 - val_loss: 0.0068\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.0049 - val_loss: 0.0345\n",
      "Epoch 7/100\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.0045 - val_loss: 0.0056\n",
      "Epoch 8/100\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.0047 - val_loss: 0.0045\n",
      "Epoch 9/100\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.0040 - val_loss: 0.0109\n",
      "Epoch 10/100\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.0042 - val_loss: 0.0052\n",
      "Epoch 11/100\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.0026 - val_loss: 0.0093\n",
      "Epoch 12/100\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.0041 - val_loss: 0.0115\n",
      "Epoch 13/100\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.0035 - val_loss: 0.0052\n",
      "Epoch 14/100\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.0035 - val_loss: 0.0036\n",
      "Epoch 15/100\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.0035 - val_loss: 0.0037\n",
      "Epoch 16/100\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.0025 - val_loss: 0.0288\n",
      "Epoch 17/100\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.0034 - val_loss: 0.0049\n",
      "Epoch 18/100\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0030 - val_loss: 0.0109\n",
      "Epoch 19/100\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.0022 - val_loss: 0.0041\n",
      "Epoch 20/100\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.0030 - val_loss: 0.0106\n",
      "Epoch 21/100\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.0023 - val_loss: 0.0038\n",
      "Epoch 22/100\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.0029 - val_loss: 0.0065\n",
      "Epoch 23/100\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.0023 - val_loss: 0.0057\n",
      "Epoch 24/100\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.0018 - val_loss: 0.0048\n",
      "Epoch 25/100\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.0024 - val_loss: 0.0032\n",
      "Epoch 26/100\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.0024 - val_loss: 0.0055\n",
      "Epoch 27/100\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.0015 - val_loss: 0.0032\n",
      "Epoch 28/100\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.0023 - val_loss: 0.0035\n",
      "Epoch 29/100\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.0020 - val_loss: 0.0032\n",
      "Epoch 30/100\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.0017 - val_loss: 0.0032\n",
      "Epoch 31/100\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.0019 - val_loss: 0.0031\n",
      "Epoch 32/100\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.0020 - val_loss: 0.0055\n",
      "Epoch 33/100\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.0017 - val_loss: 0.0105\n",
      "Epoch 34/100\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.0019 - val_loss: 0.0037\n",
      "Epoch 35/100\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.0016 - val_loss: 0.0043\n",
      "Epoch 36/100\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.0017 - val_loss: 0.0063\n",
      "Epoch 37/100\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.0017 - val_loss: 0.0057\n",
      "Epoch 38/100\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.0017 - val_loss: 0.0033\n",
      "Epoch 39/100\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.0019 - val_loss: 0.0028\n",
      "Epoch 40/100\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.0012 - val_loss: 0.0033\n",
      "Epoch 41/100\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.0019 - val_loss: 0.0042\n",
      "Epoch 42/100\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.0016 - val_loss: 0.0051\n",
      "Epoch 43/100\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0018 - val_loss: 0.0043\n",
      "Epoch 44/100\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0014 - val_loss: 0.0031\n",
      "Epoch 45/100\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0016 - val_loss: 0.0061\n",
      "Epoch 46/100\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.0015 - val_loss: 0.0060\n",
      "Epoch 47/100\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.0014 - val_loss: 0.0037\n",
      "Epoch 48/100\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.0013 - val_loss: 0.0061\n",
      "Epoch 49/100\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.0015 - val_loss: 0.0033\n",
      "Epoch 50/100\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.0015 - val_loss: 0.0034\n",
      "Epoch 51/100\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.0012 - val_loss: 0.0034\n",
      "Epoch 52/100\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.0011 - val_loss: 0.0031\n",
      "Epoch 53/100\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.0013 - val_loss: 0.0040\n",
      "Epoch 54/100\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.0014 - val_loss: 0.0038\n",
      "Epoch 55/100\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.0012 - val_loss: 0.0036\n",
      "Epoch 56/100\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.0014 - val_loss: 0.0065\n",
      "Epoch 57/100\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.0013 - val_loss: 0.0030\n",
      "Epoch 58/100\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.0013 - val_loss: 0.0034\n",
      "Epoch 59/100\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.0012 - val_loss: 0.0074\n",
      "Epoch 60/100\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.0011 - val_loss: 0.0041\n",
      "Epoch 61/100\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.0017 - val_loss: 0.0054\n",
      "Epoch 62/100\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.0012 - val_loss: 0.0042\n",
      "Epoch 63/100\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 7.7321e-04 - val_loss: 0.0030\n",
      "Epoch 64/100\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 8.6064e-04 - val_loss: 0.0031\n",
      "Epoch 65/100\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.0012 - val_loss: 0.0031\n",
      "Epoch 66/100\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.0014 - val_loss: 0.0048\n",
      "Epoch 67/100\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.0010 - val_loss: 0.0042\n",
      "Epoch 68/100\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 9.6580e-04 - val_loss: 0.0044\n",
      "Epoch 69/100\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.0011 - val_loss: 0.0044\n",
      "Epoch 70/100\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.0010 - val_loss: 0.0038\n",
      "Epoch 71/100\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 8.7735e-04 - val_loss: 0.0030\n",
      "Epoch 72/100\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.0012 - val_loss: 0.0040\n",
      "Epoch 73/100\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 8.6804e-04 - val_loss: 0.0029\n",
      "Epoch 74/100\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 9.6636e-04 - val_loss: 0.0033\n",
      "Epoch 75/100\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 8.8168e-04 - val_loss: 0.0038\n",
      "Epoch 76/100\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.0012 - val_loss: 0.0031\n",
      "Epoch 77/100\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 8.7679e-04 - val_loss: 0.0035\n",
      "Epoch 78/100\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.0011 - val_loss: 0.0032\n",
      "Epoch 79/100\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 6.7714e-04 - val_loss: 0.0055\n",
      "Epoch 80/100\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.0013 - val_loss: 0.0030\n",
      "Epoch 81/100\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 6.9306e-04 - val_loss: 0.0042\n",
      "Epoch 82/100\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.0011 - val_loss: 0.0029\n",
      "Epoch 83/100\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 9.7659e-04 - val_loss: 0.0033\n",
      "Epoch 84/100\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 9.6551e-04 - val_loss: 0.0041\n",
      "Epoch 85/100\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 7.3225e-04 - val_loss: 0.0035\n",
      "Epoch 86/100\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.0011 - val_loss: 0.0033\n",
      "Epoch 87/100\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 7.4690e-04 - val_loss: 0.0035\n",
      "Epoch 88/100\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 6.9275e-04 - val_loss: 0.0039\n",
      "Epoch 89/100\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.0010 - val_loss: 0.0032\n",
      "Epoch 90/100\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 9.2620e-04 - val_loss: 0.0039\n",
      "Epoch 91/100\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 8.7362e-04 - val_loss: 0.0032\n",
      "Epoch 92/100\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 7.1330e-04 - val_loss: 0.0046\n",
      "Epoch 93/100\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 9.0407e-04 - val_loss: 0.0028\n",
      "Epoch 94/100\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 8.1463e-04 - val_loss: 0.0031\n",
      "Epoch 95/100\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 7.6238e-04 - val_loss: 0.0025\n",
      "Epoch 96/100\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 7.1881e-04 - val_loss: 0.0038\n",
      "Epoch 97/100\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 8.1510e-04 - val_loss: 0.0032\n",
      "Epoch 98/100\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 6.4638e-04 - val_loss: 0.0024\n",
      "Epoch 99/100\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 6.6803e-04 - val_loss: 0.0036\n",
      "Epoch 100/100\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 7.0790e-04 - val_loss: 0.0034\n",
      "mae: tf.Tensor(32.96155, shape=(), dtype=float32)\n",
      "mse: tf.Tensor(3784.868, shape=(), dtype=float32)\n",
      "rmse: tf.Tensor(61.52128, shape=(), dtype=float32)\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 504 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 504 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://app.neptune.ai/mlienart/milestone2/e/MIL06KDIEQ-10\n"
     ]
    }
   ],
   "source": [
    "# Start experiment\n",
    "run = neptune.init(\n",
    "    project=\"mlienart/milestone2\",\n",
    "    api_token=neptune_key,\n",
    "    name=\"Basic Model\"\n",
    ")\n",
    "neptune_cbk = NeptuneCallback(run=run, base_namespace='metrics')\n",
    "hyper_parameters[\"optimizer\"] = \"RMSprop\"\n",
    "run['hyper-parameters'] = hyper_parameters\n",
    "\n",
    "model1.compile(loss=\"mse\", optimizer=rms_optimizer)\n",
    "history = model1.fit(X_train, y_train,\n",
    "                     validation_split=hyper_parameters[\"validation_split\"],\n",
    "                     batch_size=hyper_parameters[\"batch_size\"],\n",
    "                     epochs=hyper_parameters[\"epochs\"],\n",
    "                     shuffle=True,\n",
    "                     callbacks=[neptune_cbk])\n",
    "yhat = model1.predict(X_test, verbose=0)\n",
    "yhat_inverse = scaler.inverse_transform(yhat)\n",
    "y_test_inverse = scaler.inverse_transform(y_test)\n",
    "results_df[\"experiment_1_prediction\"] = yhat_inverse\n",
    "mae, mse, rmse = evaluate_forecast(y_test_inverse, yhat_inverse)\n",
    "run[\"eval/mae\"] = mae\n",
    "run[\"eval/mse\"] = mse\n",
    "run[\"eval/rmse\"] = rmse\n",
    "run.stop()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional_2 (Bidirectio  (None, 400)              451200    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 20)                8020      \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 20)                0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 1)                 21        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 459,241\n",
      "Trainable params: 459,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "hyper_parameters = {\n",
    "    \"validation_split\": 0.05,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"batch_size\": 32,\n",
    "    \"epochs\": 100,\n",
    "    \"lstm_units\": 200,\n",
    "    \"dense_units\": 20,\n",
    "    \"dropout\": 0.2,\n",
    "    \"output_activation\": \"linear\",\n",
    "}\n",
    "\n",
    "model2 = Sequential()\n",
    "model2.add(Bidirectional(LSTM(hyper_parameters[\"lstm_units\"]), input_shape=(7, nb_features)))\n",
    "model2.add(Dense(hyper_parameters[\"dense_units\"], activation=\"tanh\"))\n",
    "model2.add(Dropout(hyper_parameters[\"dropout\"]))\n",
    "model2.add(Dense(1, activation=hyper_parameters[\"output_activation\"]))\n",
    "model2.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/mlienart/milestone2/e/MIL06KDIEQ-11\n",
      "Remember to stop your run once you’ve finished logging your metadata (https://docs.neptune.ai/api-reference/run#.stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.\n",
      "Epoch 1/100\n",
      "12/12 [==============================] - 4s 132ms/step - loss: 0.0266 - val_loss: 0.0173\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 1s 85ms/step - loss: 0.0108 - val_loss: 0.0054\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 1s 83ms/step - loss: 0.0071 - val_loss: 0.0053\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - 1s 88ms/step - loss: 0.0055 - val_loss: 0.0099\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - 1s 87ms/step - loss: 0.0044 - val_loss: 0.0073\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - 1s 87ms/step - loss: 0.0055 - val_loss: 0.0055\n",
      "Epoch 7/100\n",
      "12/12 [==============================] - 1s 87ms/step - loss: 0.0055 - val_loss: 0.0040\n",
      "Epoch 8/100\n",
      "12/12 [==============================] - 1s 87ms/step - loss: 0.0048 - val_loss: 0.0071\n",
      "Epoch 9/100\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 0.0056 - val_loss: 0.0050\n",
      "Epoch 10/100\n",
      "12/12 [==============================] - 1s 83ms/step - loss: 0.0052 - val_loss: 0.0054\n",
      "Epoch 11/100\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 0.0046 - val_loss: 0.0079\n",
      "Epoch 12/100\n",
      "12/12 [==============================] - 1s 85ms/step - loss: 0.0037 - val_loss: 0.0033\n",
      "Epoch 13/100\n",
      "12/12 [==============================] - 1s 85ms/step - loss: 0.0043 - val_loss: 0.0087\n",
      "Epoch 14/100\n",
      "12/12 [==============================] - 1s 84ms/step - loss: 0.0044 - val_loss: 0.0074\n",
      "Epoch 15/100\n",
      "12/12 [==============================] - 1s 87ms/step - loss: 0.0042 - val_loss: 0.0077\n",
      "Epoch 16/100\n",
      "12/12 [==============================] - 1s 88ms/step - loss: 0.0039 - val_loss: 0.0116\n",
      "Epoch 17/100\n",
      "12/12 [==============================] - 1s 87ms/step - loss: 0.0043 - val_loss: 0.0032\n",
      "Epoch 18/100\n",
      "12/12 [==============================] - 1s 87ms/step - loss: 0.0039 - val_loss: 0.0032\n",
      "Epoch 19/100\n",
      "12/12 [==============================] - 1s 91ms/step - loss: 0.0049 - val_loss: 0.0077\n",
      "Epoch 20/100\n",
      "12/12 [==============================] - 1s 84ms/step - loss: 0.0044 - val_loss: 0.0091\n",
      "Epoch 21/100\n",
      "12/12 [==============================] - 1s 83ms/step - loss: 0.0034 - val_loss: 0.0091\n",
      "Epoch 22/100\n",
      "12/12 [==============================] - 1s 83ms/step - loss: 0.0037 - val_loss: 0.0073\n",
      "Epoch 23/100\n",
      "12/12 [==============================] - 1s 83ms/step - loss: 0.0036 - val_loss: 0.0067\n",
      "Epoch 24/100\n",
      "12/12 [==============================] - 1s 83ms/step - loss: 0.0031 - val_loss: 0.0053\n",
      "Epoch 25/100\n",
      "12/12 [==============================] - 1s 85ms/step - loss: 0.0029 - val_loss: 0.0033\n",
      "Epoch 26/100\n",
      "12/12 [==============================] - 1s 93ms/step - loss: 0.0036 - val_loss: 0.0033\n",
      "Epoch 27/100\n",
      "12/12 [==============================] - 1s 87ms/step - loss: 0.0032 - val_loss: 0.0036\n",
      "Epoch 28/100\n",
      "12/12 [==============================] - 1s 87ms/step - loss: 0.0044 - val_loss: 0.0025\n",
      "Epoch 29/100\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 0.0040 - val_loss: 0.0034\n",
      "Epoch 30/100\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 0.0035 - val_loss: 0.0051\n",
      "Epoch 31/100\n",
      "12/12 [==============================] - 1s 83ms/step - loss: 0.0029 - val_loss: 0.0051\n",
      "Epoch 32/100\n",
      "12/12 [==============================] - 1s 82ms/step - loss: 0.0030 - val_loss: 0.0046\n",
      "Epoch 33/100\n",
      "12/12 [==============================] - 1s 83ms/step - loss: 0.0034 - val_loss: 0.0095\n",
      "Epoch 34/100\n",
      "12/12 [==============================] - 1s 83ms/step - loss: 0.0035 - val_loss: 0.0084\n",
      "Epoch 35/100\n",
      "12/12 [==============================] - 1s 85ms/step - loss: 0.0030 - val_loss: 0.0034\n",
      "Epoch 36/100\n",
      "12/12 [==============================] - 1s 88ms/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 37/100\n",
      "12/12 [==============================] - 1s 90ms/step - loss: 0.0024 - val_loss: 0.0037\n",
      "Epoch 38/100\n",
      "12/12 [==============================] - 1s 88ms/step - loss: 0.0030 - val_loss: 0.0051\n",
      "Epoch 39/100\n",
      "12/12 [==============================] - 1s 89ms/step - loss: 0.0026 - val_loss: 0.0034\n",
      "Epoch 40/100\n",
      "12/12 [==============================] - 1s 87ms/step - loss: 0.0025 - val_loss: 0.0026\n",
      "Epoch 41/100\n",
      "12/12 [==============================] - 1s 85ms/step - loss: 0.0028 - val_loss: 0.0026\n",
      "Epoch 42/100\n",
      "12/12 [==============================] - 1s 84ms/step - loss: 0.0029 - val_loss: 0.0033\n",
      "Epoch 43/100\n",
      "12/12 [==============================] - 1s 83ms/step - loss: 0.0026 - val_loss: 0.0070\n",
      "Epoch 44/100\n",
      "12/12 [==============================] - 1s 83ms/step - loss: 0.0026 - val_loss: 0.0036\n",
      "Epoch 45/100\n",
      "12/12 [==============================] - 1s 87ms/step - loss: 0.0023 - val_loss: 0.0031\n",
      "Epoch 46/100\n",
      "12/12 [==============================] - 1s 87ms/step - loss: 0.0023 - val_loss: 0.0039\n",
      "Epoch 47/100\n",
      "12/12 [==============================] - 1s 89ms/step - loss: 0.0026 - val_loss: 0.0028\n",
      "Epoch 48/100\n",
      "12/12 [==============================] - 1s 87ms/step - loss: 0.0025 - val_loss: 0.0027\n",
      "Epoch 49/100\n",
      "12/12 [==============================] - 1s 85ms/step - loss: 0.0023 - val_loss: 0.0041\n",
      "Epoch 50/100\n",
      "12/12 [==============================] - 1s 87ms/step - loss: 0.0028 - val_loss: 0.0068\n",
      "Epoch 51/100\n",
      "12/12 [==============================] - 1s 87ms/step - loss: 0.0025 - val_loss: 0.0047\n",
      "Epoch 52/100\n",
      "12/12 [==============================] - 1s 84ms/step - loss: 0.0024 - val_loss: 0.0044\n",
      "Epoch 53/100\n",
      "12/12 [==============================] - 1s 83ms/step - loss: 0.0030 - val_loss: 0.0045\n",
      "Epoch 54/100\n",
      "12/12 [==============================] - 1s 85ms/step - loss: 0.0022 - val_loss: 0.0049\n",
      "Epoch 55/100\n",
      "12/12 [==============================] - 1s 88ms/step - loss: 0.0025 - val_loss: 0.0038\n",
      "Epoch 56/100\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 0.0023 - val_loss: 0.0030\n",
      "Epoch 57/100\n",
      "12/12 [==============================] - 1s 87ms/step - loss: 0.0022 - val_loss: 0.0040\n",
      "Epoch 58/100\n",
      "12/12 [==============================] - 1s 88ms/step - loss: 0.0018 - val_loss: 0.0045\n",
      "Epoch 59/100\n",
      "12/12 [==============================] - 1s 87ms/step - loss: 0.0015 - val_loss: 0.0048\n",
      "Epoch 60/100\n",
      "12/12 [==============================] - 1s 87ms/step - loss: 0.0022 - val_loss: 0.0053\n",
      "Epoch 61/100\n",
      "12/12 [==============================] - 1s 95ms/step - loss: 0.0022 - val_loss: 0.0054\n",
      "Epoch 62/100\n",
      "12/12 [==============================] - 1s 84ms/step - loss: 0.0023 - val_loss: 0.0044\n",
      "Epoch 63/100\n",
      "12/12 [==============================] - 1s 84ms/step - loss: 0.0019 - val_loss: 0.0064\n",
      "Epoch 64/100\n",
      "12/12 [==============================] - 1s 89ms/step - loss: 0.0022 - val_loss: 0.0026\n",
      "Epoch 65/100\n",
      "12/12 [==============================] - 1s 85ms/step - loss: 0.0024 - val_loss: 0.0062\n",
      "Epoch 66/100\n",
      "12/12 [==============================] - 1s 83ms/step - loss: 0.0022 - val_loss: 0.0033\n",
      "Epoch 67/100\n",
      "12/12 [==============================] - 1s 89ms/step - loss: 0.0020 - val_loss: 0.0046\n",
      "Epoch 68/100\n",
      "12/12 [==============================] - 1s 89ms/step - loss: 0.0025 - val_loss: 0.0085\n",
      "Epoch 69/100\n",
      "12/12 [==============================] - 1s 88ms/step - loss: 0.0023 - val_loss: 0.0045\n",
      "Epoch 70/100\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 0.0022 - val_loss: 0.0030\n",
      "Epoch 71/100\n",
      "12/12 [==============================] - 1s 87ms/step - loss: 0.0021 - val_loss: 0.0039\n",
      "Epoch 72/100\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 0.0018 - val_loss: 0.0042\n",
      "Epoch 73/100\n",
      "12/12 [==============================] - 1s 84ms/step - loss: 0.0016 - val_loss: 0.0049\n",
      "Epoch 74/100\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 0.0018 - val_loss: 0.0060\n",
      "Epoch 75/100\n",
      "12/12 [==============================] - 1s 83ms/step - loss: 0.0019 - val_loss: 0.0032\n",
      "Epoch 76/100\n",
      "12/12 [==============================] - 1s 82ms/step - loss: 0.0017 - val_loss: 0.0040\n",
      "Epoch 77/100\n",
      "12/12 [==============================] - 1s 84ms/step - loss: 0.0019 - val_loss: 0.0029\n",
      "Epoch 78/100\n",
      "12/12 [==============================] - 1s 87ms/step - loss: 0.0016 - val_loss: 0.0052\n",
      "Epoch 79/100\n",
      "12/12 [==============================] - 1s 87ms/step - loss: 0.0018 - val_loss: 0.0032\n",
      "Epoch 80/100\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 0.0018 - val_loss: 0.0046\n",
      "Epoch 81/100\n",
      "12/12 [==============================] - 1s 91ms/step - loss: 0.0015 - val_loss: 0.0038\n",
      "Epoch 82/100\n",
      "12/12 [==============================] - 1s 92ms/step - loss: 0.0017 - val_loss: 0.0037\n",
      "Epoch 83/100\n",
      "12/12 [==============================] - 1s 90ms/step - loss: 0.0017 - val_loss: 0.0034\n",
      "Epoch 84/100\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 0.0014 - val_loss: 0.0038\n",
      "Epoch 85/100\n",
      "12/12 [==============================] - 1s 85ms/step - loss: 0.0017 - val_loss: 0.0040\n",
      "Epoch 86/100\n",
      "12/12 [==============================] - 1s 83ms/step - loss: 0.0017 - val_loss: 0.0037\n",
      "Epoch 87/100\n",
      "12/12 [==============================] - 1s 82ms/step - loss: 0.0016 - val_loss: 0.0036\n",
      "Epoch 88/100\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 0.0015 - val_loss: 0.0041\n",
      "Epoch 89/100\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 0.0016 - val_loss: 0.0047\n",
      "Epoch 90/100\n",
      "12/12 [==============================] - 1s 85ms/step - loss: 0.0015 - val_loss: 0.0034\n",
      "Epoch 91/100\n",
      "12/12 [==============================] - 1s 85ms/step - loss: 0.0018 - val_loss: 0.0045\n",
      "Epoch 92/100\n",
      "12/12 [==============================] - 1s 84ms/step - loss: 0.0020 - val_loss: 0.0028\n",
      "Epoch 93/100\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 0.0019 - val_loss: 0.0040\n",
      "Epoch 94/100\n",
      "12/12 [==============================] - 1s 81ms/step - loss: 0.0016 - val_loss: 0.0038\n",
      "Epoch 95/100\n",
      "12/12 [==============================] - 1s 82ms/step - loss: 0.0014 - val_loss: 0.0030\n",
      "Epoch 96/100\n",
      "12/12 [==============================] - 1s 82ms/step - loss: 0.0014 - val_loss: 0.0045\n",
      "Epoch 97/100\n",
      "12/12 [==============================] - 1s 82ms/step - loss: 0.0015 - val_loss: 0.0034\n",
      "Epoch 98/100\n",
      "12/12 [==============================] - 1s 84ms/step - loss: 0.0012 - val_loss: 0.0031\n",
      "Epoch 99/100\n",
      "12/12 [==============================] - 1s 85ms/step - loss: 0.0014 - val_loss: 0.0037\n",
      "Epoch 100/100\n",
      "12/12 [==============================] - 1s 85ms/step - loss: 0.0013 - val_loss: 0.0039\n",
      "mae: tf.Tensor(39.115833, shape=(), dtype=float32)\n",
      "mse: tf.Tensor(4623.613, shape=(), dtype=float32)\n",
      "rmse: tf.Tensor(67.997154, shape=(), dtype=float32)\n",
      "mae: tf.Tensor(39.115833, shape=(), dtype=float32)\n",
      "mse: tf.Tensor(4623.613, shape=(), dtype=float32)\n",
      "rmse: tf.Tensor(67.997154, shape=(), dtype=float32)\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 48 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 48 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://app.neptune.ai/mlienart/milestone2/e/MIL06KDIEQ-11\n"
     ]
    }
   ],
   "source": [
    "run = neptune.init(\n",
    "    project=\"mlienart/milestone2\",\n",
    "    api_token=neptune_key,\n",
    "    name=\"Advanced Model 1\"\n",
    ")\n",
    "neptune_cbk = NeptuneCallback(run=run, base_namespace='metrics')\n",
    "hyper_parameters[\"optimizer\"] = \"Adam\"\n",
    "run['hyper-parameters'] = hyper_parameters\n",
    "\n",
    "model2.compile(loss=\"mse\", optimizer=adam_optimizer)\n",
    "history = model2.fit(X_train, y_train,\n",
    "                     validation_split=hyper_parameters[\"validation_split\"],\n",
    "                     batch_size=hyper_parameters[\"batch_size\"],\n",
    "                     epochs=hyper_parameters[\"epochs\"],\n",
    "                     shuffle=True,\n",
    "                     callbacks=[neptune_cbk])\n",
    "yhat = model2.predict(X_test, verbose=0)\n",
    "yhat_inverse = scaler.inverse_transform(yhat)\n",
    "y_test_inverse = scaler.inverse_transform(y_test)\n",
    "results_df[\"experiment_2_prediction\"] = yhat_inverse\n",
    "evaluate_forecast(y_test_inverse, yhat_inverse)\n",
    "mae, mse, rmse = evaluate_forecast(y_test_inverse, yhat_inverse)\n",
    "run[\"eval/mae\"] = mae\n",
    "run[\"eval/mse\"] = mse\n",
    "run[\"eval/rmse\"] = rmse\n",
    "run.stop()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/mlienart/milestone2/e/MIL06KDIEQ-12\n",
      "Remember to stop your run once you’ve finished logging your metadata (https://docs.neptune.ai/api-reference/run#.stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.\n",
      "Epoch 1/100\n",
      "12/12 [==============================] - 4s 146ms/step - loss: 0.0069 - val_loss: 0.0041\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 1s 83ms/step - loss: 0.0018 - val_loss: 0.0161\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 1s 85ms/step - loss: 0.0031 - val_loss: 0.0051\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - 1s 87ms/step - loss: 0.0022 - val_loss: 0.0037\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - 1s 85ms/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 0.0021 - val_loss: 0.0032\n",
      "Epoch 7/100\n",
      "12/12 [==============================] - 1s 88ms/step - loss: 0.0025 - val_loss: 0.0042\n",
      "Epoch 8/100\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 0.0028 - val_loss: 0.0048\n",
      "Epoch 9/100\n",
      "12/12 [==============================] - 1s 83ms/step - loss: 0.0019 - val_loss: 0.0038\n",
      "Epoch 10/100\n",
      "12/12 [==============================] - 1s 82ms/step - loss: 0.0021 - val_loss: 0.0023\n",
      "Epoch 11/100\n",
      "12/12 [==============================] - 1s 83ms/step - loss: 0.0021 - val_loss: 0.0044\n",
      "Epoch 12/100\n",
      "12/12 [==============================] - 1s 87ms/step - loss: 0.0027 - val_loss: 0.0058\n",
      "Epoch 13/100\n",
      "12/12 [==============================] - 1s 87ms/step - loss: 0.0026 - val_loss: 0.0042\n",
      "Epoch 14/100\n",
      "12/12 [==============================] - 1s 88ms/step - loss: 0.0024 - val_loss: 0.0052\n",
      "Epoch 15/100\n",
      "12/12 [==============================] - 1s 87ms/step - loss: 0.0022 - val_loss: 0.0094\n",
      "Epoch 16/100\n",
      "12/12 [==============================] - 1s 87ms/step - loss: 0.0024 - val_loss: 0.0085\n",
      "Epoch 17/100\n",
      "12/12 [==============================] - 1s 89ms/step - loss: 0.0019 - val_loss: 0.0040\n",
      "Epoch 18/100\n",
      "12/12 [==============================] - 1s 87ms/step - loss: 0.0014 - val_loss: 0.0032\n",
      "Epoch 19/100\n",
      "12/12 [==============================] - 1s 85ms/step - loss: 0.0022 - val_loss: 0.0067\n",
      "Epoch 20/100\n",
      "12/12 [==============================] - 1s 84ms/step - loss: 0.0026 - val_loss: 0.0042\n",
      "Epoch 21/100\n",
      "12/12 [==============================] - 1s 83ms/step - loss: 0.0019 - val_loss: 0.0045\n",
      "Epoch 22/100\n",
      "12/12 [==============================] - 1s 84ms/step - loss: 0.0021 - val_loss: 0.0035\n",
      "Epoch 23/100\n",
      "12/12 [==============================] - 1s 84ms/step - loss: 0.0019 - val_loss: 0.0158\n",
      "Epoch 24/100\n",
      "12/12 [==============================] - 1s 85ms/step - loss: 0.0023 - val_loss: 0.0032\n",
      "Epoch 25/100\n",
      "12/12 [==============================] - 1s 87ms/step - loss: 0.0019 - val_loss: 0.0042\n",
      "Epoch 26/100\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 0.0019 - val_loss: 0.0075\n",
      "Epoch 27/100\n",
      "12/12 [==============================] - 1s 90ms/step - loss: 0.0023 - val_loss: 0.0046\n",
      "Epoch 28/100\n",
      "12/12 [==============================] - 1s 87ms/step - loss: 0.0020 - val_loss: 0.0032\n",
      "Epoch 29/100\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 0.0021 - val_loss: 0.0033\n",
      "Epoch 30/100\n",
      "12/12 [==============================] - 1s 83ms/step - loss: 0.0019 - val_loss: 0.0031\n",
      "Epoch 31/100\n",
      "12/12 [==============================] - 1s 84ms/step - loss: 0.0018 - val_loss: 0.0044\n",
      "Epoch 32/100\n",
      "12/12 [==============================] - 1s 84ms/step - loss: 0.0018 - val_loss: 0.0031\n",
      "Epoch 33/100\n",
      "12/12 [==============================] - 1s 83ms/step - loss: 0.0019 - val_loss: 0.0057\n",
      "Epoch 34/100\n",
      "12/12 [==============================] - 1s 85ms/step - loss: 0.0022 - val_loss: 0.0029\n",
      "Epoch 35/100\n",
      "12/12 [==============================] - 1s 88ms/step - loss: 0.0015 - val_loss: 0.0021\n",
      "Epoch 36/100\n",
      "12/12 [==============================] - 1s 90ms/step - loss: 0.0014 - val_loss: 0.0043\n",
      "Epoch 37/100\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 0.0018 - val_loss: 0.0024\n",
      "Epoch 38/100\n",
      "12/12 [==============================] - 1s 87ms/step - loss: 0.0015 - val_loss: 0.0029\n",
      "Epoch 39/100\n",
      "12/12 [==============================] - 1s 87ms/step - loss: 0.0017 - val_loss: 0.0020\n",
      "Epoch 40/100\n",
      "12/12 [==============================] - 1s 84ms/step - loss: 0.0019 - val_loss: 0.0030\n",
      "Epoch 41/100\n",
      "12/12 [==============================] - 1s 87ms/step - loss: 0.0017 - val_loss: 0.0037\n",
      "Epoch 42/100\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 0.0016 - val_loss: 0.0057\n",
      "Epoch 43/100\n",
      "12/12 [==============================] - 1s 84ms/step - loss: 0.0018 - val_loss: 0.0088\n",
      "Epoch 44/100\n",
      "12/12 [==============================] - 1s 83ms/step - loss: 0.0016 - val_loss: 0.0045\n",
      "Epoch 45/100\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 0.0014 - val_loss: 0.0033\n",
      "Epoch 46/100\n",
      "12/12 [==============================] - 1s 87ms/step - loss: 0.0013 - val_loss: 0.0026\n",
      "Epoch 47/100\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 0.0016 - val_loss: 0.0027\n",
      "Epoch 48/100\n",
      "12/12 [==============================] - 1s 88ms/step - loss: 0.0014 - val_loss: 0.0033\n",
      "Epoch 49/100\n",
      "12/12 [==============================] - 1s 91ms/step - loss: 0.0013 - val_loss: 0.0034\n",
      "Epoch 50/100\n",
      "12/12 [==============================] - 1s 88ms/step - loss: 0.0018 - val_loss: 0.0036\n",
      "Epoch 51/100\n",
      "12/12 [==============================] - 1s 88ms/step - loss: 0.0017 - val_loss: 0.0031\n",
      "Epoch 52/100\n",
      "12/12 [==============================] - 1s 84ms/step - loss: 0.0015 - val_loss: 0.0053\n",
      "Epoch 53/100\n",
      "12/12 [==============================] - 1s 84ms/step - loss: 0.0019 - val_loss: 0.0032\n",
      "Epoch 54/100\n",
      "12/12 [==============================] - 1s 83ms/step - loss: 0.0011 - val_loss: 0.0035\n",
      "Epoch 55/100\n",
      "12/12 [==============================] - 1s 89ms/step - loss: 0.0017 - val_loss: 0.0031\n",
      "Epoch 56/100\n",
      "12/12 [==============================] - 1s 96ms/step - loss: 0.0011 - val_loss: 0.0029\n",
      "Epoch 57/100\n",
      "12/12 [==============================] - 1s 88ms/step - loss: 0.0020 - val_loss: 0.0046\n",
      "Epoch 58/100\n",
      "12/12 [==============================] - 1s 90ms/step - loss: 0.0011 - val_loss: 0.0032\n",
      "Epoch 59/100\n",
      "12/12 [==============================] - 1s 87ms/step - loss: 0.0015 - val_loss: 0.0043\n",
      "Epoch 60/100\n",
      "12/12 [==============================] - 1s 88ms/step - loss: 0.0018 - val_loss: 0.0028\n",
      "Epoch 61/100\n",
      "12/12 [==============================] - 1s 83ms/step - loss: 0.0014 - val_loss: 0.0027\n",
      "Epoch 62/100\n",
      "12/12 [==============================] - 1s 84ms/step - loss: 0.0014 - val_loss: 0.0036\n",
      "Epoch 63/100\n",
      "12/12 [==============================] - 1s 83ms/step - loss: 0.0016 - val_loss: 0.0059\n",
      "Epoch 64/100\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 0.0016 - val_loss: 0.0044\n",
      "Epoch 65/100\n",
      "12/12 [==============================] - 1s 89ms/step - loss: 0.0013 - val_loss: 0.0035\n",
      "Epoch 66/100\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 0.0016 - val_loss: 0.0035\n",
      "Epoch 67/100\n",
      "12/12 [==============================] - 1s 87ms/step - loss: 0.0011 - val_loss: 0.0041\n",
      "Epoch 68/100\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 0.0013 - val_loss: 0.0031\n",
      "Epoch 69/100\n",
      "12/12 [==============================] - 1s 88ms/step - loss: 0.0014 - val_loss: 0.0044\n",
      "Epoch 70/100\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 0.0012 - val_loss: 0.0039\n",
      "Epoch 71/100\n",
      "12/12 [==============================] - 1s 84ms/step - loss: 0.0014 - val_loss: 0.0029\n",
      "Epoch 72/100\n",
      "12/12 [==============================] - 1s 85ms/step - loss: 0.0015 - val_loss: 0.0036\n",
      "Epoch 73/100\n",
      "12/12 [==============================] - 1s 85ms/step - loss: 9.8947e-04 - val_loss: 0.0035\n",
      "Epoch 74/100\n",
      "12/12 [==============================] - 1s 84ms/step - loss: 0.0016 - val_loss: 0.0084\n",
      "Epoch 75/100\n",
      "12/12 [==============================] - 1s 96ms/step - loss: 0.0013 - val_loss: 0.0043\n",
      "Epoch 76/100\n",
      "12/12 [==============================] - 1s 88ms/step - loss: 0.0011 - val_loss: 0.0029\n",
      "Epoch 77/100\n",
      "12/12 [==============================] - 1s 89ms/step - loss: 0.0012 - val_loss: 0.0039\n",
      "Epoch 78/100\n",
      "12/12 [==============================] - 1s 88ms/step - loss: 0.0012 - val_loss: 0.0038\n",
      "Epoch 79/100\n",
      "12/12 [==============================] - 1s 88ms/step - loss: 0.0011 - val_loss: 0.0029\n",
      "Epoch 80/100\n",
      "12/12 [==============================] - 1s 87ms/step - loss: 8.9508e-04 - val_loss: 0.0040\n",
      "Epoch 81/100\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 0.0015 - val_loss: 0.0029\n",
      "Epoch 82/100\n",
      "12/12 [==============================] - 1s 84ms/step - loss: 0.0014 - val_loss: 0.0042\n",
      "Epoch 83/100\n",
      "12/12 [==============================] - 1s 84ms/step - loss: 0.0014 - val_loss: 0.0027\n",
      "Epoch 84/100\n",
      "12/12 [==============================] - 1s 87ms/step - loss: 0.0012 - val_loss: 0.0031\n",
      "Epoch 85/100\n",
      "12/12 [==============================] - 1s 84ms/step - loss: 0.0018 - val_loss: 0.0029\n",
      "Epoch 86/100\n",
      "12/12 [==============================] - 1s 87ms/step - loss: 9.7579e-04 - val_loss: 0.0029\n",
      "Epoch 87/100\n",
      "12/12 [==============================] - 1s 88ms/step - loss: 0.0010 - val_loss: 0.0033\n",
      "Epoch 88/100\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 9.4468e-04 - val_loss: 0.0037\n",
      "Epoch 89/100\n",
      "12/12 [==============================] - 1s 92ms/step - loss: 0.0011 - val_loss: 0.0025\n",
      "Epoch 90/100\n",
      "12/12 [==============================] - 1s 87ms/step - loss: 0.0011 - val_loss: 0.0031\n",
      "Epoch 91/100\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 9.4684e-04 - val_loss: 0.0029\n",
      "Epoch 92/100\n",
      "12/12 [==============================] - 1s 83ms/step - loss: 0.0013 - val_loss: 0.0031\n",
      "Epoch 93/100\n",
      "12/12 [==============================] - 1s 84ms/step - loss: 8.5876e-04 - val_loss: 0.0035\n",
      "Epoch 94/100\n",
      "12/12 [==============================] - 1s 83ms/step - loss: 0.0010 - val_loss: 0.0058\n",
      "Epoch 95/100\n",
      "12/12 [==============================] - 1s 83ms/step - loss: 0.0013 - val_loss: 0.0036\n",
      "Epoch 96/100\n",
      "12/12 [==============================] - 1s 85ms/step - loss: 9.5318e-04 - val_loss: 0.0032\n",
      "Epoch 97/100\n",
      "12/12 [==============================] - 1s 87ms/step - loss: 0.0013 - val_loss: 0.0040\n",
      "Epoch 98/100\n",
      "12/12 [==============================] - 1s 87ms/step - loss: 0.0012 - val_loss: 0.0031\n",
      "Epoch 99/100\n",
      "12/12 [==============================] - 1s 90ms/step - loss: 9.7450e-04 - val_loss: 0.0041\n",
      "Epoch 100/100\n",
      "12/12 [==============================] - 1s 87ms/step - loss: 9.1701e-04 - val_loss: 0.0036\n",
      "mae: tf.Tensor(38.735203, shape=(), dtype=float32)\n",
      "mse: tf.Tensor(4670.656, shape=(), dtype=float32)\n",
      "rmse: tf.Tensor(68.34219, shape=(), dtype=float32)\n",
      "mae: tf.Tensor(38.735203, shape=(), dtype=float32)\n",
      "mse: tf.Tensor(4670.656, shape=(), dtype=float32)\n",
      "rmse: tf.Tensor(68.34219, shape=(), dtype=float32)\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 69 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 69 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://app.neptune.ai/mlienart/milestone2/e/MIL06KDIEQ-12\n"
     ]
    }
   ],
   "source": [
    "run = neptune.init(\n",
    "    project=\"mlienart/milestone2\",\n",
    "    api_token=neptune_key,\n",
    "    name=\"Advanced Model 1\"\n",
    ")\n",
    "neptune_cbk = NeptuneCallback(run=run, base_namespace='metrics')\n",
    "hyper_parameters[\"optimizer\"] = \"RMSprop\"\n",
    "run['hyper-parameters'] = hyper_parameters\n",
    "\n",
    "model2.compile(loss=\"mse\", optimizer=rms_optimizer)\n",
    "history = model2.fit(X_train, y_train,\n",
    "                     validation_split=hyper_parameters[\"validation_split\"],\n",
    "                     batch_size=hyper_parameters[\"batch_size\"],\n",
    "                     epochs=hyper_parameters[\"epochs\"],\n",
    "                     shuffle=True,\n",
    "                     callbacks=[neptune_cbk])\n",
    "yhat = model2.predict(X_test, verbose=0)\n",
    "yhat_inverse = scaler.inverse_transform(yhat)\n",
    "y_test_inverse = scaler.inverse_transform(y_test)\n",
    "results_df[\"experiment_3_prediction\"] = yhat_inverse\n",
    "evaluate_forecast(y_test_inverse, yhat_inverse)\n",
    "mae, mse, rmse = evaluate_forecast(y_test_inverse, yhat_inverse)\n",
    "run[\"eval/mae\"] = mae\n",
    "run[\"eval/mse\"] = mse\n",
    "run[\"eval/rmse\"] = rmse\n",
    "run.stop()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/mlienart/milestone2/e/MIL06KDIEQ-13\n",
      "Remember to stop your run once you’ve finished logging your metadata (https://docs.neptune.ai/api-reference/run#.stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.\n",
      "Epoch 1/100\n",
      "12/12 [==============================] - 4s 145ms/step - loss: 9.6141e-04 - val_loss: 0.0029\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 1s 93ms/step - loss: 8.8754e-04 - val_loss: 0.0033\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 1s 92ms/step - loss: 7.0335e-04 - val_loss: 0.0031\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - 1s 92ms/step - loss: 6.1270e-04 - val_loss: 0.0029\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - 1s 93ms/step - loss: 8.2415e-04 - val_loss: 0.0030\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - 1s 97ms/step - loss: 8.9607e-04 - val_loss: 0.0032\n",
      "Epoch 7/100\n",
      "12/12 [==============================] - 1s 96ms/step - loss: 6.4623e-04 - val_loss: 0.0031\n",
      "Epoch 8/100\n",
      "12/12 [==============================] - 1s 94ms/step - loss: 6.3054e-04 - val_loss: 0.0030\n",
      "Epoch 9/100\n",
      "12/12 [==============================] - 1s 92ms/step - loss: 5.7793e-04 - val_loss: 0.0030\n",
      "Epoch 10/100\n",
      "12/12 [==============================] - 1s 91ms/step - loss: 5.8808e-04 - val_loss: 0.0031\n",
      "Epoch 11/100\n",
      "12/12 [==============================] - 1s 93ms/step - loss: 7.2423e-04 - val_loss: 0.0032\n",
      "Epoch 12/100\n",
      "12/12 [==============================] - 1s 96ms/step - loss: 7.9480e-04 - val_loss: 0.0030\n",
      "Epoch 13/100\n",
      "12/12 [==============================] - 1s 94ms/step - loss: 7.5533e-04 - val_loss: 0.0032\n",
      "Epoch 14/100\n",
      "12/12 [==============================] - 1s 94ms/step - loss: 5.9505e-04 - val_loss: 0.0032\n",
      "Epoch 15/100\n",
      "12/12 [==============================] - 1s 94ms/step - loss: 7.9775e-04 - val_loss: 0.0031\n",
      "Epoch 16/100\n",
      "12/12 [==============================] - 1s 94ms/step - loss: 7.1168e-04 - val_loss: 0.0030\n",
      "Epoch 17/100\n",
      "12/12 [==============================] - 1s 93ms/step - loss: 7.5017e-04 - val_loss: 0.0030\n",
      "Epoch 18/100\n",
      "12/12 [==============================] - 1s 93ms/step - loss: 6.0971e-04 - val_loss: 0.0031\n",
      "Epoch 19/100\n",
      "12/12 [==============================] - 1s 93ms/step - loss: 6.9903e-04 - val_loss: 0.0032\n",
      "Epoch 20/100\n",
      "12/12 [==============================] - 1s 94ms/step - loss: 5.1931e-04 - val_loss: 0.0033\n",
      "Epoch 21/100\n",
      "12/12 [==============================] - 1s 108ms/step - loss: 6.4614e-04 - val_loss: 0.0031\n",
      "Epoch 22/100\n",
      "12/12 [==============================] - 1s 119ms/step - loss: 6.1062e-04 - val_loss: 0.0031\n",
      "Epoch 23/100\n",
      "12/12 [==============================] - 1s 119ms/step - loss: 5.6966e-04 - val_loss: 0.0032\n",
      "Epoch 24/100\n",
      "12/12 [==============================] - 1s 94ms/step - loss: 6.5418e-04 - val_loss: 0.0030\n",
      "Epoch 25/100\n",
      "12/12 [==============================] - 1s 92ms/step - loss: 6.0853e-04 - val_loss: 0.0028\n",
      "Epoch 26/100\n",
      "12/12 [==============================] - 1s 93ms/step - loss: 6.5069e-04 - val_loss: 0.0031\n",
      "Epoch 27/100\n",
      "12/12 [==============================] - 1s 98ms/step - loss: 6.1529e-04 - val_loss: 0.0031\n",
      "Epoch 28/100\n",
      "12/12 [==============================] - 1s 106ms/step - loss: 6.6718e-04 - val_loss: 0.0030\n",
      "Epoch 29/100\n",
      "12/12 [==============================] - 1s 103ms/step - loss: 6.0922e-04 - val_loss: 0.0030\n",
      "Epoch 30/100\n",
      "12/12 [==============================] - 1s 99ms/step - loss: 5.4170e-04 - val_loss: 0.0031\n",
      "Epoch 31/100\n",
      "12/12 [==============================] - 1s 93ms/step - loss: 5.9082e-04 - val_loss: 0.0030\n",
      "Epoch 32/100\n",
      "12/12 [==============================] - 1s 92ms/step - loss: 4.9858e-04 - val_loss: 0.0028\n",
      "Epoch 33/100\n",
      "12/12 [==============================] - 1s 94ms/step - loss: 7.2141e-04 - val_loss: 0.0028\n",
      "Epoch 34/100\n",
      "12/12 [==============================] - 1s 97ms/step - loss: 6.1994e-04 - val_loss: 0.0029\n",
      "Epoch 35/100\n",
      "12/12 [==============================] - 1s 93ms/step - loss: 6.8315e-04 - val_loss: 0.0028\n",
      "Epoch 36/100\n",
      "12/12 [==============================] - 1s 90ms/step - loss: 6.7979e-04 - val_loss: 0.0029\n",
      "Epoch 37/100\n",
      "12/12 [==============================] - 1s 96ms/step - loss: 5.8830e-04 - val_loss: 0.0029\n",
      "Epoch 38/100\n",
      "12/12 [==============================] - 1s 111ms/step - loss: 8.6197e-04 - val_loss: 0.0029\n",
      "Epoch 39/100\n",
      "12/12 [==============================] - 1s 99ms/step - loss: 5.5641e-04 - val_loss: 0.0029\n",
      "Epoch 40/100\n",
      "12/12 [==============================] - 1s 92ms/step - loss: 5.1323e-04 - val_loss: 0.0030\n",
      "Epoch 41/100\n",
      "12/12 [==============================] - 1s 91ms/step - loss: 5.1812e-04 - val_loss: 0.0030\n",
      "Epoch 42/100\n",
      "12/12 [==============================] - 1s 91ms/step - loss: 6.1432e-04 - val_loss: 0.0030\n",
      "Epoch 43/100\n",
      "12/12 [==============================] - 1s 93ms/step - loss: 5.9642e-04 - val_loss: 0.0029\n",
      "Epoch 44/100\n",
      "12/12 [==============================] - 1s 102ms/step - loss: 5.4137e-04 - val_loss: 0.0030\n",
      "Epoch 45/100\n",
      "12/12 [==============================] - 2s 153ms/step - loss: 5.5795e-04 - val_loss: 0.0028\n",
      "Epoch 46/100\n",
      "12/12 [==============================] - 1s 91ms/step - loss: 5.8682e-04 - val_loss: 0.0028\n",
      "Epoch 47/100\n",
      "12/12 [==============================] - 1s 91ms/step - loss: 6.0694e-04 - val_loss: 0.0030\n",
      "Epoch 48/100\n",
      "12/12 [==============================] - 1s 90ms/step - loss: 5.5765e-04 - val_loss: 0.0032\n",
      "Epoch 49/100\n",
      "12/12 [==============================] - 1s 91ms/step - loss: 6.2784e-04 - val_loss: 0.0030\n",
      "Epoch 50/100\n",
      "12/12 [==============================] - 1s 95ms/step - loss: 5.6550e-04 - val_loss: 0.0029\n",
      "Epoch 51/100\n",
      "12/12 [==============================] - 1s 90ms/step - loss: 6.3161e-04 - val_loss: 0.0030\n",
      "Epoch 52/100\n",
      "12/12 [==============================] - 1s 101ms/step - loss: 6.4342e-04 - val_loss: 0.0031\n",
      "Epoch 53/100\n",
      "12/12 [==============================] - 1s 108ms/step - loss: 7.3708e-04 - val_loss: 0.0028\n",
      "Epoch 54/100\n",
      "12/12 [==============================] - 1s 125ms/step - loss: 5.3257e-04 - val_loss: 0.0030\n",
      "Epoch 55/100\n",
      "12/12 [==============================] - 1s 94ms/step - loss: 4.8851e-04 - val_loss: 0.0031\n",
      "Epoch 56/100\n",
      "12/12 [==============================] - 1s 94ms/step - loss: 5.7571e-04 - val_loss: 0.0030\n",
      "Epoch 57/100\n",
      "12/12 [==============================] - 1s 96ms/step - loss: 6.4375e-04 - val_loss: 0.0029\n",
      "Epoch 58/100\n",
      "12/12 [==============================] - 1s 96ms/step - loss: 5.8981e-04 - val_loss: 0.0031\n",
      "Epoch 59/100\n",
      "12/12 [==============================] - 1s 101ms/step - loss: 5.3614e-04 - val_loss: 0.0030\n",
      "Epoch 60/100\n",
      "12/12 [==============================] - 1s 97ms/step - loss: 5.9990e-04 - val_loss: 0.0029\n",
      "Epoch 61/100\n",
      "12/12 [==============================] - 1s 109ms/step - loss: 4.1778e-04 - val_loss: 0.0030\n",
      "Epoch 62/100\n",
      "12/12 [==============================] - 1s 108ms/step - loss: 5.9893e-04 - val_loss: 0.0030\n",
      "Epoch 63/100\n",
      "12/12 [==============================] - 1s 92ms/step - loss: 5.8254e-04 - val_loss: 0.0030\n",
      "Epoch 64/100\n",
      "12/12 [==============================] - 1s 92ms/step - loss: 6.3407e-04 - val_loss: 0.0030\n",
      "Epoch 65/100\n",
      "12/12 [==============================] - 1s 98ms/step - loss: 5.4108e-04 - val_loss: 0.0032\n",
      "Epoch 66/100\n",
      "12/12 [==============================] - 1s 92ms/step - loss: 4.8526e-04 - val_loss: 0.0031\n",
      "Epoch 67/100\n",
      "12/12 [==============================] - 1s 93ms/step - loss: 6.4968e-04 - val_loss: 0.0030\n",
      "Epoch 68/100\n",
      "12/12 [==============================] - 1s 91ms/step - loss: 4.1374e-04 - val_loss: 0.0030\n",
      "Epoch 69/100\n",
      "12/12 [==============================] - 1s 92ms/step - loss: 5.4104e-04 - val_loss: 0.0030\n",
      "Epoch 70/100\n",
      "12/12 [==============================] - 1s 126ms/step - loss: 5.5208e-04 - val_loss: 0.0030\n",
      "Epoch 71/100\n",
      "12/12 [==============================] - 1s 90ms/step - loss: 5.2015e-04 - val_loss: 0.0030\n",
      "Epoch 72/100\n",
      "12/12 [==============================] - 1s 91ms/step - loss: 4.9096e-04 - val_loss: 0.0031\n",
      "Epoch 73/100\n",
      "12/12 [==============================] - 1s 93ms/step - loss: 4.8386e-04 - val_loss: 0.0031\n",
      "Epoch 74/100\n",
      "12/12 [==============================] - 1s 92ms/step - loss: 4.6448e-04 - val_loss: 0.0029\n",
      "Epoch 75/100\n",
      "12/12 [==============================] - 1s 97ms/step - loss: 5.7094e-04 - val_loss: 0.0029\n",
      "Epoch 76/100\n",
      "12/12 [==============================] - 1s 92ms/step - loss: 4.9017e-04 - val_loss: 0.0030\n",
      "Epoch 77/100\n",
      "12/12 [==============================] - 1s 91ms/step - loss: 5.5296e-04 - val_loss: 0.0031\n",
      "Epoch 78/100\n",
      "12/12 [==============================] - 1s 92ms/step - loss: 5.0002e-04 - val_loss: 0.0030\n",
      "Epoch 79/100\n",
      "12/12 [==============================] - 2s 132ms/step - loss: 7.1319e-04 - val_loss: 0.0031\n",
      "Epoch 80/100\n",
      "12/12 [==============================] - 2s 127ms/step - loss: 5.2164e-04 - val_loss: 0.0030\n",
      "Epoch 81/100\n",
      "12/12 [==============================] - 1s 100ms/step - loss: 5.2077e-04 - val_loss: 0.0029\n",
      "Epoch 82/100\n",
      "12/12 [==============================] - 1s 98ms/step - loss: 5.0103e-04 - val_loss: 0.0030\n",
      "Epoch 83/100\n",
      "12/12 [==============================] - 1s 93ms/step - loss: 5.1398e-04 - val_loss: 0.0029\n",
      "Epoch 84/100\n",
      "12/12 [==============================] - 1s 90ms/step - loss: 4.8094e-04 - val_loss: 0.0029\n",
      "Epoch 85/100\n",
      "12/12 [==============================] - 1s 90ms/step - loss: 6.0959e-04 - val_loss: 0.0030\n",
      "Epoch 86/100\n",
      "12/12 [==============================] - 1s 91ms/step - loss: 5.4265e-04 - val_loss: 0.0030\n",
      "Epoch 87/100\n",
      "12/12 [==============================] - 1s 98ms/step - loss: 4.7410e-04 - val_loss: 0.0030\n",
      "Epoch 88/100\n",
      "12/12 [==============================] - 1s 115ms/step - loss: 4.7685e-04 - val_loss: 0.0030\n",
      "Epoch 89/100\n",
      "12/12 [==============================] - 1s 97ms/step - loss: 5.5675e-04 - val_loss: 0.0031\n",
      "Epoch 90/100\n",
      "12/12 [==============================] - 1s 95ms/step - loss: 4.3058e-04 - val_loss: 0.0030\n",
      "Epoch 91/100\n",
      "12/12 [==============================] - 1s 91ms/step - loss: 5.0699e-04 - val_loss: 0.0030\n",
      "Epoch 92/100\n",
      "12/12 [==============================] - 1s 93ms/step - loss: 4.5389e-04 - val_loss: 0.0030\n",
      "Epoch 93/100\n",
      "12/12 [==============================] - 1s 95ms/step - loss: 5.3802e-04 - val_loss: 0.0030\n",
      "Epoch 94/100\n",
      "12/12 [==============================] - 1s 90ms/step - loss: 6.0933e-04 - val_loss: 0.0032\n",
      "Epoch 95/100\n",
      "12/12 [==============================] - 1s 92ms/step - loss: 4.8619e-04 - val_loss: 0.0029\n",
      "Epoch 96/100\n",
      "12/12 [==============================] - 1s 102ms/step - loss: 4.7378e-04 - val_loss: 0.0030\n",
      "Epoch 97/100\n",
      "12/12 [==============================] - 1s 97ms/step - loss: 4.6451e-04 - val_loss: 0.0030\n",
      "Epoch 98/100\n",
      "12/12 [==============================] - 1s 93ms/step - loss: 4.0650e-04 - val_loss: 0.0031\n",
      "Epoch 99/100\n",
      "12/12 [==============================] - 1s 91ms/step - loss: 5.0577e-04 - val_loss: 0.0033\n",
      "Epoch 100/100\n",
      "12/12 [==============================] - 1s 92ms/step - loss: 5.9737e-04 - val_loss: 0.0032\n",
      "mae: tf.Tensor(37.95624, shape=(), dtype=float32)\n",
      "mse: tf.Tensor(3959.7734, shape=(), dtype=float32)\n",
      "rmse: tf.Tensor(62.92673, shape=(), dtype=float32)\n",
      "mae: tf.Tensor(37.95624, shape=(), dtype=float32)\n",
      "mse: tf.Tensor(3959.7734, shape=(), dtype=float32)\n",
      "rmse: tf.Tensor(62.92673, shape=(), dtype=float32)\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 136 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 136 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://app.neptune.ai/mlienart/milestone2/e/MIL06KDIEQ-13\n"
     ]
    }
   ],
   "source": [
    "run = neptune.init(\n",
    "    project=\"mlienart/milestone2\",\n",
    "    api_token=neptune_key,\n",
    "    name=\"Advanced Model 1\"\n",
    ")\n",
    "neptune_cbk = NeptuneCallback(run=run, base_namespace='metrics')\n",
    "hyper_parameters[\"optimizer\"] = \"Adamx\"\n",
    "run['hyper-parameters'] = hyper_parameters\n",
    "\n",
    "model2.compile(loss=\"mse\", optimizer=adamax_optimizer)\n",
    "history = model2.fit(X_train, y_train,\n",
    "                     validation_split=hyper_parameters[\"validation_split\"],\n",
    "                     batch_size=hyper_parameters[\"batch_size\"],\n",
    "                     epochs=hyper_parameters[\"epochs\"],\n",
    "                     shuffle=True,\n",
    "                     callbacks=[neptune_cbk])\n",
    "yhat = model2.predict(X_test, verbose=0)\n",
    "yhat_inverse = scaler.inverse_transform(yhat)\n",
    "y_test_inverse = scaler.inverse_transform(y_test)\n",
    "results_df[\"experiment_4_prediction\"] = yhat_inverse\n",
    "evaluate_forecast(y_test_inverse, yhat_inverse)\n",
    "mae, mse, rmse = evaluate_forecast(y_test_inverse, yhat_inverse)\n",
    "run[\"eval/mae\"] = mae\n",
    "run[\"eval/mse\"] = mse\n",
    "run[\"eval/rmse\"] = rmse\n",
    "run.stop()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "data": {
      "text/plain": "                        GSE_GWE  experiment_1_prediction  \\\nTOWNSHIP_RANGE YEAR                                        \nT01N R02E      2021   53.193636                65.490639   \nT01N R11E      2021  107.955000               146.327942   \nT01S R03E      2021   24.494538                32.694504   \nT01S R07E      2021   38.644000                36.654675   \nT01S R10E      2021  113.651250               115.802238   \n...                         ...                      ...   \nT31S R26E      2021  173.915909               204.272202   \nT31S R31E      2021  403.900000               350.634827   \nT32S R22E      2021  160.340000               218.650299   \nT32S R25E      2021  190.120000               242.645889   \nT32S R26E      2021  220.866667               187.313095   \n\n                     experiment_2_prediction  experiment_3_prediction  \\\nTOWNSHIP_RANGE YEAR                                                     \nT01N R02E      2021                64.366951                54.697304   \nT01N R11E      2021               174.259918               133.298782   \nT01S R03E      2021                37.510136                38.773586   \nT01S R07E      2021                39.726437                35.594738   \nT01S R10E      2021               122.838219               111.689659   \n...                                      ...                      ...   \nT31S R26E      2021               230.257080               184.224365   \nT31S R31E      2021               420.025665               392.852051   \nT32S R22E      2021               273.818573               254.131851   \nT32S R25E      2021               218.724792               199.926193   \nT32S R26E      2021               207.646225               161.254501   \n\n                     experiment_4_prediction  \nTOWNSHIP_RANGE YEAR                           \nT01N R02E      2021                57.295902  \nT01N R11E      2021               149.292389  \nT01S R03E      2021                34.343391  \nT01S R07E      2021                43.306934  \nT01S R10E      2021               110.948303  \n...                                      ...  \nT31S R26E      2021               201.201935  \nT31S R31E      2021               360.723633  \nT32S R22E      2021               230.082672  \nT32S R25E      2021               227.870270  \nT32S R26E      2021               170.737930  \n\n[96 rows x 5 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th>GSE_GWE</th>\n      <th>experiment_1_prediction</th>\n      <th>experiment_2_prediction</th>\n      <th>experiment_3_prediction</th>\n      <th>experiment_4_prediction</th>\n    </tr>\n    <tr>\n      <th>TOWNSHIP_RANGE</th>\n      <th>YEAR</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>T01N R02E</th>\n      <th>2021</th>\n      <td>53.193636</td>\n      <td>65.490639</td>\n      <td>64.366951</td>\n      <td>54.697304</td>\n      <td>57.295902</td>\n    </tr>\n    <tr>\n      <th>T01N R11E</th>\n      <th>2021</th>\n      <td>107.955000</td>\n      <td>146.327942</td>\n      <td>174.259918</td>\n      <td>133.298782</td>\n      <td>149.292389</td>\n    </tr>\n    <tr>\n      <th>T01S R03E</th>\n      <th>2021</th>\n      <td>24.494538</td>\n      <td>32.694504</td>\n      <td>37.510136</td>\n      <td>38.773586</td>\n      <td>34.343391</td>\n    </tr>\n    <tr>\n      <th>T01S R07E</th>\n      <th>2021</th>\n      <td>38.644000</td>\n      <td>36.654675</td>\n      <td>39.726437</td>\n      <td>35.594738</td>\n      <td>43.306934</td>\n    </tr>\n    <tr>\n      <th>T01S R10E</th>\n      <th>2021</th>\n      <td>113.651250</td>\n      <td>115.802238</td>\n      <td>122.838219</td>\n      <td>111.689659</td>\n      <td>110.948303</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>T31S R26E</th>\n      <th>2021</th>\n      <td>173.915909</td>\n      <td>204.272202</td>\n      <td>230.257080</td>\n      <td>184.224365</td>\n      <td>201.201935</td>\n    </tr>\n    <tr>\n      <th>T31S R31E</th>\n      <th>2021</th>\n      <td>403.900000</td>\n      <td>350.634827</td>\n      <td>420.025665</td>\n      <td>392.852051</td>\n      <td>360.723633</td>\n    </tr>\n    <tr>\n      <th>T32S R22E</th>\n      <th>2021</th>\n      <td>160.340000</td>\n      <td>218.650299</td>\n      <td>273.818573</td>\n      <td>254.131851</td>\n      <td>230.082672</td>\n    </tr>\n    <tr>\n      <th>T32S R25E</th>\n      <th>2021</th>\n      <td>190.120000</td>\n      <td>242.645889</td>\n      <td>218.724792</td>\n      <td>199.926193</td>\n      <td>227.870270</td>\n    </tr>\n    <tr>\n      <th>T32S R26E</th>\n      <th>2021</th>\n      <td>220.866667</td>\n      <td>187.313095</td>\n      <td>207.646225</td>\n      <td>161.254501</td>\n      <td>170.737930</td>\n    </tr>\n  </tbody>\n</table>\n<p>96 rows × 5 columns</p>\n</div>"
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}