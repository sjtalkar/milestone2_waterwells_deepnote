{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import neptune.new as neptune\n",
    "from neptune.new.integrations.tensorflow_keras import NeptuneCallback\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, TimeDistributed, Conv1D, MaxPooling1D, Flatten, Bidirectional, Input, Flatten, Activation, Reshape, RepeatVector, Concatenate\n",
    "from keras.optimizers import RMSprop, Adam, Adamax\n",
    "\n",
    "from lib.read_data import read_and_join_output_file\n",
    "#from lib.create_pipeline import create_transformation_pipeline\n",
    "from lib.deeplearning import create_transformation_pipelines\n",
    "from lib.transform_impute import convert_back_df\n",
    "from lib.split_data import train_test_group_time_split"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tensorflow.config.list_physical_devices('GPU')))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# During experiment we can try to use neptune.ai to log all the Tensorflow experiments results\n",
    "neptune_key = pickle.load(open(\"./neptune.pkl\", \"rb\"))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preparing the Dataset\n",
    "The train and test sets are split by Township-Ranges, i.e. some Township-Ranges data are either fully in the train or test set.\n",
    "The target value is the value of that variable for 2021\n",
    "Thus train/test sets are of shape (number of Township-Ranges, 7 years (2014-2020), the number of features).\n",
    "The input of 1 data point in the model is of shape (7x81\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "                     TOTALDRILLDEPTH_AVG  WELLYIELD_AVG  STATICWATERLEVEL_AVG  \\\nTOWNSHIP_RANGE YEAR                                                             \nT01N R03E      2014             0.097778       0.018246              0.037145   \n               2015             0.095238       0.021053              0.025042   \n               2016             0.114286       0.007916              0.022398   \n               2017             0.000000       0.013684              0.030885   \n               2018             0.083873       0.002474              0.034558   \n...                                  ...            ...                   ...   \nT32S R30E      2016             0.000000       0.000000              0.000000   \n               2017             0.000000       0.000000              0.000000   \n               2018             0.000000       0.000000              0.000000   \n               2019             0.000000       0.000000              0.000000   \n               2020             0.000000       0.000000              0.000000   \n\n                     TOPOFPERFORATEDINTERVAL_AVG  \\\nTOWNSHIP_RANGE YEAR                                \nT01N R03E      2014                     0.098039   \n               2015                     0.117647   \n               2016                     0.152614   \n               2017                     0.127451   \n               2018                     0.148257   \n...                                          ...   \nT32S R30E      2016                     0.000000   \n               2017                     0.000000   \n               2018                     0.000000   \n               2019                     0.000000   \n               2020                     0.000000   \n\n                     BOTTOMOFPERFORATEDINTERVAL_AVG  TOTALCOMPLETEDDEPTH_AVG  \\\nTOWNSHIP_RANGE YEAR                                                            \nT01N R03E      2014                        0.111111                 0.105856   \n               2015                        0.080460                 0.079848   \n               2016                        0.103768                 0.104880   \n               2017                        0.082375                 0.081749   \n               2018                        0.093934                 0.107605   \n...                                             ...                      ...   \nT32S R30E      2016                        0.000000                 0.000000   \n               2017                        0.000000                 0.000000   \n               2018                        0.000000                 0.000000   \n               2019                        0.000000                 0.000000   \n               2020                        0.000000                 0.000000   \n\n                     VEGETATION_BLUE_OAK-GRAY_PINE  \\\nTOWNSHIP_RANGE YEAR                                  \nT01N R03E      2014                       0.000037   \n               2015                       0.000037   \n               2016                       0.000037   \n               2017                       0.000037   \n               2018                       0.000037   \n...                                            ...   \nT32S R30E      2016                       0.033178   \n               2017                       0.033178   \n               2018                       0.033178   \n               2019                       0.033178   \n               2020                       0.033178   \n\n                     VEGETATION_CALIFORNIA_COAST_LIVE_OAK  \\\nTOWNSHIP_RANGE YEAR                                         \nT01N R03E      2014                              0.000137   \n               2015                              0.000137   \n               2016                              0.000137   \n               2017                              0.000137   \n               2018                              0.000137   \n...                                                   ...   \nT32S R30E      2016                              0.000000   \n               2017                              0.000000   \n               2018                              0.000000   \n               2019                              0.000000   \n               2020                              0.000000   \n\n                     VEGETATION_CANYON_LIVE_OAK  VEGETATION_HARD_CHAPARRAL  \\\nTOWNSHIP_RANGE YEAR                                                          \nT01N R03E      2014                    0.000000                   0.000386   \n               2015                    0.000000                   0.000386   \n               2016                    0.000000                   0.000386   \n               2017                    0.000000                   0.000386   \n               2018                    0.000000                   0.000386   \n...                                         ...                        ...   \nT32S R30E      2016                    0.002023                   0.003535   \n               2017                    0.002023                   0.003535   \n               2018                    0.002023                   0.003535   \n               2019                    0.002023                   0.003535   \n               2020                    0.002023                   0.003535   \n\n                     ...  POPULATION_DENSITY  PCT_OF_CAPACITY  \\\nTOWNSHIP_RANGE YEAR  ...                                        \nT01N R03E      2014  ...            0.252900         0.717075   \n               2015  ...            0.252799         0.717075   \n               2016  ...            0.250621         0.717075   \n               2017  ...            0.254669         0.717075   \n               2018  ...            0.256461         0.800728   \n...                  ...                 ...              ...   \nT32S R30E      2016  ...            0.004469         0.496289   \n               2017  ...            0.004457         0.496289   \n               2018  ...            0.004474         0.496289   \n               2019  ...            0.004491         0.580893   \n               2020  ...            0.004512         0.499980   \n\n                     GROUNDSURFACEELEVATION_AVG  AVERAGE_YEARLY_PRECIPITATION  \\\nTOWNSHIP_RANGE YEAR                                                             \nT01N R03E      2014                    0.023626                      0.163573   \n               2015                    0.018249                      0.217900   \n               2016                    0.024153                      0.209056   \n               2017                    0.023541                      0.213645   \n               2018                    0.020523                      0.181012   \n...                                         ...                           ...   \nT32S R30E      2016                    0.139007                      0.111564   \n               2017                    0.139007                      0.169284   \n               2018                    0.139007                      0.079747   \n               2019                    0.139007                      0.158678   \n               2020                    0.139007                      0.156231   \n\n                     SHORTAGE_COUNT   GSE_GWE  WELL_COUNT_AGRICULTURE  \\\nTOWNSHIP_RANGE YEAR                                                     \nT01N R03E      2014             0.0  0.043005                0.029412   \n               2015             0.0  0.050637                0.000000   \n               2016             0.0  0.035780                0.029412   \n               2017             0.0  0.033202                0.000000   \n               2018             0.0  0.030798                0.000000   \n...                             ...       ...                     ...   \nT32S R30E      2016             0.0  0.621728                0.000000   \n               2017             0.0  0.527907                0.000000   \n               2018             0.0  0.556283                0.000000   \n               2019             0.0  0.566892                0.000000   \n               2020             0.0  0.555112                0.000000   \n\n                     WELL_COUNT_DOMESTIC  WELL_COUNT_INDUSTRIAL  \\\nTOWNSHIP_RANGE YEAR                                               \nT01N R03E      2014             0.041667                    0.0   \n               2015             0.027778                    0.0   \n               2016             0.055556                    0.0   \n               2017             0.027778                    0.0   \n               2018             0.097222                    0.0   \n...                                  ...                    ...   \nT32S R30E      2016             0.000000                    0.0   \n               2017             0.000000                    0.0   \n               2018             0.000000                    0.0   \n               2019             0.000000                    0.0   \n               2020             0.000000                    0.0   \n\n                     WELL_COUNT_PUBLIC  \nTOWNSHIP_RANGE YEAR                     \nT01N R03E      2014                0.0  \n               2015                0.0  \n               2016                0.0  \n               2017                0.0  \n               2018                0.0  \n...                                ...  \nT32S R30E      2016                0.0  \n               2017                0.0  \n               2018                0.0  \n               2019                0.0  \n               2020                0.0  \n\n[2674 rows x 81 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th>TOTALDRILLDEPTH_AVG</th>\n      <th>WELLYIELD_AVG</th>\n      <th>STATICWATERLEVEL_AVG</th>\n      <th>TOPOFPERFORATEDINTERVAL_AVG</th>\n      <th>BOTTOMOFPERFORATEDINTERVAL_AVG</th>\n      <th>TOTALCOMPLETEDDEPTH_AVG</th>\n      <th>VEGETATION_BLUE_OAK-GRAY_PINE</th>\n      <th>VEGETATION_CALIFORNIA_COAST_LIVE_OAK</th>\n      <th>VEGETATION_CANYON_LIVE_OAK</th>\n      <th>VEGETATION_HARD_CHAPARRAL</th>\n      <th>...</th>\n      <th>POPULATION_DENSITY</th>\n      <th>PCT_OF_CAPACITY</th>\n      <th>GROUNDSURFACEELEVATION_AVG</th>\n      <th>AVERAGE_YEARLY_PRECIPITATION</th>\n      <th>SHORTAGE_COUNT</th>\n      <th>GSE_GWE</th>\n      <th>WELL_COUNT_AGRICULTURE</th>\n      <th>WELL_COUNT_DOMESTIC</th>\n      <th>WELL_COUNT_INDUSTRIAL</th>\n      <th>WELL_COUNT_PUBLIC</th>\n    </tr>\n    <tr>\n      <th>TOWNSHIP_RANGE</th>\n      <th>YEAR</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">T01N R03E</th>\n      <th>2014</th>\n      <td>0.097778</td>\n      <td>0.018246</td>\n      <td>0.037145</td>\n      <td>0.098039</td>\n      <td>0.111111</td>\n      <td>0.105856</td>\n      <td>0.000037</td>\n      <td>0.000137</td>\n      <td>0.000000</td>\n      <td>0.000386</td>\n      <td>...</td>\n      <td>0.252900</td>\n      <td>0.717075</td>\n      <td>0.023626</td>\n      <td>0.163573</td>\n      <td>0.0</td>\n      <td>0.043005</td>\n      <td>0.029412</td>\n      <td>0.041667</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2015</th>\n      <td>0.095238</td>\n      <td>0.021053</td>\n      <td>0.025042</td>\n      <td>0.117647</td>\n      <td>0.080460</td>\n      <td>0.079848</td>\n      <td>0.000037</td>\n      <td>0.000137</td>\n      <td>0.000000</td>\n      <td>0.000386</td>\n      <td>...</td>\n      <td>0.252799</td>\n      <td>0.717075</td>\n      <td>0.018249</td>\n      <td>0.217900</td>\n      <td>0.0</td>\n      <td>0.050637</td>\n      <td>0.000000</td>\n      <td>0.027778</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2016</th>\n      <td>0.114286</td>\n      <td>0.007916</td>\n      <td>0.022398</td>\n      <td>0.152614</td>\n      <td>0.103768</td>\n      <td>0.104880</td>\n      <td>0.000037</td>\n      <td>0.000137</td>\n      <td>0.000000</td>\n      <td>0.000386</td>\n      <td>...</td>\n      <td>0.250621</td>\n      <td>0.717075</td>\n      <td>0.024153</td>\n      <td>0.209056</td>\n      <td>0.0</td>\n      <td>0.035780</td>\n      <td>0.029412</td>\n      <td>0.055556</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2017</th>\n      <td>0.000000</td>\n      <td>0.013684</td>\n      <td>0.030885</td>\n      <td>0.127451</td>\n      <td>0.082375</td>\n      <td>0.081749</td>\n      <td>0.000037</td>\n      <td>0.000137</td>\n      <td>0.000000</td>\n      <td>0.000386</td>\n      <td>...</td>\n      <td>0.254669</td>\n      <td>0.717075</td>\n      <td>0.023541</td>\n      <td>0.213645</td>\n      <td>0.0</td>\n      <td>0.033202</td>\n      <td>0.000000</td>\n      <td>0.027778</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2018</th>\n      <td>0.083873</td>\n      <td>0.002474</td>\n      <td>0.034558</td>\n      <td>0.148257</td>\n      <td>0.093934</td>\n      <td>0.107605</td>\n      <td>0.000037</td>\n      <td>0.000137</td>\n      <td>0.000000</td>\n      <td>0.000386</td>\n      <td>...</td>\n      <td>0.256461</td>\n      <td>0.800728</td>\n      <td>0.020523</td>\n      <td>0.181012</td>\n      <td>0.0</td>\n      <td>0.030798</td>\n      <td>0.000000</td>\n      <td>0.097222</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">T32S R30E</th>\n      <th>2016</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.033178</td>\n      <td>0.000000</td>\n      <td>0.002023</td>\n      <td>0.003535</td>\n      <td>...</td>\n      <td>0.004469</td>\n      <td>0.496289</td>\n      <td>0.139007</td>\n      <td>0.111564</td>\n      <td>0.0</td>\n      <td>0.621728</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2017</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.033178</td>\n      <td>0.000000</td>\n      <td>0.002023</td>\n      <td>0.003535</td>\n      <td>...</td>\n      <td>0.004457</td>\n      <td>0.496289</td>\n      <td>0.139007</td>\n      <td>0.169284</td>\n      <td>0.0</td>\n      <td>0.527907</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2018</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.033178</td>\n      <td>0.000000</td>\n      <td>0.002023</td>\n      <td>0.003535</td>\n      <td>...</td>\n      <td>0.004474</td>\n      <td>0.496289</td>\n      <td>0.139007</td>\n      <td>0.079747</td>\n      <td>0.0</td>\n      <td>0.556283</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2019</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.033178</td>\n      <td>0.000000</td>\n      <td>0.002023</td>\n      <td>0.003535</td>\n      <td>...</td>\n      <td>0.004491</td>\n      <td>0.580893</td>\n      <td>0.139007</td>\n      <td>0.158678</td>\n      <td>0.0</td>\n      <td>0.566892</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2020</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.033178</td>\n      <td>0.000000</td>\n      <td>0.002023</td>\n      <td>0.003535</td>\n      <td>...</td>\n      <td>0.004512</td>\n      <td>0.499980</td>\n      <td>0.139007</td>\n      <td>0.156231</td>\n      <td>0.0</td>\n      <td>0.555112</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>2674 rows × 81 columns</p>\n</div>"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RANDOM_SEED = 42\n",
    "# Load the data from the ETL output files\n",
    "X = read_and_join_output_file()\n",
    "#X[\"WELL_COUNT\"] = X[\"WELL_COUNT_PUBLIC\"] + X[\"WELL_COUNT_AGRICULTURE\"] + X[\"WELL_COUNT_DOMESTIC\"] + X[\"WELL_COUNT_INDUSTRIAL\"]\n",
    "#X.drop(columns=[\"WELL_COUNT_PUBLIC\", \"WELL_COUNT_AGRICULTURE\", \"WELL_COUNT_DOMESTIC\", \"WELL_COUNT_INDUSTRIAL\"], inplace=True)\n",
    "# Split the data into a training and a test set\n",
    "X_train_df, X_test_df, y_train_df, y_test_df = train_test_group_time_split(X, index=[\"TOWNSHIP_RANGE\", \"YEAR\"], group=\"TOWNSHIP_RANGE\", random_seed=RANDOM_SEED)\n",
    "# Create, fit and apply the data imputation pipeline to the training and test sets\n",
    "impute_pipeline, columns = create_transformation_pipelines(X_train_df)\n",
    "X_train_impute = impute_pipeline.fit_transform(X_train_df)\n",
    "X_test_impute = impute_pipeline.transform(X_test_df)\n",
    "# Convert the X_train and X_test back to dataframes\n",
    "X_train_impute_df = pd.DataFrame(X_train_impute, index=X_train_df.index, columns=columns)\n",
    "X_test_impute_df = pd.DataFrame(X_test_impute, index=X_test_df.index, columns=columns)\n",
    "# Keep only the GSE_GWE variable as the outcome variable\n",
    "scaler = MinMaxScaler()\n",
    "y_train = scaler.fit_transform(y_train_df[[\"GSE_GWE\"]])\n",
    "y_test = scaler.transform(y_test_df[[\"GSE_GWE\"]])\n",
    "X_train_impute_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# Change the shape of the input array to (number of Township-Ranges, 7 years (2014-2020), the number of features)\n",
    "X_train = X_train_impute_df.values.reshape(len(X_train_impute_df.index.get_level_values(0).unique()), len(X_train_impute_df.index.get_level_values(1).unique()), X_train_impute_df.shape[1])\n",
    "X_test = X_test_impute_df.values.reshape(len(X_test_impute_df.index.get_level_values(0).unique()), len(X_test_impute_df.index.get_level_values(1).unique()), X_test_impute_df.shape[1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Checking the train, validation and test input (X) datasets sizes:\n",
      "Size of the X_train dataset: (382, 7, 81)\n",
      "Size of the X_test dataset: (96, 7, 81)\n",
      "====================================================================================================\n",
      "Checking the train, validation and test output (y) datasets sizes:\n",
      "Size of the y_train dataset: (382, 1)\n",
      "Size of the y_test dataset: (96, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*100)\n",
    "print(\"Checking the train, validation and test input (X) datasets sizes:\")\n",
    "print(f\"Size of the X_train dataset: {X_train.shape}\")\n",
    "#print(f\"Size of the X_val dataset: {X_val.shape}\")\n",
    "print(f\"Size of the X_test dataset: {X_test.shape}\")\n",
    "print(\"=\"*100)\n",
    "print(\"Checking the train, validation and test output (y) datasets sizes:\")\n",
    "print(f\"Size of the y_train dataset: {y_train.shape}\")\n",
    "#print(f\"Size of the y_val dataset: {y_val_df.shape}\")\n",
    "print(f\"Size of the y_test dataset: {y_test.shape}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "results_df = y_test_df[[\"GSE_GWE\"]].copy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "nb_features = len(X_train_impute_df.columns)\n",
    "\n",
    "hyper_parameters = {\n",
    "    \"validation_split\": 0.05,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"batch_size\": 32,\n",
    "    \"epochs\": 30,\n",
    "    \"lstm_units\": 200,\n",
    "    \"lstm_activation\": \"tanh\",\n",
    "    \"output_activation\": \"linear\",\n",
    "    \"nb_features\": nb_features\n",
    "}\n",
    "\n",
    "adam_optimizer = Adam(learning_rate=hyper_parameters[\"learning_rate\"])\n",
    "rms_optimizer = RMSprop(learning_rate=hyper_parameters[\"learning_rate\"])\n",
    "adamax_optimizer = Adamax(learning_rate=hyper_parameters[\"learning_rate\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "def evaluate_forecast(y_test_inverse, yhat_inverse):\n",
    "    mse_ = keras.metrics.MeanSquaredError()\n",
    "    mae_ = keras.metrics.MeanAbsoluteError()\n",
    "    rmse_ = keras.metrics.RootMeanSquaredError()\n",
    "    mae = mae_(y_test_inverse,yhat_inverse)\n",
    "    print('mae:', mae)\n",
    "    mse = mse_(y_test_inverse,yhat_inverse)\n",
    "    print('mse:', mse)\n",
    "    rmse = rmse_(y_test_inverse,yhat_inverse)\n",
    "    print('rmse:', rmse)\n",
    "    return mae, mse, rmse"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 200)               225600    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 201       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 225,801\n",
      "Trainable params: 225,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1 = Sequential()\n",
    "model1.add(LSTM(hyper_parameters[\"lstm_units\"], activation=hyper_parameters[\"lstm_activation\"], input_shape=(7, nb_features)))\n",
    "model1.add(Dense(1, activation=hyper_parameters[\"output_activation\"]))\n",
    "model1.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/milestone2-california-water-shortage/deeplearning-lstm/e/DEEPLSTM-34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Info (NVML): Not Supported. GPU usage metrics may not be reported. For more information, see https://docs-legacy.neptune.ai/logging-and-managing-experiment-results/logging-experiment-data.html#hardware-consumption \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remember to stop your run once you’ve finished logging your metadata (https://docs.neptune.ai/api-reference/run#.stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.\n",
      "Epoch 1/30\n",
      "12/12 [==============================] - 2s 47ms/step - loss: 0.0222 - val_loss: 0.0065\n",
      "Epoch 2/30\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.0090 - val_loss: 0.0060\n",
      "Epoch 3/30\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0059 - val_loss: 0.0453\n",
      "Epoch 4/30\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.0059 - val_loss: 0.0073\n",
      "Epoch 5/30\n",
      "12/12 [==============================] - 0s 27ms/step - loss: 0.0053 - val_loss: 0.0053\n",
      "Epoch 6/30\n",
      "12/12 [==============================] - 0s 12ms/step - loss: 0.0057 - val_loss: 0.0043\n",
      "Epoch 7/30\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.0038 - val_loss: 0.0180\n",
      "Epoch 8/30\n",
      "12/12 [==============================] - 0s 12ms/step - loss: 0.0044 - val_loss: 0.0047\n",
      "Epoch 9/30\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.0039 - val_loss: 0.0147\n",
      "Epoch 10/30\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.0030 - val_loss: 0.0147\n",
      "Epoch 11/30\n",
      "12/12 [==============================] - 0s 12ms/step - loss: 0.0042 - val_loss: 0.0063\n",
      "Epoch 12/30\n",
      "12/12 [==============================] - 0s 12ms/step - loss: 0.0026 - val_loss: 0.0051\n",
      "Epoch 13/30\n",
      "12/12 [==============================] - 0s 12ms/step - loss: 0.0040 - val_loss: 0.0048\n",
      "Epoch 14/30\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.0036 - val_loss: 0.0037\n",
      "Epoch 15/30\n",
      "12/12 [==============================] - 0s 12ms/step - loss: 0.0030 - val_loss: 0.0058\n",
      "Epoch 16/30\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.0027 - val_loss: 0.0141\n",
      "Epoch 17/30\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.0031 - val_loss: 0.0077\n",
      "Epoch 18/30\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.0025 - val_loss: 0.0039\n",
      "Epoch 19/30\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.0027 - val_loss: 0.0071\n",
      "Epoch 20/30\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.0026 - val_loss: 0.0042\n",
      "Epoch 21/30\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.0027 - val_loss: 0.0035\n",
      "Epoch 22/30\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.0020 - val_loss: 0.0046\n",
      "Epoch 23/30\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.0027 - val_loss: 0.0029\n",
      "Epoch 24/30\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.0027 - val_loss: 0.0034\n",
      "Epoch 25/30\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.0019 - val_loss: 0.0071\n",
      "Epoch 26/30\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.0020 - val_loss: 0.0029\n",
      "Epoch 27/30\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.0025 - val_loss: 0.0023\n",
      "Epoch 28/30\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.0018 - val_loss: 0.0027\n",
      "Epoch 29/30\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.0021 - val_loss: 0.0062\n",
      "Epoch 30/30\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.0020 - val_loss: 0.0033\n",
      "mae: tf.Tensor(34.651073, shape=(), dtype=float32)\n",
      "mse: tf.Tensor(2982.1416, shape=(), dtype=float32)\n",
      "rmse: tf.Tensor(54.60899, shape=(), dtype=float32)\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 187 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 187 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://app.neptune.ai/milestone2-california-water-shortage/deeplearning-lstm/e/DEEPLSTM-34\n"
     ]
    }
   ],
   "source": [
    "# Start experiment\n",
    "run = neptune.init(\n",
    "    project=\"milestone2-california-water-shortage/deeplearning-lstm\",\n",
    "    api_token=neptune_key,\n",
    "    name=\"Basic Model\",\n",
    "    tags=[\"WithDetailedWellCounts\", \"UnidirectionalLSTM\"]\n",
    ")\n",
    "neptune_cbk = NeptuneCallback(run=run, base_namespace='metrics')\n",
    "hyper_parameters[\"optimizer\"] = \"RMSprop\"\n",
    "run['hyper-parameters'] = hyper_parameters\n",
    "\n",
    "model1.compile(loss=\"mse\", optimizer=rms_optimizer)\n",
    "history = model1.fit(X_train, y_train,\n",
    "                     validation_split=hyper_parameters[\"validation_split\"],\n",
    "                     batch_size=hyper_parameters[\"batch_size\"],\n",
    "                     epochs=hyper_parameters[\"epochs\"],\n",
    "                     shuffle=True,\n",
    "                     callbacks=[neptune_cbk])\n",
    "yhat = model1.predict(X_test, verbose=0)\n",
    "yhat_inverse = scaler.inverse_transform(yhat)\n",
    "y_test_inverse = scaler.inverse_transform(y_test)\n",
    "results_df[\"experiment_1_prediction\"] = yhat_inverse\n",
    "mae, mse, rmse = evaluate_forecast(y_test_inverse, yhat_inverse)\n",
    "run[\"eval/mae\"] = mae\n",
    "run[\"eval/mse\"] = mse\n",
    "run[\"eval/rmse\"] = rmse\n",
    "run.stop()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional (Bidirectiona  (None, 400)              451200    \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 30)                12030     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 30)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 31        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 463,261\n",
      "Trainable params: 463,261\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "hyper_parameters[\"dense_units\"] = 30\n",
    "hyper_parameters[\"dense_activation\"] = \"tanh\"\n",
    "hyper_parameters[\"dropout\"] = 0.02\n",
    "\n",
    "model2 = Sequential()\n",
    "model2.add(Bidirectional(LSTM(hyper_parameters[\"lstm_units\"], activation=hyper_parameters[\"lstm_activation\"]), input_shape=(7, nb_features)))\n",
    "model2.add(Dense(hyper_parameters[\"dense_units\"], activation=hyper_parameters[\"dense_activation\"]))\n",
    "model2.add(Dropout(hyper_parameters[\"dropout\"]))\n",
    "model2.add(Dense(1, activation=hyper_parameters[\"output_activation\"]))\n",
    "model2.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/milestone2-california-water-shortage/deeplearning-lstm/e/DEEPLSTM-35\n",
      "Remember to stop your run once you’ve finished logging your metadata (https://docs.neptune.ai/api-reference/run#.stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.\n",
      "Epoch 1/30\n",
      "12/12 [==============================] - 3s 72ms/step - loss: 0.0429 - val_loss: 0.0108\n",
      "Epoch 2/30\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.0113 - val_loss: 0.0045\n",
      "Epoch 3/30\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.0053 - val_loss: 0.0052\n",
      "Epoch 4/30\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.0043 - val_loss: 0.0099\n",
      "Epoch 5/30\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.0034 - val_loss: 0.0073\n",
      "Epoch 6/30\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.0030 - val_loss: 0.0067\n",
      "Epoch 7/30\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.0030 - val_loss: 0.0080\n",
      "Epoch 8/30\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.0028 - val_loss: 0.0061\n",
      "Epoch 9/30\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.0029 - val_loss: 0.0073\n",
      "Epoch 10/30\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.0029 - val_loss: 0.0095\n",
      "Epoch 11/30\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.0028 - val_loss: 0.0042\n",
      "Epoch 12/30\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.0026 - val_loss: 0.0069\n",
      "Epoch 13/30\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.0027 - val_loss: 0.0047\n",
      "Epoch 14/30\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.0028 - val_loss: 0.0082\n",
      "Epoch 15/30\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.0027 - val_loss: 0.0079\n",
      "Epoch 16/30\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.0026 - val_loss: 0.0059\n",
      "Epoch 17/30\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.0023 - val_loss: 0.0053\n",
      "Epoch 18/30\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.0021 - val_loss: 0.0033\n",
      "Epoch 19/30\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.0020 - val_loss: 0.0044\n",
      "Epoch 20/30\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.0023 - val_loss: 0.0047\n",
      "Epoch 21/30\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.0021 - val_loss: 0.0043\n",
      "Epoch 22/30\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.0020 - val_loss: 0.0038\n",
      "Epoch 23/30\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.0019 - val_loss: 0.0039\n",
      "Epoch 24/30\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.0017 - val_loss: 0.0048\n",
      "Epoch 25/30\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.0018 - val_loss: 0.0047\n",
      "Epoch 26/30\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.0018 - val_loss: 0.0047\n",
      "Epoch 27/30\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.0017 - val_loss: 0.0048\n",
      "Epoch 28/30\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.0016 - val_loss: 0.0097\n",
      "Epoch 29/30\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.0020 - val_loss: 0.0047\n",
      "Epoch 30/30\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.0018 - val_loss: 0.0034\n",
      "mae: tf.Tensor(33.174473, shape=(), dtype=float32)\n",
      "mse: tf.Tensor(2935.754, shape=(), dtype=float32)\n",
      "rmse: tf.Tensor(54.1826, shape=(), dtype=float32)\n",
      "mae: tf.Tensor(33.174473, shape=(), dtype=float32)\n",
      "mse: tf.Tensor(2935.754, shape=(), dtype=float32)\n",
      "rmse: tf.Tensor(54.1826, shape=(), dtype=float32)\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 417 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 417 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://app.neptune.ai/milestone2-california-water-shortage/deeplearning-lstm/e/DEEPLSTM-35\n"
     ]
    }
   ],
   "source": [
    "run = neptune.init(\n",
    "    project=\"milestone2-california-water-shortage/deeplearning-lstm\",\n",
    "    api_token=neptune_key,\n",
    "    name=\"Advanced Model 1\",\n",
    "    tags=[\"WithDetailedWellCounts\", \"BidirectionalLSTM\"]\n",
    ")\n",
    "neptune_cbk = NeptuneCallback(run=run, base_namespace='metrics')\n",
    "hyper_parameters[\"optimizer\"] = \"Adam\"\n",
    "run['hyper-parameters'] = hyper_parameters\n",
    "\n",
    "model2.compile(loss=\"mse\", optimizer=adam_optimizer)\n",
    "history = model2.fit(X_train, y_train,\n",
    "                     validation_split=hyper_parameters[\"validation_split\"],\n",
    "                     batch_size=hyper_parameters[\"batch_size\"],\n",
    "                     epochs=hyper_parameters[\"epochs\"],\n",
    "                     shuffle=True,\n",
    "                     callbacks=[neptune_cbk])\n",
    "yhat = model2.predict(X_test, verbose=0)\n",
    "yhat_inverse = scaler.inverse_transform(yhat)\n",
    "y_test_inverse = scaler.inverse_transform(y_test)\n",
    "results_df[\"experiment_2_prediction\"] = yhat_inverse\n",
    "evaluate_forecast(y_test_inverse, yhat_inverse)\n",
    "mae, mse, rmse = evaluate_forecast(y_test_inverse, yhat_inverse)\n",
    "run[\"eval/mae\"] = mae\n",
    "run[\"eval/mse\"] = mse\n",
    "run[\"eval/rmse\"] = rmse\n",
    "run.stop()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/milestone2-california-water-shortage/deeplearning-lstm/e/DEEPLSTM-36\n",
      "Remember to stop your run once you’ve finished logging your metadata (https://docs.neptune.ai/api-reference/run#.stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.\n",
      "Epoch 1/30\n",
      "12/12 [==============================] - 3s 68ms/step - loss: 0.0207 - val_loss: 0.0105\n",
      "Epoch 2/30\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.0051 - val_loss: 0.0031\n",
      "Epoch 3/30\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.0031 - val_loss: 0.0033\n",
      "Epoch 4/30\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.0052 - val_loss: 0.0028\n",
      "Epoch 5/30\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.0027 - val_loss: 0.0170\n",
      "Epoch 6/30\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.0058 - val_loss: 0.0030\n",
      "Epoch 7/30\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.0038 - val_loss: 0.0084\n",
      "Epoch 8/30\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.0044 - val_loss: 0.0095\n",
      "Epoch 9/30\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0025 - val_loss: 0.0055\n",
      "Epoch 10/30\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0044 - val_loss: 0.0031\n",
      "Epoch 11/30\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0036 - val_loss: 0.0058\n",
      "Epoch 12/30\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.0033 - val_loss: 0.0154\n",
      "Epoch 13/30\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0042 - val_loss: 0.0034\n",
      "Epoch 14/30\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0027 - val_loss: 0.0039\n",
      "Epoch 15/30\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0033 - val_loss: 0.0092\n",
      "Epoch 16/30\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.0025 - val_loss: 0.0043\n",
      "Epoch 17/30\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.0034 - val_loss: 0.0137\n",
      "Epoch 18/30\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 19/30\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.0024 - val_loss: 0.0038\n",
      "Epoch 20/30\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.0035 - val_loss: 0.0186\n",
      "Epoch 21/30\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.0032 - val_loss: 0.0052\n",
      "Epoch 22/30\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.0037 - val_loss: 0.0038\n",
      "Epoch 23/30\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.0018 - val_loss: 0.0080\n",
      "Epoch 24/30\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.0036 - val_loss: 0.0039\n",
      "Epoch 25/30\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.0024 - val_loss: 0.0059\n",
      "Epoch 26/30\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.0018 - val_loss: 0.0034\n",
      "Epoch 27/30\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.0027 - val_loss: 0.0151\n",
      "Epoch 28/30\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.0022 - val_loss: 0.0042\n",
      "Epoch 29/30\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.0026 - val_loss: 0.0086\n",
      "Epoch 30/30\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.0022 - val_loss: 0.0057\n",
      "mae: tf.Tensor(38.980274, shape=(), dtype=float32)\n",
      "mse: tf.Tensor(3315.6663, shape=(), dtype=float32)\n",
      "rmse: tf.Tensor(57.58182, shape=(), dtype=float32)\n",
      "mae: tf.Tensor(38.980274, shape=(), dtype=float32)\n",
      "mse: tf.Tensor(3315.6663, shape=(), dtype=float32)\n",
      "rmse: tf.Tensor(57.58182, shape=(), dtype=float32)\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 436 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 436 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://app.neptune.ai/milestone2-california-water-shortage/deeplearning-lstm/e/DEEPLSTM-36\n"
     ]
    }
   ],
   "source": [
    "run = neptune.init(\n",
    "    project=\"milestone2-california-water-shortage/deeplearning-lstm\",\n",
    "    api_token=neptune_key,\n",
    "    name=\"Advanced Model 1\",\n",
    "    tags=[\"WithDetailedWellCounts\", \"BidirectionalLSTM\"]\n",
    ")\n",
    "neptune_cbk = NeptuneCallback(run=run, base_namespace='metrics')\n",
    "hyper_parameters[\"optimizer\"] = \"RMSprop\"\n",
    "run['hyper-parameters'] = hyper_parameters\n",
    "\n",
    "model2.compile(loss=\"mse\", optimizer=rms_optimizer)\n",
    "history = model2.fit(X_train, y_train,\n",
    "                     validation_split=hyper_parameters[\"validation_split\"],\n",
    "                     batch_size=hyper_parameters[\"batch_size\"],\n",
    "                     epochs=hyper_parameters[\"epochs\"],\n",
    "                     shuffle=True,\n",
    "                     callbacks=[neptune_cbk])\n",
    "yhat = model2.predict(X_test, verbose=0)\n",
    "yhat_inverse = scaler.inverse_transform(yhat)\n",
    "y_test_inverse = scaler.inverse_transform(y_test)\n",
    "results_df[\"experiment_3_prediction\"] = yhat_inverse\n",
    "evaluate_forecast(y_test_inverse, yhat_inverse)\n",
    "mae, mse, rmse = evaluate_forecast(y_test_inverse, yhat_inverse)\n",
    "run[\"eval/mae\"] = mae\n",
    "run[\"eval/mse\"] = mse\n",
    "run[\"eval/rmse\"] = rmse\n",
    "run.stop()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/milestone2-california-water-shortage/deeplearning-lstm/e/DEEPLSTM-37\n",
      "Remember to stop your run once you’ve finished logging your metadata (https://docs.neptune.ai/api-reference/run#.stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.\n",
      "Epoch 1/30\n",
      "12/12 [==============================] - 3s 67ms/step - loss: 0.0021 - val_loss: 0.0028\n",
      "Epoch 2/30\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.0013 - val_loss: 0.0033\n",
      "Epoch 3/30\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.0011 - val_loss: 0.0043\n",
      "Epoch 4/30\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 8.6124e-04 - val_loss: 0.0041\n",
      "Epoch 5/30\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 9.5157e-04 - val_loss: 0.0029\n",
      "Epoch 6/30\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 9.2064e-04 - val_loss: 0.0037\n",
      "Epoch 7/30\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 9.0162e-04 - val_loss: 0.0044\n",
      "Epoch 8/30\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 8.7335e-04 - val_loss: 0.0034\n",
      "Epoch 9/30\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 8.0207e-04 - val_loss: 0.0030\n",
      "Epoch 10/30\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 7.8549e-04 - val_loss: 0.0032\n",
      "Epoch 11/30\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 8.1858e-04 - val_loss: 0.0035\n",
      "Epoch 12/30\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 7.9478e-04 - val_loss: 0.0031\n",
      "Epoch 13/30\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 7.1242e-04 - val_loss: 0.0034\n",
      "Epoch 14/30\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 7.3040e-04 - val_loss: 0.0034\n",
      "Epoch 15/30\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 7.7008e-04 - val_loss: 0.0030\n",
      "Epoch 16/30\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 7.1474e-04 - val_loss: 0.0035\n",
      "Epoch 17/30\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 6.9721e-04 - val_loss: 0.0036\n",
      "Epoch 18/30\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 7.0744e-04 - val_loss: 0.0028\n",
      "Epoch 19/30\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 6.8037e-04 - val_loss: 0.0031\n",
      "Epoch 20/30\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 6.6667e-04 - val_loss: 0.0032\n",
      "Epoch 21/30\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 6.9263e-04 - val_loss: 0.0033\n",
      "Epoch 22/30\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 6.3610e-04 - val_loss: 0.0036\n",
      "Epoch 23/30\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 6.6681e-04 - val_loss: 0.0029\n",
      "Epoch 24/30\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 6.7872e-04 - val_loss: 0.0031\n",
      "Epoch 25/30\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 5.9665e-04 - val_loss: 0.0031\n",
      "Epoch 26/30\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 6.3725e-04 - val_loss: 0.0041\n",
      "Epoch 27/30\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 6.9521e-04 - val_loss: 0.0036\n",
      "Epoch 28/30\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 6.3002e-04 - val_loss: 0.0027\n",
      "Epoch 29/30\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 6.4573e-04 - val_loss: 0.0036\n",
      "Epoch 30/30\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 5.5426e-04 - val_loss: 0.0030\n",
      "mae: tf.Tensor(30.849747, shape=(), dtype=float32)\n",
      "mse: tf.Tensor(3099.0996, shape=(), dtype=float32)\n",
      "rmse: tf.Tensor(55.669556, shape=(), dtype=float32)\n",
      "mae: tf.Tensor(30.849747, shape=(), dtype=float32)\n",
      "mse: tf.Tensor(3099.0996, shape=(), dtype=float32)\n",
      "rmse: tf.Tensor(55.669556, shape=(), dtype=float32)\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 415 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 415 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://app.neptune.ai/milestone2-california-water-shortage/deeplearning-lstm/e/DEEPLSTM-37\n"
     ]
    }
   ],
   "source": [
    "run = neptune.init(\n",
    "    project=\"milestone2-california-water-shortage/deeplearning-lstm\",\n",
    "    api_token=neptune_key,\n",
    "    name=\"Advanced Model 1\",\n",
    "    tags=[\"WithDetailedWellCounts\", \"BidirectionalLSTM\"]\n",
    ")\n",
    "neptune_cbk = NeptuneCallback(run=run, base_namespace='metrics')\n",
    "hyper_parameters[\"optimizer\"] = \"Adamx\"\n",
    "run['hyper-parameters'] = hyper_parameters\n",
    "\n",
    "model2.compile(loss=\"mse\", optimizer=adamax_optimizer)\n",
    "history = model2.fit(X_train, y_train,\n",
    "                     validation_split=hyper_parameters[\"validation_split\"],\n",
    "                     batch_size=hyper_parameters[\"batch_size\"],\n",
    "                     epochs=hyper_parameters[\"epochs\"],\n",
    "                     shuffle=True,\n",
    "                     callbacks=[neptune_cbk])\n",
    "yhat = model2.predict(X_test, verbose=0)\n",
    "yhat_inverse = scaler.inverse_transform(yhat)\n",
    "y_test_inverse = scaler.inverse_transform(y_test)\n",
    "results_df[\"experiment_4_prediction\"] = yhat_inverse\n",
    "evaluate_forecast(y_test_inverse, yhat_inverse)\n",
    "mae, mse, rmse = evaluate_forecast(y_test_inverse, yhat_inverse)\n",
    "run[\"eval/mae\"] = mae\n",
    "run[\"eval/mse\"] = mse\n",
    "run[\"eval/rmse\"] = rmse\n",
    "run.stop()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "                        GSE_GWE  experiment_1_prediction  \\\nTOWNSHIP_RANGE YEAR                                        \nT01N R02E      2021   53.193636                66.969551   \nT01N R11E      2021  107.955000               149.968704   \nT01S R03E      2021   24.494538                44.015427   \nT01S R07E      2021   38.644000                60.856419   \nT01S R10E      2021  113.651250               121.789856   \n...                         ...                      ...   \nT31S R26E      2021  173.915909               208.498795   \nT31S R31E      2021  403.900000               374.315643   \nT32S R22E      2021  160.340000               219.454758   \nT32S R25E      2021  190.120000               213.570786   \nT32S R26E      2021  220.866667               222.432190   \n\n                     experiment_2_prediction  experiment_3_prediction  \\\nTOWNSHIP_RANGE YEAR                                                     \nT01N R02E      2021                62.852299                60.793201   \nT01N R11E      2021               139.532776               173.708160   \nT01S R03E      2021                27.803671                59.542545   \nT01S R07E      2021                59.966618                70.209099   \nT01S R10E      2021                98.660057               145.644806   \n...                                      ...                      ...   \nT31S R26E      2021               211.421799               223.918640   \nT31S R31E      2021               262.225342               355.519470   \nT32S R22E      2021               224.454468               271.028564   \nT32S R25E      2021               187.799042               207.387421   \nT32S R26E      2021               203.618607               229.686935   \n\n                     experiment_4_prediction  \nTOWNSHIP_RANGE YEAR                           \nT01N R02E      2021                63.680000  \nT01N R11E      2021               148.508972  \nT01S R03E      2021                46.503307  \nT01S R07E      2021                48.184971  \nT01S R10E      2021               106.338142  \n...                                      ...  \nT31S R26E      2021               237.329681  \nT31S R31E      2021               367.976868  \nT32S R22E      2021               219.436127  \nT32S R25E      2021               217.892975  \nT32S R26E      2021               207.271332  \n\n[96 rows x 5 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th>GSE_GWE</th>\n      <th>experiment_1_prediction</th>\n      <th>experiment_2_prediction</th>\n      <th>experiment_3_prediction</th>\n      <th>experiment_4_prediction</th>\n    </tr>\n    <tr>\n      <th>TOWNSHIP_RANGE</th>\n      <th>YEAR</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>T01N R02E</th>\n      <th>2021</th>\n      <td>53.193636</td>\n      <td>66.969551</td>\n      <td>62.852299</td>\n      <td>60.793201</td>\n      <td>63.680000</td>\n    </tr>\n    <tr>\n      <th>T01N R11E</th>\n      <th>2021</th>\n      <td>107.955000</td>\n      <td>149.968704</td>\n      <td>139.532776</td>\n      <td>173.708160</td>\n      <td>148.508972</td>\n    </tr>\n    <tr>\n      <th>T01S R03E</th>\n      <th>2021</th>\n      <td>24.494538</td>\n      <td>44.015427</td>\n      <td>27.803671</td>\n      <td>59.542545</td>\n      <td>46.503307</td>\n    </tr>\n    <tr>\n      <th>T01S R07E</th>\n      <th>2021</th>\n      <td>38.644000</td>\n      <td>60.856419</td>\n      <td>59.966618</td>\n      <td>70.209099</td>\n      <td>48.184971</td>\n    </tr>\n    <tr>\n      <th>T01S R10E</th>\n      <th>2021</th>\n      <td>113.651250</td>\n      <td>121.789856</td>\n      <td>98.660057</td>\n      <td>145.644806</td>\n      <td>106.338142</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>T31S R26E</th>\n      <th>2021</th>\n      <td>173.915909</td>\n      <td>208.498795</td>\n      <td>211.421799</td>\n      <td>223.918640</td>\n      <td>237.329681</td>\n    </tr>\n    <tr>\n      <th>T31S R31E</th>\n      <th>2021</th>\n      <td>403.900000</td>\n      <td>374.315643</td>\n      <td>262.225342</td>\n      <td>355.519470</td>\n      <td>367.976868</td>\n    </tr>\n    <tr>\n      <th>T32S R22E</th>\n      <th>2021</th>\n      <td>160.340000</td>\n      <td>219.454758</td>\n      <td>224.454468</td>\n      <td>271.028564</td>\n      <td>219.436127</td>\n    </tr>\n    <tr>\n      <th>T32S R25E</th>\n      <th>2021</th>\n      <td>190.120000</td>\n      <td>213.570786</td>\n      <td>187.799042</td>\n      <td>207.387421</td>\n      <td>217.892975</td>\n    </tr>\n    <tr>\n      <th>T32S R26E</th>\n      <th>2021</th>\n      <td>220.866667</td>\n      <td>222.432190</td>\n      <td>203.618607</td>\n      <td>229.686935</td>\n      <td>207.271332</td>\n    </tr>\n  </tbody>\n</table>\n<p>96 rows × 5 columns</p>\n</div>"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}