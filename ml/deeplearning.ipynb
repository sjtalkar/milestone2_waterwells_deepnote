{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Training, Testing and Predicting With an LSTM Model\n",
    "In the notebook `ml/deeplearning_tuning.ipynb` we performed a Bayesian hyperparameter tuning to find the best hyperparameters for 3 potential LSTM models architectures.\n",
    "In this notebook we:\n",
    "1. train each of the 3 LSTM architectures with their best hyperparameters\n",
    "2. tested their results on the test dataset to select the best model\n",
    "3. analyze the sensibility of the best model to hyperparameters\n",
    "4. use the best model to predict the groundwater depth for the year 2022.\n",
    "## Multiple Multivariate Time Series Predictions with LSTM\n",
    "The dataset is made of 478 Township-Ranges, each containing a multivariate (80 features) time series (data between 2014 to 2021). This dataset can thus be seen as a 3 dimensional dataset of\n",
    "$478\\ TownshipRanges\\ *\\ 8\\ time stamps\\ *\\ 80\\ features$\n",
    "The objective is to predict the 2022 target value of `GSE_GWE` (Ground Surface Elevation to Groundwater Water Elevation - Depth to groundwater elevation in feet below ground surface) for each Township-Range.\n",
    "\n",
    "LSTMs are used for time series and NLP because they are both sequential data and depend on previous states.\n",
    "The future prediction *Y(t+n+1)* depends not only on the last state *X1(t+n), ..., Y(t+n)*, not only on past values of the feature *Y(t+1), ..., Y(t+n)*, but on the entire past states sequence.\n",
    "\n",
    "![Multi-Variate Multi TImes-Series Predictions with LSTM - Training and Prediction](../doc/images/lstm_inputs_outputs.jpg)\n",
    "\n",
    "During training and predictions:\n",
    "* Township-Ranges are passed into the model one by one\n",
    "* each cell in the LSM neural network receives a Township-Range state for a specific year (the state of the Township-Range at a specific position in the series)\n",
    "* each state (year) in the series is represented by a multidimensional vector of all 80 features (including the target feature Y `GSE_GWE`)\n",
    "\n",
    "The output is the Township-Ranges next year's value for the specific feature Y `GSE_GWE`. The model is trained on 2014-2020 (7 years) data to predict 2021.\n",
    "During inference the last 7 years (2015-2021) of data are passed as input to predict the 2022 value.\n",
    "\n",
    "![Multi-Variate Multi TImes-Series Predictions with LSTM - Cells Inputs](../doc/images/lstm_table_to_cells.jpg)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "cell_id": "0ecc9d30-6818-4805-89ba-0fc40048f951",
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 1821.609375
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "cell_id": "00001-45ec143e-17c1-4779-a233-9d45db5638da",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "18561438",
    "execution_start": 1662892797892,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 94
   },
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    },
    "cell_id": "00002-23c9276e-61cc-4602-9f7d-f63ae60e4497",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "bc5de647",
    "execution_start": 1662892797893,
    "execution_millis": 5470,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 464.75
   },
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import tensorflow\n",
    "\n",
    "from sklearn import set_config\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, TimeDistributed, RepeatVector\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from lib.township_range import TownshipRanges\n",
    "from lib.read_data import read_and_join_output_file\n",
    "from lib.deeplearning import get_train_test_datasets, get_sets_shapes, get_data_for_prediction, evaluate_forecast, combine_all_target_years, get_year_to_year_differences\n",
    "from lib.viz import view_trs_side_by_side, draw_hyperparameters_distribution"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "cell_id": "00003-893eb8de-b2f4-4a5d-996a-6607604022f8",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "65465ae4",
    "execution_start": 1662892803365,
    "execution_millis": 1,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 130
   },
   "source": [
    "RANDOM_SEED = 31\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tensorflow.random.set_seed(RANDOM_SEED)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "cell_id": "00004-298dfc55-4c61-44a9-a815-33f7a7010061",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "2a96f01a",
    "execution_start": 1662892803368,
    "execution_millis": 45,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 171.75
   },
   "source": [
    "print(\"Num GPUs Available: \", len(tensorflow.config.list_physical_devices('GPU')))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preparing the Dataset\n",
    "### The Train-Test Split\n",
    "To fit our dataset and objective, as well as LSTM neural networks architecture we perform the train test split as follows:\n",
    "* Training and Test sets will be split by Township-Ranges. I.e., some Township-Ranges will have all their 2014-2021 data points in the training set, some others will be in the test set.\n",
    "* The model will be trained based on the 2014-2020 data for all features - including the target feature - with the training target being the 2021 value of the target feature.\n",
    "\n",
    "With such a method, unlike a simple time series forecasting where the target feature is forecasted only based on its past value, we allow past value of other features (in our case cultivated crops, precipitations, population density, number of wells drilled) to influence the future value of the target feature.\n",
    "\n",
    "![Train-Test Split](../doc/images/lstm-train-test-split.jpg)\n",
    "\n",
    "We do not create a validation dataset as we use Keras internal cross-validation mechanism to shuffle the data points (i.e., the Township-Ranges) and keep some for the validation at each training epoch.\n",
    "### Data Imputation and Scaling\n",
    "Missing data imputation for a Township-Range is performed only using the existing data of that Township-Range (not the data of all Township-Ranges). For example:\n",
    "* a *fill forward* approach is used for many fields like crops, vegetation and soils. The percentage of land use per crop in 2014 in a Township-Range is imputed into the missing year 2015 for that particular Township-Range.\n",
    "* for fields like `PCT_OF_CAPACITY` (the capacity percentage of water reservoir), missing values in a Township-Range are filled using the min, mean, median or max values of that particular Township-Range. In this case the *fit* method of our custom impute pipeline does nothing. Not only the impute pipeline does not need to learn values from other Township-Ranges data points to impute missing values in a Township-Range, if it does it will not be able to impute data in the test set as we impute by Township-Range and the Township-Ranges in the test set are not seen when *fitting* the impute pipeline. The *transform* method, then simply fills the missing values in a Township-Range based on past values of that Township range. This way, we can split the train and test sets by Township-Range and impute missing value without any data leakage as the impute pipeline does not learn anything from the Township-Ranges in the test set.\n",
    "\n",
    "We use a MinMax scaler to scale all values between 0 and 1 for the neural network, except for the vegetation, soils and crops datasets which are already scaled between 0 and 1.\n",
    "\n",
    "It should be noted that we do not need to do any data imputation on the training and test sets *y* target feature since it does not have any missing data point."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "cell_id": "00005-bfcadfef-a8db-49d0-b935-b119372afd45",
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 1651.828125
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "cell_id": "00006-7869daf3-5d23-438c-adb9-3856df68ecbc",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "d6c52b72",
    "execution_start": 1662892803413,
    "execution_millis": 1312,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 470
   },
   "source": [
    "test_size=0.15\n",
    "target_variable=\"GSE_GWE\"\n",
    "# Load the data from the ETL output files\n",
    "X = read_and_join_output_file()\n",
    "X.drop([\"SHORTAGE_COUNT\"], inplace=True, axis=1)\n",
    "# Split the input pandas Dataframe into training and test datasets, applies the impute pipeline\n",
    "# transformation and reshapes the datasets to 3D (samples, time, features) numpy arrays\n",
    "X_train, X_test, y_train, y_test, impute_pipeline, impute_columns, target_scaler = get_train_test_datasets(X, target_variable=target_variable,\n",
    "                                                                                           test_size=test_size, random_seed=RANDOM_SEED, save_to_file=True)\n",
    "model_predictions_df = pd.DataFrame(y_test, columns=[target_variable])\n",
    "model_scores_df = pd.DataFrame(columns=[\"mae\", \"mse\", \"rmse\"])\n",
    "nb_features = X_train.shape[-1]\n",
    "get_sets_shapes(X_train, X_test)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "cell_id": "00007-37992655-66f5-40e3-b123-60f647b6b035",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "d53e1c9f",
    "execution_start": 1662892804076,
    "execution_millis": 887,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 249.390625,
    "deepnote_output_heights": [
     139.390625
    ]
   },
   "source": [
    "set_config(display=\"diagram\")\n",
    "display(impute_pipeline)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training Different Models\n",
    "We tried 3 different LSTM models:\n",
    "* A simple model made of a single *LSTM* layer and an output *Dense* layer\n",
    "* A model made of a *LSTM* layer followed by a *Dense* and *Dropout* layers before the output layer\n",
    "* An Encoder-Decoder model made of 2 *LSTM* layers followed by a *Dense* and *Dropout* layers\n",
    "\n",
    "![LSTM Model Architectures](../doc/images/lstm_architectures.jpg)\n",
    "\n",
    "\n",
    "Encoder-decoder architectures are more common for sequence-to-sequence learning e.g., when forecasting the next 3 days (output sequence of length 3) based on the past year data (input sequence of length 365). In our case we only predict data for 1 time step in the feature. The output sequence being of length 1 this architecture might seem superfluous but has been tested anyway. This architecture was inspired by the Encoder-Decoder architecture in this article: *[CNN-LSTM-Based Models for Multiple Parallel Input and Multi-Step Forecast](https://towardsdatascience.com/cnn-lstm-based-models-for-multiple-parallel-input-and-multi-step-forecast-6fe2172f7668)*.\n",
    "\n",
    "As such models are made for sequence to sequence learning and forecasting, the output of such a model is different from the previous ones. It has an output of size *[samples, forecasting sequence length, target features]*. In our case the forecasting sequence length and number of target features are both 1.\n",
    "### Training Model 1 - Simple LSTM Model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "cell_id": "00008-8dbefa1f-97eb-49d2-8b5b-0723848dec3b",
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 1215.984375
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "cell_id": "00009-a4225230-65d3-4f60-9f3f-8a943a85fdf2",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "572e554a",
    "execution_start": 1662892804200,
    "execution_millis": 768,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 614.8125
   },
   "source": [
    "m1_hyper_parameters = {\n",
    "    \"lstm_units\": 160,\n",
    "    \"lstm_activation\": \"sigmoid\",\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"validation_split\": 0.1,\n",
    "    \"batch_size\": 128,\n",
    "    \"epochs\": 270,\n",
    "}\n",
    "\n",
    "model1 = Sequential()\n",
    "model1.add(LSTM(m1_hyper_parameters[\"lstm_units\"], activation=m1_hyper_parameters[\"lstm_activation\"], input_shape=(7, nb_features)))\n",
    "model1.add(Dense(1, activation=\"linear\"))\n",
    "model1.summary()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "cell_id": "00010-e719ffb9-4344-42b5-9ec9-6f4411fddf28",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "4b5f6b50",
    "execution_start": 1662892804352,
    "execution_millis": 49779,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 863
   },
   "source": [
    "model1.compile(loss=\"mse\", optimizer=Adam(learning_rate=m1_hyper_parameters[\"learning_rate\"]), metrics=[keras.metrics.RootMeanSquaredError()])\n",
    "model1.fit(X_train, y_train,\n",
    "           validation_split=m1_hyper_parameters[\"validation_split\"],\n",
    "           batch_size=m1_hyper_parameters[\"batch_size\"],\n",
    "           epochs=m1_hyper_parameters[\"epochs\"],\n",
    "           shuffle=True)\n",
    "yhat = model1.predict(X_test, verbose=0)\n",
    "yhat_inverse = target_scaler.inverse_transform(yhat)\n",
    "model_predictions_df[\"model_1_prediction\"] = yhat_inverse\n",
    "model_scores_df.loc[\"model 1\"] = evaluate_forecast(y_test, yhat_inverse)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training Model 2 - LSTM + Dense Layer Model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "cell_id": "00011-e6b1dd65-7776-46ca-8edc-082f8ffeeb5a",
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 62
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "cell_id": "00012-783999f3-4a70-4d8d-80df-10b51c0144a5",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "bea59a1b",
    "execution_start": 1662892854130,
    "execution_millis": 167,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 741.1875
   },
   "source": [
    "m2_hyper_parameters = {\n",
    "    \"lstm_units\": 100,\n",
    "    \"lstm_activation\": \"sigmoid\",\n",
    "    \"dense_units\": 11,\n",
    "    \"dense_activation\": \"tanh\",\n",
    "    \"dropout\": 0.1,\n",
    "    \"learning_rate\": 0.0001,\n",
    "    \"validation_split\": 0.1,\n",
    "    \"batch_size\": 32,\n",
    "    \"epochs\": 200,\n",
    "}\n",
    "\n",
    "model2 = Sequential()\n",
    "model2.add(LSTM(m2_hyper_parameters[\"lstm_units\"], activation=m2_hyper_parameters[\"lstm_activation\"], input_shape=(7, nb_features)))\n",
    "model2.add(Dense(m2_hyper_parameters[\"dense_units\"], activation=m2_hyper_parameters[\"dense_activation\"]))\n",
    "model2.add(Dropout(m2_hyper_parameters[\"dropout\"]))\n",
    "model2.add(Dense(1, activation=\"linear\"))\n",
    "model2.summary()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "cell_id": "00013-743dc426-12e6-4e71-b9cc-197f88604697",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "42ac0cf9",
    "execution_start": 1662892854297,
    "execution_millis": 53927,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 863
   },
   "source": [
    "model2.compile(loss=\"mse\", optimizer=Adam(learning_rate=m2_hyper_parameters[\"learning_rate\"]), metrics=[keras.metrics.RootMeanSquaredError()])\n",
    "model2.fit(X_train, y_train,\n",
    "           validation_split=m2_hyper_parameters[\"validation_split\"],\n",
    "           batch_size=m2_hyper_parameters[\"batch_size\"],\n",
    "           epochs=m2_hyper_parameters[\"epochs\"],\n",
    "           shuffle=True)\n",
    "yhat = model2.predict(X_test, verbose=0)\n",
    "yhat_inverse = target_scaler.inverse_transform(yhat)\n",
    "model_predictions_df[\"model_2_prediction\"] = yhat_inverse\n",
    "model_scores_df.loc[\"model 2\"] = evaluate_forecast(y_test, yhat_inverse)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training Model 3 - Encoder-Decoder LSTM Model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "cell_id": "00014-9cd3a6e9-50d1-4229-9ac9-c9fdc9b259ba",
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 62
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "cell_id": "00015-20135aa5-3f26-4c22-80b9-7e4f15db7b95",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "4eda919f",
    "execution_start": 1662892908226,
    "execution_millis": 259,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 952.3125
   },
   "source": [
    "m3_hyper_parameters = {\n",
    "    \"lstm_units\": 300,\n",
    "    \"lstm_activation\": \"sigmoid\",\n",
    "    \"2nd_lstm_units\": 140,\n",
    "    \"2nd_lstm_activation\": \"sigmoid\",\n",
    "    \"dense_units\": 21,\n",
    "    \"dense_activation\": \"tanh\",\n",
    "    \"dropout\": 0.2,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"validation_split\": 0.1,\n",
    "    \"batch_size\": 32,\n",
    "    \"epochs\": 200,\n",
    "}\n",
    "\n",
    "\n",
    "model3 = Sequential()\n",
    "model3.add(LSTM(m3_hyper_parameters[\"lstm_units\"], activation=m3_hyper_parameters[\"lstm_activation\"], input_shape=(7, nb_features)))\n",
    "model3.add(RepeatVector(1))\n",
    "model3.add(LSTM(m3_hyper_parameters[\"2nd_lstm_units\"], activation=m3_hyper_parameters[\"lstm_activation\"], return_sequences=True))\n",
    "model3.add(TimeDistributed(Dense(m3_hyper_parameters[\"dense_units\"], activation=m3_hyper_parameters[\"dense_activation\"])))\n",
    "model3.add(Dropout(m3_hyper_parameters[\"dropout\"]))\n",
    "model3.add(Dense(1, activation=\"linear\"))\n",
    "model3.summary()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "cell_id": "00016-304bc0a1-68ca-49cc-a5de-eb6b891fe1c5",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "cf84507f",
    "execution_start": 1662892908533,
    "execution_millis": 154393,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 881
   },
   "source": [
    "y_train_3d =  y_train[..., np.newaxis]\n",
    "model3.compile(loss=\"mse\", optimizer=Adam(learning_rate=m3_hyper_parameters[\"learning_rate\"]), metrics=[keras.metrics.RootMeanSquaredError()])\n",
    "model3.fit(X_train, y_train_3d,\n",
    "           validation_split=m3_hyper_parameters[\"validation_split\"],\n",
    "           batch_size=m3_hyper_parameters[\"batch_size\"],\n",
    "           epochs=m3_hyper_parameters[\"epochs\"],\n",
    "           shuffle=True)\n",
    "yhat = model3.predict(X_test, verbose=0)\n",
    "yhat_inverse = target_scaler.inverse_transform(yhat.squeeze(2))\n",
    "model_predictions_df[\"model_3_prediction\"] = yhat_inverse\n",
    "model_scores_df.loc[\"model 3\"] = evaluate_forecast(y_test, yhat_inverse)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Comparing the Different Models\n",
    "### Comparing the Model Scores"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "cell_id": "00017-9c7865d2-476b-4a26-90de-be34a071ed60",
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 118
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "cell_id": "00018-d8c65b52-4048-461d-aef6-520b46a190e1",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "693c61b9",
    "execution_start": 1662893062931,
    "execution_millis": 15,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 308
   },
   "source": [
    "model_scores_df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Comparing the Model Predictions on the Test Dataset\n",
    "Here we are comparing the target variable values for the year 2021 for the Township-Ranges in the test set compared to the prediction made by each model based on the 2014-2020 data for the Township-Ranges in the test set."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "cell_id": "00019-8ef0db79-bb9b-4acc-a823-94f42f9fdfaf",
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 122.78125
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "cell_id": "00020-170ccca4-1d89-43af-9cae-b454aed41824",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "9d40daa8",
    "execution_start": 1662893062984,
    "execution_millis": 20,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 595
   },
   "source": [
    "model_predictions_df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Based on the model scores it turns out that the simplest of the three LSTM models is the one having the best scores.\n",
    "\n",
    "However, considering all the measurements between 2014 and 2022, the `GSE_GWE` (Ground Surface Elevation to Groundwater Water Elevation - Depth to groundwater elevation in feet below ground surface) target value has a\n",
    "* median of 137.09 (~41.7 meters)\n",
    "* mean value of 167.37 feet (~50.9 meters)\n",
    "* min value of 0.5 feet (0 meters)\n",
    "* max value of 727.5 feet (221.6 meters)\n",
    "\n",
    "A mean average error of 23.66 feet (7.2 meters), and root mean square error of 34.82 feet (10.6 meters) in the prediction is fairly large. Even the best model does not seem to be accurate enough to be useful.\n",
    "\n",
    "We save the best model anyway to perform predictions and analyze the results. Refer to the notebook `/ml/deeplearning_results.ipynb` for the analysis of the 2022 predictions results."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "cell_id": "00021-6b002d14-1702-4d6f-b828-8901cadadfb1",
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 341.296875
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "cell_id": "00022-ab071fcd-f199-4609-a30e-c36d501cb8ae",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "cf9d7c78",
    "execution_start": 1662893063007,
    "execution_millis": 1453,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 323.1875
   },
   "source": [
    "model_dir = \"../assets/models/\"\n",
    "keras_model_dir = os.path.join(model_dir, \"keras_lstm_model\")\n",
    "os.makedirs(keras_model_dir, exist_ok=True)\n",
    "# Save the Keras Model\n",
    "model1.save(keras_model_dir)\n",
    "# Save the data imputation pipeline and target min-max scaler\n",
    "pipeline_data = {\n",
    "    \"impute_pipeline\": impute_pipeline,\n",
    "    \"impute_columns\": impute_columns,\n",
    "    \"target_scaler\": target_scaler\n",
    "}\n",
    "with open(os.path.join(model_dir, \"lstm_model_pipeline.pkl\"), \"wb\") as file:\n",
    "    pickle.dump(pipeline_data, file)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Sensitivity Analysis\n",
    "We perform here an analysis of the best model's sensitivity to the following hyperparamters:\n",
    "* the optimizer used (e.g. Adam RMSprop, Adagrad)\n",
    "* the training validation datasets split\n",
    "* the number of lstm units\n",
    "* the learning rate\n",
    "* the batch size\n",
    "* the number of training epochs\n",
    "\n",
    "To perform this analysis, we trained 33,345 LSTM models for all possible combinations (within the selected ranges of values) of those 6 hyperparameters on the best model, and recorded for each model, the Root Mean Square Error (RMSE) on the test set.\n",
    "The results are stored in a CSV file available in the ../assets/tuning folder.\n",
    "\n",
    "The below visualization displays for each hyperparameter value, the distribution of the RMSE, and the mean of RMSE (using the color), for all models trained with that hyperparameter value. This allows us to show if a specific hyperparameter tends to lead to lower or higher RMSE and to compare the distribution between two values of the same hyperparameters."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "cell_id": "00023-e00df511-40bd-4af0-a656-131bd9752e7a",
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 434.078125
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "cell_id": "00024-6a5212a7-cc1f-47bd-aabe-ad36780f86a3",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "cb4f37a6",
    "execution_start": 1662893064463,
    "execution_millis": 647,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 775,
    "deepnote_output_heights": [
     606
    ]
   },
   "source": [
    "hpt_df = pd.read_csv(r\"../assets/tuning/hpt_results.csv\")\n",
    "# We can discard some hyperparameter data to reduce the size of the visualization and improve readability\n",
    "#hpt_df = hpt_df[hpt_df[\"epochs\"].isin(range(50, 310, 40))]\n",
    "#hpt_df = hpt_df[hpt_df[\"lstm_units\"].isin(range(10, 200, 30))]\n",
    "draw_hyperparameters_distribution(hpt_df, [\"optimizer\", \"validation_split\", \"learning_rate\", \"batch_size\", \"epochs\", \"lstm_units\"])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Looking at this visualization, we can see, with some surprise, that the hyperparameters which seem to have the biggest impact on the model performance have little to do with the model architecture itself (the number of LSTM units) but with how the model is trained.\n",
    "* The choice of the optimizer seems to have the largest impact on the model performance, with both the mean and distribution of the RMSE for all models trained with an `Adagrad` optimizer being really bad.\n",
    "* The training-validation percentage split seems to have little impact. Assign 5 or 10 % of the training data to validation seems to affect very little the performance of the trained models. But above 10% the performance starts to deteriorate.\n",
    "* The bigger the learning rate, the better the models performs in terms or RMSE. The distribution of all models RMSE shows that with a learning rate of 0.1, most models have low RMSE around 60, while with a learning rate of 0.0001, the distribution of the RMSE of the models is more evenly spread across all values\n",
    "* On the other hand, the smaller the batch size, the more models have a low RMSE.\n",
    "* Although there is less of a difference if we compare similar values (e.g. 50 and 70 epochs or 270 and 290' epochs), we still see clearly that the bigger the number training epochs the more there are trained models with a low RMSE.\n",
    "* The number of LSTM units, impacting the number of neurons in the LSTM model, seems to have less impact on the performance of the RMSE. The distribution of all models RMSE doesn't change much. But we see that the mean RMSE does slightly drop when the number of LSTM units increases.\n",
    "## Predicting 2022\n",
    "Even though our best model has a too large error to be useful, we can try as an exercise, to predict the 2022 target variable for all the Township-Ranges.\n",
    "\n",
    "The model was trained to predict the 2021 data based on the previous 7 years of data 2014 to 2020. To predict 2022 we thus need to pass the previous 7 years of data (2015-2021). To do so:\n",
    "1. We use our impute pipeline trained on the training dataset to impute values on the entire dataset and normalize the data\n",
    "2. We drop the 2014 data points\n",
    "3. We reshape the dataset as a 3 dimensional numpy array in the form of [all Township-Ranges, 2015-2021, 80 features]\n",
    "\n",
    "Once we predict the 2022 values of the target variable, we extract the 2021 values from the original dataset to compare the 2021 values with the predicted 2022 values."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "cell_id": "00025-dea28470-c4e6-4bd4-a460-1158f23f5e87",
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 736.765625
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "cell_id": "00026-48321042-52a8-46ca-b4f3-a453861115dc",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "80297276",
    "execution_start": 1662893064806,
    "execution_millis": 656,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 633.1875,
    "deepnote_output_heights": [
     410.1875
    ]
   },
   "source": [
    "# Predict the 2022 values for all Township-Ranges for the target variable based on 2015-2021 data\n",
    "X_2015_to_2021 = get_data_for_prediction(X, impute_pipeline, impute_columns)\n",
    "yhat_2022 = model1.predict(X_2015_to_2021, verbose=0)\n",
    "yhat_inverse_2022 = target_scaler.inverse_transform(yhat_2022)\n",
    "predictions_2022_df = pd.DataFrame(yhat_inverse_2022, index=X.index.get_level_values(0).unique(), columns=[target_variable])\n",
    "# Add the 2022 values of the target variable to the existing ones\n",
    "all_years_df = combine_all_target_years(X, target_variable, predictions_2022_df)\n",
    "all_years_df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "cell_id": "00027-9d12b0ae-369a-4731-8bef-dd99f10b9e21",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "b216e253",
    "execution_start": 1662893065445,
    "execution_millis": 11702,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 739,
    "deepnote_output_heights": [
     606
    ]
   },
   "source": [
    "township_range = TownshipRanges()\n",
    "all_years_map_df = pd.merge(township_range.sjv_township_range_df, all_years_df.reset_index(), how=\"left\", on=[\"TOWNSHIP_RANGE\", ])\n",
    "view_trs_side_by_side(all_years_map_df, feature=\"YEAR\", value=\"GSE_GWE\", title=\"San Joaquin Valley GSE_GWE with 2022 predictions\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "When we look at the 2022 predictions of the `GSE_GWE` compared to the actual 2014-2021 the predictions are fairly consistent. Areas with high `GSE_GWE` values (i.e., deep ground to water depth) remain the same, and area with low `GSE_GWE` remain the same. However, if the model follows the past year trend, even with a high RMSE of 34.82 feet (10.6 meters), areas with high `GSE_GWE` will remain areas of high `GSE_GWE`. Comparing the past years `GSE_GWE` measurement values with the 2022 predictions is thus only partly informative about the quality of the prediction.\n",
    "\n",
    "We thus try to also compare the year-to-year *difference* in the `GSE_GWE` from 2014 to 2021 and between our 2022 predictions and the 2021 values."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "cell_id": "00028-cec6234b-da6a-4d45-ae19-3a5a798568b8",
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 200.734375
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "cell_id": "00029-166e1aed-dc54-46bb-a5d3-5bab6109e14d",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "95f3ba13",
    "execution_start": 1662893076503,
    "execution_millis": 645,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 630
   },
   "source": [
    "yty_difference_df = get_year_to_year_differences(X, target_variable, predictions_2022_df)\n",
    "yty_difference_df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "cell_id": "00030-6cd7b0ca-adae-4912-a848-d2be91d200e2",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "97bc6ad",
    "execution_start": 1662893076504,
    "execution_millis": 4049,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 721,
    "deepnote_output_heights": [
     606
    ]
   },
   "source": [
    "difference_df = pd.merge(township_range.sjv_township_range_df, pd.melt(yty_difference_df.reset_index(), id_vars=[\"TOWNSHIP_RANGE\"], var_name=\"YEAR\", value_name=\"GSE_GWE_DIFFERENCE\"), how=\"left\", on=[\"TOWNSHIP_RANGE\", ])\n",
    "view_trs_side_by_side(difference_df, feature=\"YEAR\", value=\"GSE_GWE_DIFFERENCE\", title=\"San Joaquin Valley GSE_GWE year-to-year variations from 2014 until 2022 predictions\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "cell_id": "00031-0a4ec1da-d826-416e-8898-d93e09e6e55f",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "3e69990f",
    "execution_start": 1662893080445,
    "execution_millis": 110,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 530
   },
   "source": [
    "yty_difference_df.describe()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Looking at the above table, here too, the difference between the 2022 predictions and 2021 measurements of `GSE_GWE` remains consistent with the year-to-year difference from previous years. Despite the RMSE score being bad, the year-to-year variation of `GSE_GWE` remains within acceptable range.\n",
    "## Conclusion\n",
    "Using a simple LSTM neural network to make next year predictions based on the past 7 years of data, we are able to achieve a more accurate prediction on the test set with an RMSE of 34.82 feet (10.6 meters) compared to an RMSE between 75 and 95 feet (22.8 and 28.9 meters) using supervised algorithms like XGBoost or K-Neighbors regressor.\n",
    "\n",
    "The 2022 predictions look to be within the range of acceptable values and year-to-year variations. However, if the objective is to help policymakers and water resources management agencies predict a year in advance where to focus their attention in terms of well water shortages and drilling, the level of error of the model feels too big to be useful."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "cell_id": "00032-b96cc429-58f7-40ee-a21d-ee8fc69bc8dd",
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 315.515625
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "cell_id": "00033-562149d5-04e8-478c-a4e5-58da995b98f3",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "b623e53d",
    "execution_start": 1662893080532,
    "execution_millis": 1,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 76
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 1,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "deepnote_notebook_id": "5f86a7cf-0b45-42de-a821-ea56d023ff8c",
  "deepnote": {},
  "deepnote_execution_queue": []
 }
}