{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import neptune.new as neptune\n",
    "from neptune.new.integrations.tensorflow_keras import NeptuneCallback\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, TimeDistributed, Conv1D, MaxPooling1D, Flatten, Bidirectional, Input, Flatten, Activation, Reshape, RepeatVector, Concatenate\n",
    "from keras.optimizers import RMSprop, Adam, Adamax\n",
    "\n",
    "from lib.read_data import read_and_join_output_file\n",
    "from lib.create_pipeline import create_transformation_pipeline\n",
    "from lib.transform_impute import convert_back_df\n",
    "from lib.split_data import train_test_group_time_split"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tensorflow.config.list_physical_devices('GPU')))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "# During experiment we can try to use neptune.ai to log all the Tensorflow experiments results\n",
    "neptune_key = pickle.load(open(\"./neptune.pkl\", \"rb\"))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preparing the Dataset\n",
    "The train and test sets are split by Township-Ranges, i.e. some Township-Ranges data are either fully in the train or test set.\n",
    "The target value is the value of that variable for 2021\n",
    "Thus train/test sets are of shape (number of Township-Ranges, 7 years (2014-2020), the number of features).\n",
    "The input of 1 data point in the model is of shape (7x81\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "                     TOTALDRILLDEPTH_AVG  WELLYIELD_AVG  STATICWATERLEVEL_AVG  \\\nTOWNSHIP_RANGE YEAR                                                             \nT01N R03E      2014             0.097778       0.018246              0.037145   \n               2015             0.095238       0.021053              0.025042   \n               2016             0.114286       0.007916              0.022398   \n               2017             0.000000       0.013684              0.030885   \n               2018             0.083873       0.002474              0.034558   \n...                                  ...            ...                   ...   \nT32S R30E      2016             0.000000       0.000000              0.000000   \n               2017             0.000000       0.000000              0.000000   \n               2018             0.000000       0.000000              0.000000   \n               2019             0.000000       0.000000              0.000000   \n               2020             0.000000       0.000000              0.000000   \n\n                     TOPOFPERFORATEDINTERVAL_AVG  \\\nTOWNSHIP_RANGE YEAR                                \nT01N R03E      2014                     0.098039   \n               2015                     0.117647   \n               2016                     0.152614   \n               2017                     0.127451   \n               2018                     0.148257   \n...                                          ...   \nT32S R30E      2016                     0.000000   \n               2017                     0.000000   \n               2018                     0.000000   \n               2019                     0.000000   \n               2020                     0.000000   \n\n                     BOTTOMOFPERFORATEDINTERVAL_AVG  TOTALCOMPLETEDDEPTH_AVG  \\\nTOWNSHIP_RANGE YEAR                                                            \nT01N R03E      2014                        0.111111                 0.105856   \n               2015                        0.080460                 0.079848   \n               2016                        0.103768                 0.104880   \n               2017                        0.082375                 0.081749   \n               2018                        0.093934                 0.107605   \n...                                             ...                      ...   \nT32S R30E      2016                        0.000000                 0.000000   \n               2017                        0.000000                 0.000000   \n               2018                        0.000000                 0.000000   \n               2019                        0.000000                 0.000000   \n               2020                        0.000000                 0.000000   \n\n                     VEGETATION_BLUE_OAK-GRAY_PINE  \\\nTOWNSHIP_RANGE YEAR                                  \nT01N R03E      2014                       0.000037   \n               2015                       0.000037   \n               2016                       0.000037   \n               2017                       0.000037   \n               2018                       0.000037   \n...                                            ...   \nT32S R30E      2016                       0.033178   \n               2017                       0.033178   \n               2018                       0.033178   \n               2019                       0.033178   \n               2020                       0.033178   \n\n                     VEGETATION_CALIFORNIA_COAST_LIVE_OAK  \\\nTOWNSHIP_RANGE YEAR                                         \nT01N R03E      2014                              0.000137   \n               2015                              0.000137   \n               2016                              0.000137   \n               2017                              0.000137   \n               2018                              0.000137   \n...                                                   ...   \nT32S R30E      2016                              0.000000   \n               2017                              0.000000   \n               2018                              0.000000   \n               2019                              0.000000   \n               2020                              0.000000   \n\n                     VEGETATION_CANYON_LIVE_OAK  VEGETATION_HARD_CHAPARRAL  \\\nTOWNSHIP_RANGE YEAR                                                          \nT01N R03E      2014                    0.000000                   0.000386   \n               2015                    0.000000                   0.000386   \n               2016                    0.000000                   0.000386   \n               2017                    0.000000                   0.000386   \n               2018                    0.000000                   0.000386   \n...                                         ...                        ...   \nT32S R30E      2016                    0.002023                   0.003535   \n               2017                    0.002023                   0.003535   \n               2018                    0.002023                   0.003535   \n               2019                    0.002023                   0.003535   \n               2020                    0.002023                   0.003535   \n\n                     ...  POPULATION_DENSITY  PCT_OF_CAPACITY  \\\nTOWNSHIP_RANGE YEAR  ...                                        \nT01N R03E      2014  ...            0.252900         0.717075   \n               2015  ...            0.252799         0.717075   \n               2016  ...            0.250621         0.717075   \n               2017  ...            0.254669         0.717075   \n               2018  ...            0.256461         0.800728   \n...                  ...                 ...              ...   \nT32S R30E      2016  ...            0.004469         0.496289   \n               2017  ...            0.004457         0.496289   \n               2018  ...            0.004474         0.496289   \n               2019  ...            0.004491         0.580893   \n               2020  ...            0.004512         0.499980   \n\n                     GROUNDSURFACEELEVATION_AVG  AVERAGE_YEARLY_PRECIPITATION  \\\nTOWNSHIP_RANGE YEAR                                                             \nT01N R03E      2014                    0.023626                      0.163573   \n               2015                    0.018249                      0.217900   \n               2016                    0.024153                      0.209056   \n               2017                    0.023541                      0.213645   \n               2018                    0.020523                      0.181012   \n...                                         ...                           ...   \nT32S R30E      2016                    0.139007                      0.111564   \n               2017                    0.139007                      0.169284   \n               2018                    0.139007                      0.079747   \n               2019                    0.139007                      0.158678   \n               2020                    0.139007                      0.156231   \n\n                     SHORTAGE_COUNT   GSE_GWE  WELL_COUNT_AGRICULTURE  \\\nTOWNSHIP_RANGE YEAR                                                     \nT01N R03E      2014             0.0  0.043005                0.029412   \n               2015             0.0  0.050637                0.000000   \n               2016             0.0  0.035780                0.029412   \n               2017             0.0  0.033202                0.000000   \n               2018             0.0  0.030798                0.000000   \n...                             ...       ...                     ...   \nT32S R30E      2016             0.0  0.621728                0.000000   \n               2017             0.0  0.527907                0.000000   \n               2018             0.0  0.556283                0.000000   \n               2019             0.0  0.566892                0.000000   \n               2020             0.0  0.555112                0.000000   \n\n                     WELL_COUNT_DOMESTIC  WELL_COUNT_INDUSTRIAL  \\\nTOWNSHIP_RANGE YEAR                                               \nT01N R03E      2014             0.041667                    0.0   \n               2015             0.027778                    0.0   \n               2016             0.055556                    0.0   \n               2017             0.027778                    0.0   \n               2018             0.097222                    0.0   \n...                                  ...                    ...   \nT32S R30E      2016             0.000000                    0.0   \n               2017             0.000000                    0.0   \n               2018             0.000000                    0.0   \n               2019             0.000000                    0.0   \n               2020             0.000000                    0.0   \n\n                     WELL_COUNT_PUBLIC  \nTOWNSHIP_RANGE YEAR                     \nT01N R03E      2014                0.0  \n               2015                0.0  \n               2016                0.0  \n               2017                0.0  \n               2018                0.0  \n...                                ...  \nT32S R30E      2016                0.0  \n               2017                0.0  \n               2018                0.0  \n               2019                0.0  \n               2020                0.0  \n\n[2674 rows x 81 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th>TOTALDRILLDEPTH_AVG</th>\n      <th>WELLYIELD_AVG</th>\n      <th>STATICWATERLEVEL_AVG</th>\n      <th>TOPOFPERFORATEDINTERVAL_AVG</th>\n      <th>BOTTOMOFPERFORATEDINTERVAL_AVG</th>\n      <th>TOTALCOMPLETEDDEPTH_AVG</th>\n      <th>VEGETATION_BLUE_OAK-GRAY_PINE</th>\n      <th>VEGETATION_CALIFORNIA_COAST_LIVE_OAK</th>\n      <th>VEGETATION_CANYON_LIVE_OAK</th>\n      <th>VEGETATION_HARD_CHAPARRAL</th>\n      <th>...</th>\n      <th>POPULATION_DENSITY</th>\n      <th>PCT_OF_CAPACITY</th>\n      <th>GROUNDSURFACEELEVATION_AVG</th>\n      <th>AVERAGE_YEARLY_PRECIPITATION</th>\n      <th>SHORTAGE_COUNT</th>\n      <th>GSE_GWE</th>\n      <th>WELL_COUNT_AGRICULTURE</th>\n      <th>WELL_COUNT_DOMESTIC</th>\n      <th>WELL_COUNT_INDUSTRIAL</th>\n      <th>WELL_COUNT_PUBLIC</th>\n    </tr>\n    <tr>\n      <th>TOWNSHIP_RANGE</th>\n      <th>YEAR</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">T01N R03E</th>\n      <th>2014</th>\n      <td>0.097778</td>\n      <td>0.018246</td>\n      <td>0.037145</td>\n      <td>0.098039</td>\n      <td>0.111111</td>\n      <td>0.105856</td>\n      <td>0.000037</td>\n      <td>0.000137</td>\n      <td>0.000000</td>\n      <td>0.000386</td>\n      <td>...</td>\n      <td>0.252900</td>\n      <td>0.717075</td>\n      <td>0.023626</td>\n      <td>0.163573</td>\n      <td>0.0</td>\n      <td>0.043005</td>\n      <td>0.029412</td>\n      <td>0.041667</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2015</th>\n      <td>0.095238</td>\n      <td>0.021053</td>\n      <td>0.025042</td>\n      <td>0.117647</td>\n      <td>0.080460</td>\n      <td>0.079848</td>\n      <td>0.000037</td>\n      <td>0.000137</td>\n      <td>0.000000</td>\n      <td>0.000386</td>\n      <td>...</td>\n      <td>0.252799</td>\n      <td>0.717075</td>\n      <td>0.018249</td>\n      <td>0.217900</td>\n      <td>0.0</td>\n      <td>0.050637</td>\n      <td>0.000000</td>\n      <td>0.027778</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2016</th>\n      <td>0.114286</td>\n      <td>0.007916</td>\n      <td>0.022398</td>\n      <td>0.152614</td>\n      <td>0.103768</td>\n      <td>0.104880</td>\n      <td>0.000037</td>\n      <td>0.000137</td>\n      <td>0.000000</td>\n      <td>0.000386</td>\n      <td>...</td>\n      <td>0.250621</td>\n      <td>0.717075</td>\n      <td>0.024153</td>\n      <td>0.209056</td>\n      <td>0.0</td>\n      <td>0.035780</td>\n      <td>0.029412</td>\n      <td>0.055556</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2017</th>\n      <td>0.000000</td>\n      <td>0.013684</td>\n      <td>0.030885</td>\n      <td>0.127451</td>\n      <td>0.082375</td>\n      <td>0.081749</td>\n      <td>0.000037</td>\n      <td>0.000137</td>\n      <td>0.000000</td>\n      <td>0.000386</td>\n      <td>...</td>\n      <td>0.254669</td>\n      <td>0.717075</td>\n      <td>0.023541</td>\n      <td>0.213645</td>\n      <td>0.0</td>\n      <td>0.033202</td>\n      <td>0.000000</td>\n      <td>0.027778</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2018</th>\n      <td>0.083873</td>\n      <td>0.002474</td>\n      <td>0.034558</td>\n      <td>0.148257</td>\n      <td>0.093934</td>\n      <td>0.107605</td>\n      <td>0.000037</td>\n      <td>0.000137</td>\n      <td>0.000000</td>\n      <td>0.000386</td>\n      <td>...</td>\n      <td>0.256461</td>\n      <td>0.800728</td>\n      <td>0.020523</td>\n      <td>0.181012</td>\n      <td>0.0</td>\n      <td>0.030798</td>\n      <td>0.000000</td>\n      <td>0.097222</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">T32S R30E</th>\n      <th>2016</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.033178</td>\n      <td>0.000000</td>\n      <td>0.002023</td>\n      <td>0.003535</td>\n      <td>...</td>\n      <td>0.004469</td>\n      <td>0.496289</td>\n      <td>0.139007</td>\n      <td>0.111564</td>\n      <td>0.0</td>\n      <td>0.621728</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2017</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.033178</td>\n      <td>0.000000</td>\n      <td>0.002023</td>\n      <td>0.003535</td>\n      <td>...</td>\n      <td>0.004457</td>\n      <td>0.496289</td>\n      <td>0.139007</td>\n      <td>0.169284</td>\n      <td>0.0</td>\n      <td>0.527907</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2018</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.033178</td>\n      <td>0.000000</td>\n      <td>0.002023</td>\n      <td>0.003535</td>\n      <td>...</td>\n      <td>0.004474</td>\n      <td>0.496289</td>\n      <td>0.139007</td>\n      <td>0.079747</td>\n      <td>0.0</td>\n      <td>0.556283</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2019</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.033178</td>\n      <td>0.000000</td>\n      <td>0.002023</td>\n      <td>0.003535</td>\n      <td>...</td>\n      <td>0.004491</td>\n      <td>0.580893</td>\n      <td>0.139007</td>\n      <td>0.158678</td>\n      <td>0.0</td>\n      <td>0.566892</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2020</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.033178</td>\n      <td>0.000000</td>\n      <td>0.002023</td>\n      <td>0.003535</td>\n      <td>...</td>\n      <td>0.004512</td>\n      <td>0.499980</td>\n      <td>0.139007</td>\n      <td>0.156231</td>\n      <td>0.0</td>\n      <td>0.555112</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>2674 rows × 81 columns</p>\n</div>"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RANDOM_SEED = 42\n",
    "# Load the data from the ETL output files\n",
    "X = read_and_join_output_file()\n",
    "#X[\"WELL_COUNT\"] = X[\"WELL_COUNT_PUBLIC\"] + X[\"WELL_COUNT_AGRICULTURE\"] + X[\"WELL_COUNT_DOMESTIC\"] + X[\"WELL_COUNT_INDUSTRIAL\"]\n",
    "#X.drop(columns=[\"WELL_COUNT_PUBLIC\", \"WELL_COUNT_AGRICULTURE\", \"WELL_COUNT_DOMESTIC\", \"WELL_COUNT_INDUSTRIAL\"], inplace=True)\n",
    "# Split the data into a training and a test set\n",
    "X_train_df, X_test_df, y_train_df, y_test_df = train_test_group_time_split(X, index=[\"TOWNSHIP_RANGE\", \"YEAR\"], group=\"TOWNSHIP_RANGE\", random_seed=RANDOM_SEED)\n",
    "# Create, fit and apply the data imputation pipeline to the training and test sets\n",
    "impute_pipeline = create_transformation_pipeline(X_train_df, scaler = MinMaxScaler())\n",
    "X_train_impute = impute_pipeline.fit_transform(X_train_df)\n",
    "X_test_impute = impute_pipeline.fit_transform(X_test_df)\n",
    "# Convert the X_train and X_test back to dataframes\n",
    "X_train_impute_df = convert_back_df(X_train_impute, impute_pipeline, X_train_df)\n",
    "X_test_impute_df = convert_back_df(X_test_impute, impute_pipeline, X_test_df)\n",
    "# Keep only the GSE_GWE variable as the outcome variable\n",
    "scaler = MinMaxScaler()\n",
    "y_train = scaler.fit_transform(y_train_df[[\"GSE_GWE\"]])\n",
    "y_test = scaler.transform(y_test_df[[\"GSE_GWE\"]])\n",
    "X_train_impute_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "# Change the shape of the input array to (number of Township-Ranges, 7 years (2014-2020), the number of features)\n",
    "X_train = X_train_impute_df.values.reshape(len(X_train_impute_df.index.get_level_values(0).unique()), len(X_train_impute_df.index.get_level_values(1).unique()), X_train_impute_df.shape[1])\n",
    "X_test = X_test_impute_df.values.reshape(len(X_test_impute_df.index.get_level_values(0).unique()), len(X_test_impute_df.index.get_level_values(1).unique()), X_test_impute_df.shape[1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Checking the train, validation and test input (X) datasets sizes:\n",
      "Size of the X_train dataset: (382, 7, 81)\n",
      "Size of the X_test dataset: (96, 7, 81)\n",
      "====================================================================================================\n",
      "Checking the train, validation and test output (y) datasets sizes:\n",
      "Size of the y_train dataset: (382, 1)\n",
      "Size of the y_test dataset: (96, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*100)\n",
    "print(\"Checking the train, validation and test input (X) datasets sizes:\")\n",
    "print(f\"Size of the X_train dataset: {X_train.shape}\")\n",
    "#print(f\"Size of the X_val dataset: {X_val.shape}\")\n",
    "print(f\"Size of the X_test dataset: {X_test.shape}\")\n",
    "print(\"=\"*100)\n",
    "print(\"Checking the train, validation and test output (y) datasets sizes:\")\n",
    "print(f\"Size of the y_train dataset: {y_train.shape}\")\n",
    "#print(f\"Size of the y_val dataset: {y_val_df.shape}\")\n",
    "print(f\"Size of the y_test dataset: {y_test.shape}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "results_df = y_test_df[[\"GSE_GWE\"]].copy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "hyper_parameters = {\n",
    "    \"validation_split\": 0.05,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"batch_size\": 32,\n",
    "    \"epochs\": 100,\n",
    "    \"lstm_units\": 200,\n",
    "    \"dense_units\": 20,\n",
    "    \"dropout\": 0.2,\n",
    "    \"output_activation\": \"linear\",\n",
    "}\n",
    "nb_features = len(X_train_impute_df.columns)\n",
    "\n",
    "adam_optimizer = Adam(learning_rate=hyper_parameters[\"learning_rate\"])\n",
    "rms_optimizer = RMSprop(learning_rate=hyper_parameters[\"learning_rate\"])\n",
    "adamax_optimizer = Adamax(learning_rate=hyper_parameters[\"learning_rate\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "def evaluate_forecast(y_test_inverse, yhat_inverse):\n",
    "    mse_ = keras.metrics.MeanSquaredError()\n",
    "    mae_ = keras.metrics.MeanAbsoluteError()\n",
    "    rmse_ = keras.metrics.RootMeanSquaredError()\n",
    "    mae = mae_(y_test_inverse,yhat_inverse)\n",
    "    print('mae:', mae)\n",
    "    mse = mse_(y_test_inverse,yhat_inverse)\n",
    "    print('mse:', mse)\n",
    "    rmse = rmse_(y_test_inverse,yhat_inverse)\n",
    "    print('rmse:', rmse)\n",
    "    return mae, mse, rmse"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_2 (LSTM)               (None, 200)               225600    \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 201       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 225,801\n",
      "Trainable params: 225,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1 = Sequential()\n",
    "model1.add(LSTM(hyper_parameters[\"lstm_units\"], input_shape=(7, nb_features)))\n",
    "model1.add(Dense(1, activation=hyper_parameters[\"output_activation\"]))\n",
    "model1.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/mlienart/milestone2/e/MIL06KDIEQ-6\n",
      "Remember to stop your run once you’ve finished logging your metadata (https://docs.neptune.ai/api-reference/run#.stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.\n",
      "Epoch 1/30\n",
      "12/12 [==============================] - 2s 44ms/step - loss: 0.0227 - val_loss: 0.0060\n",
      "Epoch 2/30\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.0083 - val_loss: 0.0265\n",
      "Epoch 3/30\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.0051 - val_loss: 0.0206\n",
      "Epoch 4/30\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.0076 - val_loss: 0.0175\n",
      "Epoch 5/30\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.0049 - val_loss: 0.0105\n",
      "Epoch 6/30\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0058 - val_loss: 0.0093\n",
      "Epoch 7/30\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0040 - val_loss: 0.0041\n",
      "Epoch 8/30\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0044 - val_loss: 0.0086\n",
      "Epoch 9/30\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0047 - val_loss: 0.0033\n",
      "Epoch 10/30\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0027 - val_loss: 0.0031\n",
      "Epoch 11/30\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0048 - val_loss: 0.0064\n",
      "Epoch 12/30\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0039 - val_loss: 0.0070\n",
      "Epoch 13/30\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0032 - val_loss: 0.0054\n",
      "Epoch 14/30\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0035 - val_loss: 0.0052\n",
      "Epoch 15/30\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0031 - val_loss: 0.0167\n",
      "Epoch 16/30\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0031 - val_loss: 0.0035\n",
      "Epoch 17/30\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0033 - val_loss: 0.0072\n",
      "Epoch 18/30\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0025 - val_loss: 0.0028\n",
      "Epoch 19/30\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.0031 - val_loss: 0.0053\n",
      "Epoch 20/30\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.0024 - val_loss: 0.0035\n",
      "Epoch 21/30\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0023 - val_loss: 0.0027\n",
      "Epoch 22/30\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0027 - val_loss: 0.0036\n",
      "Epoch 23/30\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.0019 - val_loss: 0.0069\n",
      "Epoch 24/30\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.0029 - val_loss: 0.0035\n",
      "Epoch 25/30\n",
      "12/12 [==============================] - 0s 12ms/step - loss: 0.0020 - val_loss: 0.0052\n",
      "Epoch 26/30\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.0022 - val_loss: 0.0024\n",
      "Epoch 27/30\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0026 - val_loss: 0.0084\n",
      "Epoch 28/30\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0021 - val_loss: 0.0046\n",
      "Epoch 29/30\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.0019 - val_loss: 0.0051\n",
      "Epoch 30/30\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.0021 - val_loss: 0.0030\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001D0FFA5A3A0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "mae: tf.Tensor(38.663807, shape=(), dtype=float32)\n",
      "mse: tf.Tensor(4080.8516, shape=(), dtype=float32)\n",
      "rmse: tf.Tensor(63.881542, shape=(), dtype=float32)\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 110 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 110 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://app.neptune.ai/mlienart/milestone2/e/MIL06KDIEQ-6\n"
     ]
    }
   ],
   "source": [
    "# Start experiment\n",
    "run = neptune.init(\n",
    "    project=\"mlienart/milestone2\",\n",
    "    api_token=neptune_key,\n",
    "    name=\"Basic Model\"\n",
    ")\n",
    "neptune_cbk = NeptuneCallback(run=run, base_namespace='metrics')\n",
    "hyper_parameters[\"optimizer\"] = \"RMSprop\"\n",
    "run['hyper-parameters'] = hyper_parameters\n",
    "\n",
    "model1.compile(loss=\"mse\", optimizer=rms_optimizer)\n",
    "history = model1.fit(X_train, y_train,\n",
    "                     validation_split=hyper_parameters[\"validation_split\"],\n",
    "                     batch_size=hyper_parameters[\"batch_size\"],\n",
    "                     epochs=hyper_parameters[\"epochs\"],\n",
    "                     shuffle=True,\n",
    "                     callbacks=[neptune_cbk])\n",
    "yhat = model1.predict(X_test, verbose=0)\n",
    "yhat_inverse = scaler.inverse_transform(yhat)\n",
    "y_test_inverse = scaler.inverse_transform(y_test)\n",
    "results_df[\"experiment_1_prediction\"] = yhat_inverse\n",
    "mae, mse, rmse = evaluate_forecast(y_test_inverse, yhat_inverse)\n",
    "run[\"eval/mae\"] = mae\n",
    "run[\"eval/mse\"] = mse\n",
    "run[\"eval/rmse\"] = rmse\n",
    "run.stop()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional_1 (Bidirectio  (None, 400)              451200    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 20)                8020      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 20)                0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 21        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 459,241\n",
      "Trainable params: 459,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Bidirectional(LSTM(hyper_parameters[\"lstm_units\"]), input_shape=(7, nb_features)))\n",
    "model2.add(Dense(hyper_parameters[\"dense_units\"], activation=\"tanh\"))\n",
    "model2.add(Dropout(hyper_parameters[\"dropout\"]))\n",
    "model2.add(Dense(1, activation=hyper_parameters[\"output_activation\"]))\n",
    "model2.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/mlienart/milestone2/e/MIL06KDIEQ-7\n",
      "Remember to stop your run once you’ve finished logging your metadata (https://docs.neptune.ai/api-reference/run#.stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.\n",
      "Epoch 1/30\n",
      "12/12 [==============================] - 5s 125ms/step - loss: 0.0354 - val_loss: 0.0192\n",
      "Epoch 2/30\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 0.0132 - val_loss: 0.0034\n",
      "Epoch 3/30\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 0.0075 - val_loss: 0.0060\n",
      "Epoch 4/30\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 0.0069 - val_loss: 0.0047\n",
      "Epoch 5/30\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 6/30\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 7/30\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 0.0060 - val_loss: 0.0041\n",
      "Epoch 8/30\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 0.0050 - val_loss: 0.0034\n",
      "Epoch 9/30\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 0.0048 - val_loss: 0.0055\n",
      "Epoch 10/30\n",
      "12/12 [==============================] - 1s 63ms/step - loss: 0.0049 - val_loss: 0.0044\n",
      "Epoch 11/30\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 0.0048 - val_loss: 0.0065\n",
      "Epoch 12/30\n",
      "12/12 [==============================] - 1s 64ms/step - loss: 0.0046 - val_loss: 0.0054\n",
      "Epoch 13/30\n",
      "12/12 [==============================] - 1s 64ms/step - loss: 0.0040 - val_loss: 0.0058\n",
      "Epoch 14/30\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 0.0036 - val_loss: 0.0047\n",
      "Epoch 15/30\n",
      "12/12 [==============================] - 1s 64ms/step - loss: 0.0035 - val_loss: 0.0081\n",
      "Epoch 16/30\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 0.0042 - val_loss: 0.0055\n",
      "Epoch 17/30\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.0039 - val_loss: 0.0053\n",
      "Epoch 18/30\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.0043 - val_loss: 0.0043\n",
      "Epoch 19/30\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.0040 - val_loss: 0.0047\n",
      "Epoch 20/30\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.0044 - val_loss: 0.0046\n",
      "Epoch 21/30\n",
      "12/12 [==============================] - 1s 72ms/step - loss: 0.0039 - val_loss: 0.0052\n",
      "Epoch 22/30\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 0.0040 - val_loss: 0.0095\n",
      "Epoch 23/30\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 0.0052 - val_loss: 0.0055\n",
      "Epoch 24/30\n",
      "12/12 [==============================] - 1s 63ms/step - loss: 0.0041 - val_loss: 0.0046\n",
      "Epoch 25/30\n",
      "12/12 [==============================] - 1s 63ms/step - loss: 0.0041 - val_loss: 0.0035\n",
      "Epoch 26/30\n",
      "12/12 [==============================] - 1s 64ms/step - loss: 0.0034 - val_loss: 0.0073\n",
      "Epoch 27/30\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.0042 - val_loss: 0.0051\n",
      "Epoch 28/30\n",
      "12/12 [==============================] - 1s 64ms/step - loss: 0.0038 - val_loss: 0.0070\n",
      "Epoch 29/30\n",
      "12/12 [==============================] - 1s 64ms/step - loss: 0.0041 - val_loss: 0.0053\n",
      "Epoch 30/30\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.0030 - val_loss: 0.0066\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001D0E6D36040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "mae: tf.Tensor(47.703247, shape=(), dtype=float32)\n",
      "mse: tf.Tensor(4278.1597, shape=(), dtype=float32)\n",
      "rmse: tf.Tensor(65.40764, shape=(), dtype=float32)\n",
      "mae: tf.Tensor(47.703247, shape=(), dtype=float32)\n",
      "mse: tf.Tensor(4278.1597, shape=(), dtype=float32)\n",
      "rmse: tf.Tensor(65.40764, shape=(), dtype=float32)\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 65 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 65 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://app.neptune.ai/mlienart/milestone2/e/MIL06KDIEQ-7\n"
     ]
    }
   ],
   "source": [
    "run = neptune.init(\n",
    "    project=\"mlienart/milestone2\",\n",
    "    api_token=neptune_key,\n",
    "    name=\"Advanced Model 1\"\n",
    ")\n",
    "neptune_cbk = NeptuneCallback(run=run, base_namespace='metrics')\n",
    "hyper_parameters[\"optimizer\"] = \"Adam\"\n",
    "run['hyper-parameters'] = hyper_parameters\n",
    "\n",
    "model2.compile(loss=\"mse\", optimizer=adam_optimizer)\n",
    "history = model2.fit(X_train, y_train,\n",
    "                     validation_split=hyper_parameters[\"validation_split\"],\n",
    "                     batch_size=hyper_parameters[\"batch_size\"],\n",
    "                     epochs=hyper_parameters[\"epochs\"],\n",
    "                     shuffle=True,\n",
    "                     callbacks=[neptune_cbk])\n",
    "yhat = model2.predict(X_test, verbose=0)\n",
    "yhat_inverse = scaler.inverse_transform(yhat)\n",
    "y_test_inverse = scaler.inverse_transform(y_test)\n",
    "results_df[\"experiment_2_prediction\"] = yhat_inverse\n",
    "evaluate_forecast(y_test_inverse, yhat_inverse)\n",
    "mae, mse, rmse = evaluate_forecast(y_test_inverse, yhat_inverse)\n",
    "run[\"eval/mae\"] = mae\n",
    "run[\"eval/mse\"] = mse\n",
    "run[\"eval/rmse\"] = rmse\n",
    "run.stop()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/mlienart/milestone2/e/MIL06KDIEQ-8\n",
      "Remember to stop your run once you’ve finished logging your metadata (https://docs.neptune.ai/api-reference/run#.stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.\n",
      "Epoch 1/30\n",
      "12/12 [==============================] - 4s 109ms/step - loss: 0.0173 - val_loss: 0.0050\n",
      "Epoch 2/30\n",
      "12/12 [==============================] - 1s 58ms/step - loss: 0.0047 - val_loss: 0.0182\n",
      "Epoch 3/30\n",
      "12/12 [==============================] - 1s 59ms/step - loss: 0.0052 - val_loss: 0.0032\n",
      "Epoch 4/30\n",
      "12/12 [==============================] - 1s 57ms/step - loss: 0.0055 - val_loss: 0.0038\n",
      "Epoch 5/30\n",
      "12/12 [==============================] - 1s 57ms/step - loss: 0.0065 - val_loss: 0.0110\n",
      "Epoch 6/30\n",
      "12/12 [==============================] - 1s 57ms/step - loss: 0.0062 - val_loss: 0.0040\n",
      "Epoch 7/30\n",
      "12/12 [==============================] - 1s 57ms/step - loss: 0.0051 - val_loss: 0.0036\n",
      "Epoch 8/30\n",
      "12/12 [==============================] - 1s 58ms/step - loss: 0.0044 - val_loss: 0.0070\n",
      "Epoch 9/30\n",
      "12/12 [==============================] - 1s 60ms/step - loss: 0.0054 - val_loss: 0.0062\n",
      "Epoch 10/30\n",
      "12/12 [==============================] - 1s 59ms/step - loss: 0.0046 - val_loss: 0.0034\n",
      "Epoch 11/30\n",
      "12/12 [==============================] - 1s 58ms/step - loss: 0.0049 - val_loss: 0.0041\n",
      "Epoch 12/30\n",
      "12/12 [==============================] - 1s 58ms/step - loss: 0.0043 - val_loss: 0.0026\n",
      "Epoch 13/30\n",
      "12/12 [==============================] - 1s 59ms/step - loss: 0.0043 - val_loss: 0.0044\n",
      "Epoch 14/30\n",
      "12/12 [==============================] - 1s 59ms/step - loss: 0.0033 - val_loss: 0.0086\n",
      "Epoch 15/30\n",
      "12/12 [==============================] - 1s 58ms/step - loss: 0.0069 - val_loss: 0.0055\n",
      "Epoch 16/30\n",
      "12/12 [==============================] - 1s 57ms/step - loss: 0.0035 - val_loss: 0.0083\n",
      "Epoch 17/30\n",
      "12/12 [==============================] - 1s 57ms/step - loss: 0.0048 - val_loss: 0.0071\n",
      "Epoch 18/30\n",
      "12/12 [==============================] - 1s 58ms/step - loss: 0.0038 - val_loss: 0.0042\n",
      "Epoch 19/30\n",
      "12/12 [==============================] - 1s 57ms/step - loss: 0.0036 - val_loss: 0.0078\n",
      "Epoch 20/30\n",
      "12/12 [==============================] - 1s 57ms/step - loss: 0.0037 - val_loss: 0.0171\n",
      "Epoch 21/30\n",
      "12/12 [==============================] - 1s 57ms/step - loss: 0.0046 - val_loss: 0.0051\n",
      "Epoch 22/30\n",
      "12/12 [==============================] - 1s 56ms/step - loss: 0.0045 - val_loss: 0.0079\n",
      "Epoch 23/30\n",
      "12/12 [==============================] - 1s 58ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 24/30\n",
      "12/12 [==============================] - 1s 58ms/step - loss: 0.0033 - val_loss: 0.0133\n",
      "Epoch 25/30\n",
      "12/12 [==============================] - 1s 60ms/step - loss: 0.0041 - val_loss: 0.0086\n",
      "Epoch 26/30\n",
      "12/12 [==============================] - 1s 59ms/step - loss: 0.0033 - val_loss: 0.0036\n",
      "Epoch 27/30\n",
      "12/12 [==============================] - 1s 59ms/step - loss: 0.0032 - val_loss: 0.0076\n",
      "Epoch 28/30\n",
      "12/12 [==============================] - 1s 59ms/step - loss: 0.0037 - val_loss: 0.0037\n",
      "Epoch 29/30\n",
      "12/12 [==============================] - 1s 59ms/step - loss: 0.0027 - val_loss: 0.0042\n",
      "Epoch 30/30\n",
      "12/12 [==============================] - 1s 60ms/step - loss: 0.0032 - val_loss: 0.0058\n",
      "mae: tf.Tensor(37.92641, shape=(), dtype=float32)\n",
      "mse: tf.Tensor(3045.455, shape=(), dtype=float32)\n",
      "rmse: tf.Tensor(55.185642, shape=(), dtype=float32)\n",
      "mae: tf.Tensor(37.92641, shape=(), dtype=float32)\n",
      "mse: tf.Tensor(3045.455, shape=(), dtype=float32)\n",
      "rmse: tf.Tensor(55.185642, shape=(), dtype=float32)\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 159 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 159 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://app.neptune.ai/mlienart/milestone2/e/MIL06KDIEQ-8\n"
     ]
    }
   ],
   "source": [
    "run = neptune.init(\n",
    "    project=\"mlienart/milestone2\",\n",
    "    api_token=neptune_key,\n",
    "    name=\"Advanced Model 1\"\n",
    ")\n",
    "neptune_cbk = NeptuneCallback(run=run, base_namespace='metrics')\n",
    "hyper_parameters[\"optimizer\"] = \"RMSprop\"\n",
    "run['hyper-parameters'] = hyper_parameters\n",
    "\n",
    "model2.compile(loss=\"mse\", optimizer=rms_optimizer)\n",
    "history = model2.fit(X_train, y_train,\n",
    "                     validation_split=hyper_parameters[\"validation_split\"],\n",
    "                     batch_size=hyper_parameters[\"batch_size\"],\n",
    "                     epochs=hyper_parameters[\"epochs\"],\n",
    "                     shuffle=True,\n",
    "                     callbacks=[neptune_cbk])\n",
    "yhat = model2.predict(X_test, verbose=0)\n",
    "yhat_inverse = scaler.inverse_transform(yhat)\n",
    "y_test_inverse = scaler.inverse_transform(y_test)\n",
    "results_df[\"experiment_3_prediction\"] = yhat_inverse\n",
    "evaluate_forecast(y_test_inverse, yhat_inverse)\n",
    "mae, mse, rmse = evaluate_forecast(y_test_inverse, yhat_inverse)\n",
    "run[\"eval/mae\"] = mae\n",
    "run[\"eval/mse\"] = mse\n",
    "run[\"eval/rmse\"] = rmse\n",
    "run.stop()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/mlienart/milestone2/e/MIL06KDIEQ-9\n",
      "Remember to stop your run once you’ve finished logging your metadata (https://docs.neptune.ai/api-reference/run#.stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.\n",
      "Epoch 1/30\n",
      "12/12 [==============================] - 4s 99ms/step - loss: 0.0029 - val_loss: 0.0055\n",
      "Epoch 2/30\n",
      "12/12 [==============================] - 1s 43ms/step - loss: 0.0024 - val_loss: 0.0030\n",
      "Epoch 3/30\n",
      "12/12 [==============================] - 1s 42ms/step - loss: 0.0025 - val_loss: 0.0037\n",
      "Epoch 4/30\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 0.0021 - val_loss: 0.0039\n",
      "Epoch 5/30\n",
      "12/12 [==============================] - 0s 42ms/step - loss: 0.0019 - val_loss: 0.0027\n",
      "Epoch 6/30\n",
      "12/12 [==============================] - 0s 42ms/step - loss: 0.0019 - val_loss: 0.0028\n",
      "Epoch 7/30\n",
      "12/12 [==============================] - 1s 42ms/step - loss: 0.0019 - val_loss: 0.0037\n",
      "Epoch 8/30\n",
      "12/12 [==============================] - 0s 42ms/step - loss: 0.0017 - val_loss: 0.0032\n",
      "Epoch 9/30\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 0.0017 - val_loss: 0.0033\n",
      "Epoch 10/30\n",
      "12/12 [==============================] - 1s 42ms/step - loss: 0.0017 - val_loss: 0.0031\n",
      "Epoch 11/30\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 0.0021 - val_loss: 0.0032\n",
      "Epoch 12/30\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 0.0018 - val_loss: 0.0036\n",
      "Epoch 13/30\n",
      "12/12 [==============================] - 0s 42ms/step - loss: 0.0018 - val_loss: 0.0023\n",
      "Epoch 14/30\n",
      "12/12 [==============================] - 1s 42ms/step - loss: 0.0017 - val_loss: 0.0030\n",
      "Epoch 15/30\n",
      "12/12 [==============================] - 0s 42ms/step - loss: 0.0015 - val_loss: 0.0035\n",
      "Epoch 16/30\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 0.0017 - val_loss: 0.0030\n",
      "Epoch 17/30\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 0.0018 - val_loss: 0.0031\n",
      "Epoch 18/30\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 0.0018 - val_loss: 0.0028\n",
      "Epoch 19/30\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 0.0017 - val_loss: 0.0030\n",
      "Epoch 20/30\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 0.0019 - val_loss: 0.0038\n",
      "Epoch 21/30\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 0.0017 - val_loss: 0.0029\n",
      "Epoch 22/30\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 0.0018 - val_loss: 0.0028\n",
      "Epoch 23/30\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 0.0016 - val_loss: 0.0028\n",
      "Epoch 24/30\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 0.0017 - val_loss: 0.0034\n",
      "Epoch 25/30\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 0.0016 - val_loss: 0.0028\n",
      "Epoch 26/30\n",
      "12/12 [==============================] - 0s 42ms/step - loss: 0.0015 - val_loss: 0.0038\n",
      "Epoch 27/30\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 0.0016 - val_loss: 0.0033\n",
      "Epoch 28/30\n",
      "12/12 [==============================] - 0s 42ms/step - loss: 0.0014 - val_loss: 0.0032\n",
      "Epoch 29/30\n",
      "12/12 [==============================] - 0s 42ms/step - loss: 0.0016 - val_loss: 0.0035\n",
      "Epoch 30/30\n",
      "12/12 [==============================] - 1s 42ms/step - loss: 0.0016 - val_loss: 0.0032\n",
      "mae: tf.Tensor(36.350475, shape=(), dtype=float32)\n",
      "mse: tf.Tensor(4081.327, shape=(), dtype=float32)\n",
      "rmse: tf.Tensor(63.885265, shape=(), dtype=float32)\n",
      "mae: tf.Tensor(36.350475, shape=(), dtype=float32)\n",
      "mse: tf.Tensor(4081.327, shape=(), dtype=float32)\n",
      "rmse: tf.Tensor(63.885265, shape=(), dtype=float32)\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 133 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 133 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://app.neptune.ai/mlienart/milestone2/e/MIL06KDIEQ-9\n"
     ]
    }
   ],
   "source": [
    "run = neptune.init(\n",
    "    project=\"mlienart/milestone2\",\n",
    "    api_token=neptune_key,\n",
    "    name=\"Advanced Model 1\"\n",
    ")\n",
    "neptune_cbk = NeptuneCallback(run=run, base_namespace='metrics')\n",
    "hyper_parameters[\"optimizer\"] = \"Adamx\"\n",
    "run['hyper-parameters'] = hyper_parameters\n",
    "\n",
    "model2.compile(loss=\"mse\", optimizer=adamax_optimizer)\n",
    "history = model2.fit(X_train, y_train,\n",
    "                     validation_split=hyper_parameters[\"validation_split\"],\n",
    "                     batch_size=hyper_parameters[\"batch_size\"],\n",
    "                     epochs=hyper_parameters[\"epochs\"],\n",
    "                     shuffle=True,\n",
    "                     callbacks=[neptune_cbk])\n",
    "yhat = model2.predict(X_test, verbose=0)\n",
    "yhat_inverse = scaler.inverse_transform(yhat)\n",
    "y_test_inverse = scaler.inverse_transform(y_test)\n",
    "results_df[\"experiment_4_prediction\"] = yhat_inverse\n",
    "evaluate_forecast(y_test_inverse, yhat_inverse)\n",
    "mae, mse, rmse = evaluate_forecast(y_test_inverse, yhat_inverse)\n",
    "run[\"eval/mae\"] = mae\n",
    "run[\"eval/mse\"] = mse\n",
    "run[\"eval/rmse\"] = rmse\n",
    "run.stop()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "data": {
      "text/plain": "                        GSE_GWE  experiment_1_prediction  \\\nTOWNSHIP_RANGE YEAR                                        \nT01N R02E      2021   53.193636                91.465973   \nT01N R11E      2021  107.955000               130.581757   \nT01S R03E      2021   24.494538                60.070644   \nT01S R07E      2021   38.644000                74.647034   \nT01S R10E      2021  113.651250               134.993210   \n...                         ...                      ...   \nT31S R26E      2021  173.915909               216.072052   \nT31S R31E      2021  403.900000               362.841461   \nT32S R22E      2021  160.340000               238.303131   \nT32S R25E      2021  190.120000               214.127045   \nT32S R26E      2021  220.866667               246.715103   \n\n                     experiment_2_prediction  experiment_3_prediction  \\\nTOWNSHIP_RANGE YEAR                                                     \nT01N R02E      2021                72.094315                68.847969   \nT01N R11E      2021               184.866486               153.414902   \nT01S R03E      2021                54.500401                34.797050   \nT01S R07E      2021                64.518814                50.588486   \nT01S R10E      2021               174.184525               100.749992   \n...                                      ...                      ...   \nT31S R26E      2021               245.963211               135.791000   \nT31S R31E      2021               387.175690               294.881317   \nT32S R22E      2021               300.588348               196.507721   \nT32S R25E      2021               222.470657               127.866165   \nT32S R26E      2021               255.014175               157.054535   \n\n                     experiment_4_prediction  \nTOWNSHIP_RANGE YEAR                           \nT01N R02E      2021                60.577919  \nT01N R11E      2021               168.913544  \nT01S R03E      2021                33.061378  \nT01S R07E      2021                46.458038  \nT01S R10E      2021               138.436981  \n...                                      ...  \nT31S R26E      2021               219.470413  \nT31S R31E      2021               353.500610  \nT32S R22E      2021               252.820343  \nT32S R25E      2021               214.638840  \nT32S R26E      2021               226.730621  \n\n[96 rows x 5 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th>GSE_GWE</th>\n      <th>experiment_1_prediction</th>\n      <th>experiment_2_prediction</th>\n      <th>experiment_3_prediction</th>\n      <th>experiment_4_prediction</th>\n    </tr>\n    <tr>\n      <th>TOWNSHIP_RANGE</th>\n      <th>YEAR</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>T01N R02E</th>\n      <th>2021</th>\n      <td>53.193636</td>\n      <td>91.465973</td>\n      <td>72.094315</td>\n      <td>68.847969</td>\n      <td>60.577919</td>\n    </tr>\n    <tr>\n      <th>T01N R11E</th>\n      <th>2021</th>\n      <td>107.955000</td>\n      <td>130.581757</td>\n      <td>184.866486</td>\n      <td>153.414902</td>\n      <td>168.913544</td>\n    </tr>\n    <tr>\n      <th>T01S R03E</th>\n      <th>2021</th>\n      <td>24.494538</td>\n      <td>60.070644</td>\n      <td>54.500401</td>\n      <td>34.797050</td>\n      <td>33.061378</td>\n    </tr>\n    <tr>\n      <th>T01S R07E</th>\n      <th>2021</th>\n      <td>38.644000</td>\n      <td>74.647034</td>\n      <td>64.518814</td>\n      <td>50.588486</td>\n      <td>46.458038</td>\n    </tr>\n    <tr>\n      <th>T01S R10E</th>\n      <th>2021</th>\n      <td>113.651250</td>\n      <td>134.993210</td>\n      <td>174.184525</td>\n      <td>100.749992</td>\n      <td>138.436981</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>T31S R26E</th>\n      <th>2021</th>\n      <td>173.915909</td>\n      <td>216.072052</td>\n      <td>245.963211</td>\n      <td>135.791000</td>\n      <td>219.470413</td>\n    </tr>\n    <tr>\n      <th>T31S R31E</th>\n      <th>2021</th>\n      <td>403.900000</td>\n      <td>362.841461</td>\n      <td>387.175690</td>\n      <td>294.881317</td>\n      <td>353.500610</td>\n    </tr>\n    <tr>\n      <th>T32S R22E</th>\n      <th>2021</th>\n      <td>160.340000</td>\n      <td>238.303131</td>\n      <td>300.588348</td>\n      <td>196.507721</td>\n      <td>252.820343</td>\n    </tr>\n    <tr>\n      <th>T32S R25E</th>\n      <th>2021</th>\n      <td>190.120000</td>\n      <td>214.127045</td>\n      <td>222.470657</td>\n      <td>127.866165</td>\n      <td>214.638840</td>\n    </tr>\n    <tr>\n      <th>T32S R26E</th>\n      <th>2021</th>\n      <td>220.866667</td>\n      <td>246.715103</td>\n      <td>255.014175</td>\n      <td>157.054535</td>\n      <td>226.730621</td>\n    </tr>\n  </tbody>\n</table>\n<p>96 rows × 5 columns</p>\n</div>"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}