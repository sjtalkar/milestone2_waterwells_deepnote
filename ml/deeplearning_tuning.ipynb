{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Deeplearning LSTM Model Hyperpamaters Tuning"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import tensorflow\n",
    "import keras_tuner\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, TimeDistributed, RepeatVector\n",
    "from keras.optimizers import Adam\n",
    "from lib.read_data import read_and_join_output_file\n",
    "from lib.deeplearning import get_train_test_datasets,  get_sets_shapes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "RANDOM_SEED = 31\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tensorflow.random.set_seed(RANDOM_SEED)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preparing the Dataset\n",
    "The dataset is prepared as explained in the /ml/deeplearning.ipynb notebook. Please refer to it for more details. As a summary:\n",
    "* The train and test sets are split by Township-Ranges, i.e. some Township-Ranges data are either fully in the train or test set.\n",
    "* The target value is the value of that variable for 2021\n",
    "* Data are imputed using a custom pipeline\n",
    "\n",
    "The resulting train and test sets are of shape [number of Township-Ranges, 7 years (2014-2020), the number of features].\n",
    "We do not create a validation dataset as we use Keras internal cross-validation mechanism to shuffle the data points (i.e., the Township-Ranges) and keep some for the validation at each training epoch."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "                  nb_items  nb_timestamps  nb_features\ntraining dataset       406              7           82\ntest dataset            72              7           82",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>nb_items</th>\n      <th>nb_timestamps</th>\n      <th>nb_features</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>training dataset</th>\n      <td>406</td>\n      <td>7</td>\n      <td>82</td>\n    </tr>\n    <tr>\n      <th>test dataset</th>\n      <td>72</td>\n      <td>7</td>\n      <td>82</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_size=0.15\n",
    "target_variable=\"GSE_GWE\"\n",
    "# Load the data from the ETL output files\n",
    "X = read_and_join_output_file()\n",
    "# Split the input pandas Dataframe into training and test datasets, applies the impute pipeline\n",
    "# transformation and reshapes the datasets to 3D (samples, time, features) numpy arrays\n",
    "X_train, X_test, y_train, y_test, _, _ = get_train_test_datasets(X, target_variable=target_variable, test_size=test_size, random_seed=RANDOM_SEED)\n",
    "nb_features = X_train.shape[-1]\n",
    "get_sets_shapes(X_train, X_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Hyperparameters Tuning\n",
    "For each of the 3 LSTM models architectures (from simplest to most complex), we use the Keras BayesianOptimization hyperparameters tuner to estimate the best values for the following hyperparameters:\n",
    "* the number of units for each *LSTM* or *Dense* unit\n",
    "* the activation function (*sigmoid*, *tanh*, *relu*) used for all layers, except the output layer which is fixed to a *linear* activation function.\n",
    "* the learning rate\n",
    "* the size of the validation dataset\n",
    "* the batch size\n",
    "* the number of epochs\n",
    "## Simple Model Hyper-parameter Tuning\n",
    "![Simple LSTM Model](../doc/images/deeplearning-architecture-1.jpg)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "class Model1(keras_tuner.HyperModel):\n",
    "    def build(self, hp):\n",
    "        model = Sequential()\n",
    "        hp_units = hp.Int(\"units\", min_value=10, max_value=300, step=10)\n",
    "        hp_activ = hp.Choice(\"activation\", values=[\"tanh\", \"sigmoid\"])\n",
    "        model.add(LSTM(units=hp_units, activation=hp_activ, input_shape=(7, nb_features)))\n",
    "        model.add(Dense(1, activation=\"linear\"))\n",
    "        hp_learning_rate = hp.Choice(\"learning_rate\", values=[1e-2, 1e-3, 1e-4])\n",
    "        model.compile(loss=\"mse\", optimizer=Adam(learning_rate=hp_learning_rate), metrics=[keras.metrics.RootMeanSquaredError()])\n",
    "        return model\n",
    "\n",
    "    def fit(self, hp, model, *args, **kwargs):\n",
    "        return model.fit(\n",
    "            *args,\n",
    "            validation_split=hp.Choice(\"validation_split\", values=[0.05, 0.1, 0.15, 0.2]),\n",
    "            batch_size=hp.Int(\"batch_size\", min_value=32, max_value=192, step=16),\n",
    "            epochs=hp.Int(\"epochs\", min_value=30, max_value=500, step=5),\n",
    "            shuffle=True,\n",
    "            **kwargs,\n",
    "        )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 150 Complete [00h 00m 31s]\n",
      "val_root_mean_squared_error: 0.08564456552267075\n",
      "\n",
      "Best val_root_mean_squared_error So Far: 0.0542435497045517\n",
      "Total elapsed time: 01h 10m 42s\n",
      "\n",
      "Search: Running Trial #151\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "300               |120               |units\n",
      "tanh              |sigmoid           |activation\n",
      "0.0001            |0.01              |learning_rate\n",
      "0.2               |0.05              |validation_split\n",
      "80                |32                |batch_size\n",
      "440               |500               |epochs\n",
      "\n",
      "Epoch 1/440\n",
      "5/5 [==============================] - 2s 183ms/step - loss: 0.0641 - root_mean_squared_error: 0.2533 - val_loss: 0.0844 - val_root_mean_squared_error: 0.2905\n",
      "Epoch 2/440\n",
      "5/5 [==============================] - 0s 99ms/step - loss: 0.0405 - root_mean_squared_error: 0.2011 - val_loss: 0.0566 - val_root_mean_squared_error: 0.2380\n",
      "Epoch 3/440\n",
      "5/5 [==============================] - 0s 95ms/step - loss: 0.0331 - root_mean_squared_error: 0.1818 - val_loss: 0.0454 - val_root_mean_squared_error: 0.2132\n",
      "Epoch 4/440\n",
      "5/5 [==============================] - 0s 100ms/step - loss: 0.0296 - root_mean_squared_error: 0.1720 - val_loss: 0.0402 - val_root_mean_squared_error: 0.2005\n",
      "Epoch 5/440\n",
      "5/5 [==============================] - 0s 94ms/step - loss: 0.0264 - root_mean_squared_error: 0.1626 - val_loss: 0.0394 - val_root_mean_squared_error: 0.1986\n",
      "Epoch 6/440\n",
      "5/5 [==============================] - 0s 50ms/step - loss: 0.0224 - root_mean_squared_error: 0.1498 - val_loss: 0.0402 - val_root_mean_squared_error: 0.2006\n",
      "Epoch 7/440\n",
      "5/5 [==============================] - 0s 51ms/step - loss: 0.0193 - root_mean_squared_error: 0.1390 - val_loss: 0.0404 - val_root_mean_squared_error: 0.2010\n",
      "Epoch 8/440\n",
      "5/5 [==============================] - 0s 90ms/step - loss: 0.0171 - root_mean_squared_error: 0.1309 - val_loss: 0.0386 - val_root_mean_squared_error: 0.1966\n",
      "Epoch 9/440\n",
      "5/5 [==============================] - 0s 97ms/step - loss: 0.0154 - root_mean_squared_error: 0.1239 - val_loss: 0.0355 - val_root_mean_squared_error: 0.1883\n",
      "Epoch 10/440\n",
      "5/5 [==============================] - 0s 98ms/step - loss: 0.0137 - root_mean_squared_error: 0.1169 - val_loss: 0.0331 - val_root_mean_squared_error: 0.1819\n",
      "Epoch 11/440\n",
      "5/5 [==============================] - 0s 90ms/step - loss: 0.0123 - root_mean_squared_error: 0.1110 - val_loss: 0.0307 - val_root_mean_squared_error: 0.1754\n",
      "Epoch 12/440\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [6]\u001B[0m, in \u001B[0;36m<cell line: 10>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m stop_early \u001B[38;5;241m=\u001B[39m tensorflow\u001B[38;5;241m.\u001B[39mkeras\u001B[38;5;241m.\u001B[39mcallbacks\u001B[38;5;241m.\u001B[39mEarlyStopping(monitor\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mval_root_mean_squared_error\u001B[39m\u001B[38;5;124m'\u001B[39m, patience\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m, verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m      2\u001B[0m tuner \u001B[38;5;241m=\u001B[39m keras_tuner\u001B[38;5;241m.\u001B[39mBayesianOptimization(Model1(),\n\u001B[0;32m      3\u001B[0m                              objective\u001B[38;5;241m=\u001B[39mkeras_tuner\u001B[38;5;241m.\u001B[39mObjective(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mval_root_mean_squared_error\u001B[39m\u001B[38;5;124m\"\u001B[39m, direction\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmin\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[0;32m      4\u001B[0m                              max_trials\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m250\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m      8\u001B[0m                              directory\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mkeras_tuner\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m      9\u001B[0m                              project_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel1_tuner\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m---> 10\u001B[0m \u001B[43mtuner\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msearch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43mstop_early\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\milestone2\\lib\\site-packages\\keras_tuner\\engine\\base_tuner.py:179\u001B[0m, in \u001B[0;36mBaseTuner.search\u001B[1;34m(self, *fit_args, **fit_kwargs)\u001B[0m\n\u001B[0;32m    176\u001B[0m     \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[0;32m    178\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mon_trial_begin(trial)\n\u001B[1;32m--> 179\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrun_trial(trial, \u001B[38;5;241m*\u001B[39mfit_args, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfit_kwargs)\n\u001B[0;32m    180\u001B[0m \u001B[38;5;66;03m# `results` is None indicates user updated oracle in `run_trial()`.\u001B[39;00m\n\u001B[0;32m    181\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m results \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\milestone2\\lib\\site-packages\\keras_tuner\\engine\\tuner.py:294\u001B[0m, in \u001B[0;36mTuner.run_trial\u001B[1;34m(self, trial, *args, **kwargs)\u001B[0m\n\u001B[0;32m    292\u001B[0m     callbacks\u001B[38;5;241m.\u001B[39mappend(model_checkpoint)\n\u001B[0;32m    293\u001B[0m     copied_kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcallbacks\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m callbacks\n\u001B[1;32m--> 294\u001B[0m     obj_value \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_build_and_fit_model(trial, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mcopied_kwargs)\n\u001B[0;32m    296\u001B[0m     histories\u001B[38;5;241m.\u001B[39mappend(obj_value)\n\u001B[0;32m    297\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m histories\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\milestone2\\lib\\site-packages\\keras_tuner\\engine\\tuner.py:222\u001B[0m, in \u001B[0;36mTuner._build_and_fit_model\u001B[1;34m(self, trial, *args, **kwargs)\u001B[0m\n\u001B[0;32m    220\u001B[0m hp \u001B[38;5;241m=\u001B[39m trial\u001B[38;5;241m.\u001B[39mhyperparameters\n\u001B[0;32m    221\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_try_build(hp)\n\u001B[1;32m--> 222\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhypermodel\u001B[38;5;241m.\u001B[39mfit(hp, model, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    223\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m tuner_utils\u001B[38;5;241m.\u001B[39mconvert_to_metrics_dict(\n\u001B[0;32m    224\u001B[0m     results, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moracle\u001B[38;5;241m.\u001B[39mobjective, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mHyperModel.fit()\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    225\u001B[0m )\n",
      "Input \u001B[1;32mIn [5]\u001B[0m, in \u001B[0;36mModel1.fit\u001B[1;34m(self, hp, model, *args, **kwargs)\u001B[0m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfit\u001B[39m(\u001B[38;5;28mself\u001B[39m, hp, model, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m---> 13\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m model\u001B[38;5;241m.\u001B[39mfit(\n\u001B[0;32m     14\u001B[0m         \u001B[38;5;241m*\u001B[39margs,\n\u001B[0;32m     15\u001B[0m         validation_split\u001B[38;5;241m=\u001B[39mhp\u001B[38;5;241m.\u001B[39mChoice(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvalidation_split\u001B[39m\u001B[38;5;124m\"\u001B[39m, values\u001B[38;5;241m=\u001B[39m[\u001B[38;5;241m0.05\u001B[39m, \u001B[38;5;241m0.1\u001B[39m, \u001B[38;5;241m0.15\u001B[39m, \u001B[38;5;241m0.2\u001B[39m]),\n\u001B[0;32m     16\u001B[0m         batch_size\u001B[38;5;241m=\u001B[39mhp\u001B[38;5;241m.\u001B[39mInt(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbatch_size\u001B[39m\u001B[38;5;124m\"\u001B[39m, min_value\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m32\u001B[39m, max_value\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m192\u001B[39m, step\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m16\u001B[39m),\n\u001B[0;32m     17\u001B[0m         epochs\u001B[38;5;241m=\u001B[39mhp\u001B[38;5;241m.\u001B[39mInt(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mepochs\u001B[39m\u001B[38;5;124m\"\u001B[39m, min_value\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m30\u001B[39m, max_value\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m500\u001B[39m, step\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m5\u001B[39m),\n\u001B[0;32m     18\u001B[0m         shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m     19\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m     20\u001B[0m     )\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\milestone2\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     62\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m     63\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 64\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m     65\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:  \u001B[38;5;66;03m# pylint: disable=broad-except\u001B[39;00m\n\u001B[0;32m     66\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\milestone2\\lib\\site-packages\\keras\\engine\\training.py:1409\u001B[0m, in \u001B[0;36mModel.fit\u001B[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[0;32m   1402\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m tf\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mexperimental\u001B[38;5;241m.\u001B[39mTrace(\n\u001B[0;32m   1403\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m   1404\u001B[0m     epoch_num\u001B[38;5;241m=\u001B[39mepoch,\n\u001B[0;32m   1405\u001B[0m     step_num\u001B[38;5;241m=\u001B[39mstep,\n\u001B[0;32m   1406\u001B[0m     batch_size\u001B[38;5;241m=\u001B[39mbatch_size,\n\u001B[0;32m   1407\u001B[0m     _r\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m):\n\u001B[0;32m   1408\u001B[0m   callbacks\u001B[38;5;241m.\u001B[39mon_train_batch_begin(step)\n\u001B[1;32m-> 1409\u001B[0m   tmp_logs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43miterator\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1410\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m data_handler\u001B[38;5;241m.\u001B[39mshould_sync:\n\u001B[0;32m   1411\u001B[0m     context\u001B[38;5;241m.\u001B[39masync_wait()\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\milestone2\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    148\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    149\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 150\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    151\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    152\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\milestone2\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001B[0m, in \u001B[0;36mFunction.__call__\u001B[1;34m(self, *args, **kwds)\u001B[0m\n\u001B[0;32m    912\u001B[0m compiler \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mxla\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jit_compile \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnonXla\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    914\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m OptionalXlaContext(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jit_compile):\n\u001B[1;32m--> 915\u001B[0m   result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)\n\u001B[0;32m    917\u001B[0m new_tracing_count \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexperimental_get_tracing_count()\n\u001B[0;32m    918\u001B[0m without_tracing \u001B[38;5;241m=\u001B[39m (tracing_count \u001B[38;5;241m==\u001B[39m new_tracing_count)\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\milestone2\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001B[0m, in \u001B[0;36mFunction._call\u001B[1;34m(self, *args, **kwds)\u001B[0m\n\u001B[0;32m    944\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock\u001B[38;5;241m.\u001B[39mrelease()\n\u001B[0;32m    945\u001B[0m   \u001B[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001B[39;00m\n\u001B[0;32m    946\u001B[0m   \u001B[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001B[39;00m\n\u001B[1;32m--> 947\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_stateless_fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)  \u001B[38;5;66;03m# pylint: disable=not-callable\u001B[39;00m\n\u001B[0;32m    948\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_stateful_fn \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    949\u001B[0m   \u001B[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001B[39;00m\n\u001B[0;32m    950\u001B[0m   \u001B[38;5;66;03m# in parallel.\u001B[39;00m\n\u001B[0;32m    951\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock\u001B[38;5;241m.\u001B[39mrelease()\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\milestone2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2453\u001B[0m, in \u001B[0;36mFunction.__call__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   2450\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock:\n\u001B[0;32m   2451\u001B[0m   (graph_function,\n\u001B[0;32m   2452\u001B[0m    filtered_flat_args) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_maybe_define_function(args, kwargs)\n\u001B[1;32m-> 2453\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mgraph_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_flat\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   2454\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfiltered_flat_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcaptured_inputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgraph_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcaptured_inputs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\milestone2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1860\u001B[0m, in \u001B[0;36mConcreteFunction._call_flat\u001B[1;34m(self, args, captured_inputs, cancellation_manager)\u001B[0m\n\u001B[0;32m   1856\u001B[0m possible_gradient_type \u001B[38;5;241m=\u001B[39m gradients_util\u001B[38;5;241m.\u001B[39mPossibleTapeGradientTypes(args)\n\u001B[0;32m   1857\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (possible_gradient_type \u001B[38;5;241m==\u001B[39m gradients_util\u001B[38;5;241m.\u001B[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001B[0;32m   1858\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m executing_eagerly):\n\u001B[0;32m   1859\u001B[0m   \u001B[38;5;66;03m# No tape is watching; skip to running the function.\u001B[39;00m\n\u001B[1;32m-> 1860\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_build_call_outputs(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_inference_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcall\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1861\u001B[0m \u001B[43m      \u001B[49m\u001B[43mctx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcancellation_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcancellation_manager\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m   1862\u001B[0m forward_backward \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_select_forward_and_backward_functions(\n\u001B[0;32m   1863\u001B[0m     args,\n\u001B[0;32m   1864\u001B[0m     possible_gradient_type,\n\u001B[0;32m   1865\u001B[0m     executing_eagerly)\n\u001B[0;32m   1866\u001B[0m forward_function, args_with_tangents \u001B[38;5;241m=\u001B[39m forward_backward\u001B[38;5;241m.\u001B[39mforward()\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\milestone2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:497\u001B[0m, in \u001B[0;36m_EagerDefinedFunction.call\u001B[1;34m(self, ctx, args, cancellation_manager)\u001B[0m\n\u001B[0;32m    495\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m _InterpolateFunctionError(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    496\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m cancellation_manager \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 497\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[43mexecute\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexecute\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    498\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msignature\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    499\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnum_outputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_num_outputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    500\u001B[0m \u001B[43m        \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    501\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattrs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattrs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    502\u001B[0m \u001B[43m        \u001B[49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mctx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    503\u001B[0m   \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    504\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m execute\u001B[38;5;241m.\u001B[39mexecute_with_cancellation(\n\u001B[0;32m    505\u001B[0m         \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msignature\u001B[38;5;241m.\u001B[39mname),\n\u001B[0;32m    506\u001B[0m         num_outputs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_outputs,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    509\u001B[0m         ctx\u001B[38;5;241m=\u001B[39mctx,\n\u001B[0;32m    510\u001B[0m         cancellation_manager\u001B[38;5;241m=\u001B[39mcancellation_manager)\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\milestone2\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001B[0m, in \u001B[0;36mquick_execute\u001B[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001B[0m\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m     53\u001B[0m   ctx\u001B[38;5;241m.\u001B[39mensure_initialized()\n\u001B[1;32m---> 54\u001B[0m   tensors \u001B[38;5;241m=\u001B[39m \u001B[43mpywrap_tfe\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTFE_Py_Execute\u001B[49m\u001B[43m(\u001B[49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_handle\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mop_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     55\u001B[0m \u001B[43m                                      \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattrs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_outputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     56\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m core\u001B[38;5;241m.\u001B[39m_NotOkStatusException \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m     57\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "stop_early = tensorflow.keras.callbacks.EarlyStopping(monitor='val_root_mean_squared_error', patience=10, verbose=1)\n",
    "tuner = keras_tuner.BayesianOptimization(Model1(),\n",
    "                             objective=keras_tuner.Objective(\"val_root_mean_squared_error\", direction=\"min\"),\n",
    "                             max_trials=250,\n",
    "                             beta=3.2,\n",
    "                             seed=RANDOM_SEED,\n",
    "                             overwrite=True,\n",
    "                             directory=\"keras_tuner\",\n",
    "                             project_name=\"model1_tuner\")\n",
    "tuner.search(X_train, y_train, callbacks=[stop_early])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Best Model Hyperparameters"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get the optimal hyperparameters\n",
    "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(f\"\"\"\n",
    "The hyperparameter search is complete.\n",
    "validation_split: {best_hps.get('validation_split')}\n",
    "lstm_units: {best_hps.get('units')}\n",
    "lstm_activation: {best_hps.get('activation')}\n",
    "learning_rate: {best_hps.get('learning_rate')}\n",
    "batch_size: {best_hps.get('batch_size')}\n",
    "epochs: {best_hps.get('epochs')}\n",
    "\"\"\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Hyperparameters Tuning Summary"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tuner.results_summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model2 Hyper-parameter tuning\n",
    "![LSTM Model With Dense Layer](../doc/images/deeplearning-architecture-2.jpg)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "class Model2(keras_tuner.HyperModel):\n",
    "    def build(self, hp):\n",
    "        model = Sequential()\n",
    "        lstm_units = hp.Int(\"lstm_units\", min_value=10, max_value=300, step=10)\n",
    "        lstm_activ = hp.Choice(\"lstm_activation\", values=[\"tanh\", \"sigmoid\"])\n",
    "        model.add(LSTM(units=lstm_units, activation=lstm_activ, input_shape=(7, nb_features)))\n",
    "        dense_units = hp.Int(\"dense_units\", min_value=11, max_value=101, step=2)\n",
    "        dense_activation = hp.Choice(\"dense_activation\", values=[\"relu\", \"tanh\", \"sigmoid\"])\n",
    "        model.add(Dense(dense_units, activation=dense_activation))\n",
    "        hp_dropout = hp.Float(\"dropout_rate\", min_value=0.05, max_value=0.25, step=0.05)\n",
    "        model.add(Dropout(hp_dropout))\n",
    "        model.add(Dense(1, activation=\"linear\"))\n",
    "        hp_learning_rate = hp.Choice(\"learning_rate\", values=[1e-2, 1e-3, 1e-4])\n",
    "        model.compile(loss=\"mse\", optimizer=Adam(learning_rate=hp_learning_rate), metrics=[keras.metrics.RootMeanSquaredError()])\n",
    "        return model\n",
    "\n",
    "    def fit(self, hp, model, *args, **kwargs):\n",
    "        return model.fit(\n",
    "            *args,\n",
    "            validation_split=hp.Choice(\"validation_split\", values=[0.05, 0.1, 0.15, 0.2]),\n",
    "            batch_size=hp.Int(\"batch_size\", min_value=32, max_value=192, step=16),\n",
    "            epochs=hp.Int(\"epochs\", min_value=30, max_value=500, step=5),\n",
    "            shuffle=True,\n",
    "            **kwargs,\n",
    "        )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 400 Complete [00h 00m 06s]\n",
      "val_root_mean_squared_error: 0.08002614974975586\n",
      "\n",
      "Best val_root_mean_squared_error So Far: 0.06468775868415833\n",
      "Total elapsed time: 03h 14m 13s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "stop_early = tensorflow.keras.callbacks.EarlyStopping(monitor=\"val_root_mean_squared_error\", patience=10, verbose=1)\n",
    "tuner = keras_tuner.BayesianOptimization(Model2(),\n",
    "                              objective=keras_tuner.Objective(\"val_root_mean_squared_error\", direction=\"min\"),\n",
    "                              max_trials=400,\n",
    "                              beta=3.2,\n",
    "                              seed=RANDOM_SEED,\n",
    "                              overwrite=True,\n",
    "                              directory=\"keras_tuner\",\n",
    "                              project_name=\"model2_tuner\")\n",
    "tuner.search(X_train, y_train, callbacks=[stop_early])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Best Model Hyperparameters"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The hyperparameter search is complete.\n",
      "validation_split: 0.05\n",
      "lstm_units: 10\n",
      "lstm_activation: sigmoid\n",
      "dense_units: 11\n",
      "dense_activation: relu\n",
      "dropout_rate: 0.05\n",
      "learning_rate: 0.01\n",
      "batch_size: 32\n",
      "epochs: 500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the optimal hyperparameters\n",
    "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(f\"\"\"\n",
    "The hyperparameter search is complete.\n",
    "validation_split: {best_hps.get('validation_split')}\n",
    "lstm_units: {best_hps.get('lstm_units')}\n",
    "lstm_activation: {best_hps.get('lstm_activation')}\n",
    "dense_units: {best_hps.get('dense_units')}\n",
    "dense_activation: {best_hps.get('dense_activation')}\n",
    "dropout_rate: {best_hps.get('dropout_rate')}\n",
    "learning_rate: {best_hps.get('learning_rate')}\n",
    "batch_size: {best_hps.get('batch_size')}\n",
    "epochs: {best_hps.get('epochs')}\n",
    "\"\"\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Hyperparameters Tuning Summary"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results summary\n",
      "Results in keras_tuner\\model2_tuner\n",
      "Showing 10 best trials\n",
      "<keras_tuner.engine.objective.Objective object at 0x00000220D8130B50>\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "lstm_units: 10\n",
      "lstm_activation: sigmoid\n",
      "dense_units: 11\n",
      "dense_activation: relu\n",
      "dropout_rate: 0.05\n",
      "learning_rate: 0.01\n",
      "validation_split: 0.05\n",
      "batch_size: 32\n",
      "epochs: 500\n",
      "Score: 0.06468775868415833\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "lstm_units: 10\n",
      "lstm_activation: sigmoid\n",
      "dense_units: 11\n",
      "dense_activation: relu\n",
      "dropout_rate: 0.1\n",
      "learning_rate: 0.01\n",
      "validation_split: 0.05\n",
      "batch_size: 32\n",
      "epochs: 500\n",
      "Score: 0.06481722742319107\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "lstm_units: 10\n",
      "lstm_activation: sigmoid\n",
      "dense_units: 11\n",
      "dense_activation: relu\n",
      "dropout_rate: 0.15000000000000002\n",
      "learning_rate: 0.01\n",
      "validation_split: 0.05\n",
      "batch_size: 32\n",
      "epochs: 415\n",
      "Score: 0.06549284607172012\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "lstm_units: 10\n",
      "lstm_activation: sigmoid\n",
      "dense_units: 11\n",
      "dense_activation: relu\n",
      "dropout_rate: 0.15000000000000002\n",
      "learning_rate: 0.01\n",
      "validation_split: 0.05\n",
      "batch_size: 32\n",
      "epochs: 415\n",
      "Score: 0.06606409698724747\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "lstm_units: 10\n",
      "lstm_activation: sigmoid\n",
      "dense_units: 57\n",
      "dense_activation: relu\n",
      "dropout_rate: 0.2\n",
      "learning_rate: 0.01\n",
      "validation_split: 0.05\n",
      "batch_size: 32\n",
      "epochs: 145\n",
      "Score: 0.06691084057092667\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "lstm_units: 10\n",
      "lstm_activation: sigmoid\n",
      "dense_units: 11\n",
      "dense_activation: relu\n",
      "dropout_rate: 0.05\n",
      "learning_rate: 0.01\n",
      "validation_split: 0.05\n",
      "batch_size: 32\n",
      "epochs: 500\n",
      "Score: 0.06699465960264206\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "lstm_units: 10\n",
      "lstm_activation: sigmoid\n",
      "dense_units: 11\n",
      "dense_activation: relu\n",
      "dropout_rate: 0.15000000000000002\n",
      "learning_rate: 0.01\n",
      "validation_split: 0.05\n",
      "batch_size: 32\n",
      "epochs: 410\n",
      "Score: 0.06715192645788193\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "lstm_units: 10\n",
      "lstm_activation: sigmoid\n",
      "dense_units: 11\n",
      "dense_activation: relu\n",
      "dropout_rate: 0.05\n",
      "learning_rate: 0.01\n",
      "validation_split: 0.05\n",
      "batch_size: 32\n",
      "epochs: 500\n",
      "Score: 0.06768625229597092\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "lstm_units: 10\n",
      "lstm_activation: sigmoid\n",
      "dense_units: 21\n",
      "dense_activation: relu\n",
      "dropout_rate: 0.05\n",
      "learning_rate: 0.01\n",
      "validation_split: 0.05\n",
      "batch_size: 32\n",
      "epochs: 360\n",
      "Score: 0.06787063926458359\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "lstm_units: 10\n",
      "lstm_activation: sigmoid\n",
      "dense_units: 11\n",
      "dense_activation: relu\n",
      "dropout_rate: 0.05\n",
      "learning_rate: 0.01\n",
      "validation_split: 0.05\n",
      "batch_size: 32\n",
      "epochs: 500\n",
      "Score: 0.06824542582035065\n"
     ]
    }
   ],
   "source": [
    "tuner.results_summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model3 Hyper-parameter tuning\n",
    "![Encoder-Decoder LSTM Model](../doc/images/deeplearning-architecture-3.jpg)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "class Model3(keras_tuner.HyperModel):\n",
    "    def build(self, hp):\n",
    "        model = Sequential()\n",
    "        lstm_units = hp.Int(\"lstm_units\", min_value=10, max_value=300, step=10)\n",
    "        lstm_activ = hp.Choice(\"lstm_activation\", values=[\"tanh\", \"sigmoid\"])\n",
    "        model.add(LSTM(units=lstm_units, activation=\"sigmoid\", input_shape=(7, nb_features)))\n",
    "        model.add(RepeatVector(1))\n",
    "        lstm_units_2 = hp.Int(\"2nd_lstm_units\", min_value=10, max_value=300, step=10)\n",
    "        lstm_activ_2 = hp.Choice(\"2nd_lstm_activation\", values=[\"tanh\", \"sigmoid\"])\n",
    "        model.add(LSTM(units=lstm_units_2, activation=\"sigmoid\", return_sequences=True))\n",
    "        dense_units = hp.Int(\"dense_units\", min_value=11, max_value=101, step=2)\n",
    "        dense_activation = hp.Choice(\"dense_activation\", values=[\"relu\", \"tanh\", \"sigmoid\"])\n",
    "        model.add(TimeDistributed(Dense(dense_units, activation=dense_activation)))\n",
    "        hp_dropout = hp.Float(\"dropout_rate\", min_value=0.05, max_value=0.25, step=0.05)\n",
    "        model.add(Dropout(hp_dropout))\n",
    "        model.add(Dense(1, activation=\"linear\"))\n",
    "        hp_learning_rate = hp.Choice(\"learning_rate\", values=[1e-2, 1e-3, 1e-4])\n",
    "        model.compile(loss=\"mse\", optimizer=Adam(learning_rate=hp_learning_rate), metrics=[keras.metrics.RootMeanSquaredError()])\n",
    "        return model\n",
    "\n",
    "    def fit(self, hp, model, *args, **kwargs):\n",
    "        return model.fit(\n",
    "            *args,\n",
    "            validation_split=hp.Choice(\"validation_split\", values=[0.05, 0.1, 0.15, 0.2]),\n",
    "            batch_size=hp.Int(\"batch_size\", min_value=32, max_value=192, step=16),\n",
    "            epochs=hp.Int(\"epochs\", min_value=30, max_value=500, step=5),\n",
    "            shuffle=True,\n",
    "            **kwargs,\n",
    "        )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 500 Complete [00h 00m 18s]\n",
      "val_root_mean_squared_error: 0.1150362491607666\n",
      "\n",
      "Best val_root_mean_squared_error So Far: 0.06309118866920471\n",
      "Total elapsed time: 07h 29m 36s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "stop_early = tensorflow.keras.callbacks.EarlyStopping(monitor=\"val_root_mean_squared_error\", patience=10, verbose=1)\n",
    "tuner = keras_tuner.BayesianOptimization(Model3(),\n",
    "                              objective=keras_tuner.Objective(\"val_root_mean_squared_error\", direction=\"min\"),\n",
    "                              max_trials=400,\n",
    "                              beta=3.2,\n",
    "                              seed=RANDOM_SEED,\n",
    "                              overwrite=True,\n",
    "                              directory=\"keras_tuner\",\n",
    "                              project_name=\"model3_tuner\")\n",
    "tuner.search(X_train, y_train, callbacks=[stop_early])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Best Model Hyperparameters"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The hyperparameter search is complete.\n",
      "validation_split: 0.05\n",
      "lstm_units: 300\n",
      "lstm_activation: sigmoid\n",
      "2nd_lstm_units: 300\n",
      "2nd_lstm_activation: tanh\n",
      "dense_units: 11\n",
      "dense_activation: sigmoid\n",
      "dropout_rate: 0.05\n",
      "learning_rate: 0.01\n",
      "batch_size: 32\n",
      "epochs: 30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the optimal hyperparameters\n",
    "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(f\"\"\"\n",
    "The hyperparameter search is complete.\n",
    "validation_split: {best_hps.get('validation_split')}\n",
    "lstm_units: {best_hps.get('lstm_units')}\n",
    "lstm_activation: {best_hps.get('lstm_activation')}\n",
    "2nd_lstm_units: {best_hps.get('2nd_lstm_units')}\n",
    "2nd_lstm_activation: {best_hps.get('2nd_lstm_activation')}\n",
    "dense_units: {best_hps.get('dense_units')}\n",
    "dense_activation: {best_hps.get('dense_activation')}\n",
    "dropout_rate: {best_hps.get('dropout_rate')}\n",
    "learning_rate: {best_hps.get('learning_rate')}\n",
    "batch_size: {best_hps.get('batch_size')}\n",
    "epochs: {best_hps.get('epochs')}\n",
    "\"\"\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Hyperparameters Tuning Summary"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results summary\n",
      "Results in keras_tuner\\model3_tuner\n",
      "Showing 10 best trials\n",
      "<keras_tuner.engine.objective.Objective object at 0x000002575F038250>\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "lstm_units: 300\n",
      "lstm_activation: sigmoid\n",
      "2nd_lstm_units: 300\n",
      "2nd_lstm_activation: tanh\n",
      "dense_units: 11\n",
      "dense_activation: sigmoid\n",
      "dropout_rate: 0.05\n",
      "learning_rate: 0.01\n",
      "validation_split: 0.05\n",
      "batch_size: 32\n",
      "epochs: 30\n",
      "Score: 0.06309118866920471\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "lstm_units: 300\n",
      "lstm_activation: sigmoid\n",
      "2nd_lstm_units: 300\n",
      "2nd_lstm_activation: tanh\n",
      "dense_units: 11\n",
      "dense_activation: sigmoid\n",
      "dropout_rate: 0.05\n",
      "learning_rate: 0.01\n",
      "validation_split: 0.05\n",
      "batch_size: 32\n",
      "epochs: 30\n",
      "Score: 0.06499446928501129\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "lstm_units: 300\n",
      "lstm_activation: sigmoid\n",
      "2nd_lstm_units: 300\n",
      "2nd_lstm_activation: sigmoid\n",
      "dense_units: 11\n",
      "dense_activation: sigmoid\n",
      "dropout_rate: 0.05\n",
      "learning_rate: 0.01\n",
      "validation_split: 0.05\n",
      "batch_size: 32\n",
      "epochs: 30\n",
      "Score: 0.06594016402959824\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "lstm_units: 300\n",
      "lstm_activation: sigmoid\n",
      "2nd_lstm_units: 300\n",
      "2nd_lstm_activation: tanh\n",
      "dense_units: 11\n",
      "dense_activation: sigmoid\n",
      "dropout_rate: 0.05\n",
      "learning_rate: 0.01\n",
      "validation_split: 0.05\n",
      "batch_size: 32\n",
      "epochs: 30\n",
      "Score: 0.06661056727170944\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "lstm_units: 300\n",
      "lstm_activation: sigmoid\n",
      "2nd_lstm_units: 300\n",
      "2nd_lstm_activation: tanh\n",
      "dense_units: 11\n",
      "dense_activation: sigmoid\n",
      "dropout_rate: 0.05\n",
      "learning_rate: 0.01\n",
      "validation_split: 0.05\n",
      "batch_size: 32\n",
      "epochs: 30\n",
      "Score: 0.06901232153177261\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "lstm_units: 300\n",
      "lstm_activation: sigmoid\n",
      "2nd_lstm_units: 300\n",
      "2nd_lstm_activation: tanh\n",
      "dense_units: 11\n",
      "dense_activation: sigmoid\n",
      "dropout_rate: 0.05\n",
      "learning_rate: 0.01\n",
      "validation_split: 0.05\n",
      "batch_size: 32\n",
      "epochs: 30\n",
      "Score: 0.06999509781599045\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "lstm_units: 300\n",
      "lstm_activation: sigmoid\n",
      "2nd_lstm_units: 300\n",
      "2nd_lstm_activation: tanh\n",
      "dense_units: 11\n",
      "dense_activation: sigmoid\n",
      "dropout_rate: 0.05\n",
      "learning_rate: 0.01\n",
      "validation_split: 0.05\n",
      "batch_size: 32\n",
      "epochs: 30\n",
      "Score: 0.07048741728067398\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "lstm_units: 300\n",
      "lstm_activation: tanh\n",
      "2nd_lstm_units: 300\n",
      "2nd_lstm_activation: tanh\n",
      "dense_units: 11\n",
      "dense_activation: sigmoid\n",
      "dropout_rate: 0.05\n",
      "learning_rate: 0.01\n",
      "validation_split: 0.05\n",
      "batch_size: 32\n",
      "epochs: 30\n",
      "Score: 0.07083387672901154\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "lstm_units: 300\n",
      "lstm_activation: tanh\n",
      "2nd_lstm_units: 300\n",
      "2nd_lstm_activation: tanh\n",
      "dense_units: 11\n",
      "dense_activation: sigmoid\n",
      "dropout_rate: 0.05\n",
      "learning_rate: 0.01\n",
      "validation_split: 0.05\n",
      "batch_size: 32\n",
      "epochs: 30\n",
      "Score: 0.07127515226602554\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "lstm_units: 300\n",
      "lstm_activation: sigmoid\n",
      "2nd_lstm_units: 300\n",
      "2nd_lstm_activation: sigmoid\n",
      "dense_units: 11\n",
      "dense_activation: sigmoid\n",
      "dropout_rate: 0.05\n",
      "learning_rate: 0.01\n",
      "validation_split: 0.05\n",
      "batch_size: 32\n",
      "epochs: 30\n",
      "Score: 0.07147232443094254\n"
     ]
    }
   ],
   "source": [
    "tuner.results_summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}