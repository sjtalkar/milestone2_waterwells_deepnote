{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Deeplearning LSTM Model Hyperpamaters Tuning\n",
    "This notebook runs a Bayesian tuning process to find the best hyperparameters for 3 different LSTM models architectures."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "cell_id": "18337427-36ce-4fe5-8247-a76920367c21",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "cell_id": "00001-06fa5675-9b65-4e8a-8cfa-9cac63e669d9",
    "deepnote_cell_type": "code"
   },
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "cell_id": "00002-c8998ab4-cdb1-423c-b355-71b104dc2263",
    "deepnote_cell_type": "code"
   },
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import tensorflow\n",
    "import keras_tuner\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, TimeDistributed, RepeatVector\n",
    "from keras.optimizers import Adam\n",
    "from lib.read_data import read_and_join_output_file\n",
    "from lib.deeplearning import get_train_test_datasets,  get_sets_shapes"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "cell_id": "00003-cd907517-9663-4391-ac26-29e95188437a",
    "deepnote_cell_type": "code"
   },
   "source": [
    "RANDOM_SEED = 31\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tensorflow.random.set_seed(RANDOM_SEED)"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preparing the Dataset\n",
    "The dataset is prepared as explained in the /ml/deeplearning_training.ipynb notebook. Please refer to it for more details. As a summary:\n",
    "* The train and test sets are split by Township-Ranges, i.e. some Township-Ranges data are either fully in the train or test set.\n",
    "* The target value is the value of that variable for 2021\n",
    "* Data are imputed using a custom pipeline\n",
    "\n",
    "The resulting train and test sets are of shape [number of Township-Ranges, 7 years (2014-2020), the number of features].\n",
    "We do not create a validation dataset as we use Keras internal cross-validation mechanism to shuffle the data points (i.e., the Township-Ranges) and keep some for the validation at each training epoch."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "cell_id": "00004-afd3bb8d-a214-4047-a2a6-49c4025f1e5e",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "                  nb_items  nb_timestamps  nb_features\ntraining dataset       406              7           80\ntest dataset            72              7           80",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>nb_items</th>\n      <th>nb_timestamps</th>\n      <th>nb_features</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>training dataset</th>\n      <td>406</td>\n      <td>7</td>\n      <td>80</td>\n    </tr>\n    <tr>\n      <th>test dataset</th>\n      <td>72</td>\n      <td>7</td>\n      <td>80</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_size=0.15\n",
    "target_variable=\"GSE_GWE\"\n",
    "# Load the data from the ETL output files\n",
    "X = read_and_join_output_file()\n",
    "X.drop([\"SHORTAGE_COUNT\"], inplace=True, axis=1)\n",
    "# Split the input pandas Dataframe into training and test datasets, applies the impute pipeline\n",
    "# transformation and reshapes the datasets to 3D (samples, time, features) numpy arrays\n",
    "X_train, X_test, y_train, y_test, _, _, _ = get_train_test_datasets(X, target_variable=target_variable, test_size=test_size, random_seed=RANDOM_SEED)\n",
    "nb_features = X_train.shape[-1]\n",
    "get_sets_shapes(X_train, X_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Hyperparameters Tuning\n",
    "For each of the 3 LSTM models architectures (from simplest to most complex), we use the Keras BayesianOptimization hyperparameters tuner to estimate the best values for the following hyperparameters:\n",
    "* the number of units for each *LSTM* or *Dense* unit\n",
    "* the activation function (*sigmoid*, *tanh*, *relu*) used for all layers, except the output layer which is fixed to a *linear* activation function.\n",
    "* the learning rate\n",
    "* the size of the validation dataset\n",
    "* the batch size\n",
    "* the number of epochs\n",
    "## Simple Model Hyper-parameter Tuning\n",
    "![Simple LSTM Model](../doc/images/lstm_architecture_1.jpg)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "class Model1(keras_tuner.HyperModel):\n",
    "    def build(self, hp):\n",
    "        model = Sequential()\n",
    "        hp_units = hp.Int(\"units\", min_value=10, max_value=300, step=10)\n",
    "        model.add(LSTM(units=hp_units, activation=\"sigmoid\", input_shape=(7, nb_features)))\n",
    "        model.add(Dense(1, activation=\"linear\"))\n",
    "        hp_learning_rate = hp.Choice(\"learning_rate\", values=[1e-2, 1e-3, 1e-4])\n",
    "        model.compile(loss=\"mse\", optimizer=Adam(learning_rate=hp_learning_rate), metrics=[keras.metrics.RootMeanSquaredError()])\n",
    "        return model\n",
    "\n",
    "    def fit(self, hp, model, *args, **kwargs):\n",
    "        return model.fit(\n",
    "            *args,\n",
    "            validation_split=hp.Choice(\"validation_split\", values=[0.05, 0.1, 0.15, 0.2]),\n",
    "            batch_size=hp.Int(\"batch_size\", min_value=32, max_value=192, step=16),\n",
    "            epochs=hp.Int(\"epochs\", min_value=30, max_value=500, step=5),\n",
    "            shuffle=True,\n",
    "            **kwargs,\n",
    "        )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 250 Complete [00h 00m 07s]\n",
      "val_root_mean_squared_error: 0.06851499527692795\n",
      "\n",
      "Best val_root_mean_squared_error So Far: 0.053719695657491684\n",
      "Total elapsed time: 00h 41m 12s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "stop_early = tensorflow.keras.callbacks.EarlyStopping(monitor='val_root_mean_squared_error', patience=10, verbose=1)\n",
    "tuner = keras_tuner.BayesianOptimization(Model1(),\n",
    "                             objective=keras_tuner.Objective(\"val_root_mean_squared_error\", direction=\"min\"),\n",
    "                             max_trials=250,\n",
    "                             beta=3.2,\n",
    "                             seed=RANDOM_SEED,\n",
    "                             overwrite=True,\n",
    "                             directory=\"keras_tuner\",\n",
    "                             project_name=\"model1_tuner\")\n",
    "tuner.search(X_train, y_train, callbacks=[stop_early])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Best Model Hyperparameters"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The hyperparameter search is complete.\n",
      "validation_split: 0.05\n",
      "lstm_units: 10\n",
      "learning_rate: 0.01\n",
      "batch_size: 32\n",
      "epochs: 500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the optimal hyperparameters\n",
    "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(f\"\"\"\n",
    "The hyperparameter search is complete.\n",
    "validation_split: {best_hps.get('validation_split')}\n",
    "lstm_units: {best_hps.get('units')}\n",
    "learning_rate: {best_hps.get('learning_rate')}\n",
    "batch_size: {best_hps.get('batch_size')}\n",
    "epochs: {best_hps.get('epochs')}\n",
    "\"\"\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Hyperparameters Tuning Summary"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results summary\n",
      "Results in keras_tuner\\model1_tuner\n",
      "Showing 10 best trials\n",
      "<keras_tuner.engine.objective.Objective object at 0x0000017EBF26CE80>\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 10\n",
      "learning_rate: 0.01\n",
      "validation_split: 0.05\n",
      "batch_size: 32\n",
      "epochs: 500\n",
      "Score: 0.053719695657491684\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 10\n",
      "learning_rate: 0.01\n",
      "validation_split: 0.05\n",
      "batch_size: 32\n",
      "epochs: 500\n",
      "Score: 0.054562896490097046\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 10\n",
      "learning_rate: 0.01\n",
      "validation_split: 0.05\n",
      "batch_size: 32\n",
      "epochs: 500\n",
      "Score: 0.05479402840137482\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 10\n",
      "learning_rate: 0.01\n",
      "validation_split: 0.05\n",
      "batch_size: 32\n",
      "epochs: 500\n",
      "Score: 0.05566919222474098\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 10\n",
      "learning_rate: 0.01\n",
      "validation_split: 0.05\n",
      "batch_size: 32\n",
      "epochs: 30\n",
      "Score: 0.0573287308216095\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 10\n",
      "learning_rate: 0.01\n",
      "validation_split: 0.05\n",
      "batch_size: 32\n",
      "epochs: 500\n",
      "Score: 0.05777304619550705\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 10\n",
      "learning_rate: 0.01\n",
      "validation_split: 0.05\n",
      "batch_size: 32\n",
      "epochs: 130\n",
      "Score: 0.05781768262386322\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 10\n",
      "learning_rate: 0.01\n",
      "validation_split: 0.05\n",
      "batch_size: 32\n",
      "epochs: 30\n",
      "Score: 0.05819150432944298\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 10\n",
      "learning_rate: 0.01\n",
      "validation_split: 0.05\n",
      "batch_size: 32\n",
      "epochs: 30\n",
      "Score: 0.05823618546128273\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 10\n",
      "learning_rate: 0.01\n",
      "validation_split: 0.05\n",
      "batch_size: 32\n",
      "epochs: 30\n",
      "Score: 0.05844183266162872\n"
     ]
    }
   ],
   "source": [
    "tuner.results_summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "###  Results\n",
    "Running the above BayesianOptimization and other hyperparameters tuning jobs and model tests, the best results on the validation set where obtained with a different set hyperparameters than the ones found by the above BayesianOptimization:\n",
    "* lstm_units: 160\n",
    "* lstm_activation: \"sigmoid\"\n",
    "* learning_rate: 0.001\n",
    "* validation_split: 0.1\n",
    "* batch_size: 128\n",
    "* epochs: 270\n",
    "## Model2 Hyper-parameter tuning\n",
    "![LSTM Model With Dense Layer](../doc/images/lstm_architecture_2.jpg)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "class Model2(keras_tuner.HyperModel):\n",
    "    def build(self, hp):\n",
    "        model = Sequential()\n",
    "        lstm_units = hp.Int(\"lstm_units\", min_value=10, max_value=300, step=10)\n",
    "        model.add(LSTM(units=lstm_units, activation=\"sigmoid\", input_shape=(7, nb_features)))\n",
    "        dense_units = hp.Int(\"dense_units\", min_value=11, max_value=101, step=2)\n",
    "        dense_activation = hp.Choice(\"dense_activation\", values=[\"relu\", \"tanh\", \"sigmoid\"])\n",
    "        model.add(Dense(dense_units, activation=dense_activation))\n",
    "        hp_dropout = hp.Float(\"dropout_rate\", min_value=0.05, max_value=0.25, step=0.05)\n",
    "        model.add(Dropout(hp_dropout))\n",
    "        model.add(Dense(1, activation=\"linear\"))\n",
    "        hp_learning_rate = hp.Choice(\"learning_rate\", values=[1e-2, 1e-3, 1e-4])\n",
    "        model.compile(loss=\"mse\", optimizer=Adam(learning_rate=hp_learning_rate), metrics=[keras.metrics.RootMeanSquaredError()])\n",
    "        return model\n",
    "\n",
    "    def fit(self, hp, model, *args, **kwargs):\n",
    "        return model.fit(\n",
    "            *args,\n",
    "            validation_split=hp.Choice(\"validation_split\", values=[0.05, 0.1, 0.15, 0.2]),\n",
    "            batch_size=hp.Int(\"batch_size\", min_value=32, max_value=192, step=16),\n",
    "            epochs=hp.Int(\"epochs\", min_value=30, max_value=500, step=5),\n",
    "            shuffle=True,\n",
    "            **kwargs,\n",
    "        )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 400 Complete [00h 00m 06s]\n",
      "val_root_mean_squared_error: 0.08698336780071259\n",
      "\n",
      "Best val_root_mean_squared_error So Far: 0.06801072508096695\n",
      "Total elapsed time: 03h 21m 41s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "stop_early = tensorflow.keras.callbacks.EarlyStopping(monitor=\"val_root_mean_squared_error\", patience=10, verbose=1)\n",
    "tuner = keras_tuner.BayesianOptimization(Model2(),\n",
    "                              objective=keras_tuner.Objective(\"val_root_mean_squared_error\", direction=\"min\"),\n",
    "                              max_trials=400,\n",
    "                              beta=3.2,\n",
    "                              seed=RANDOM_SEED,\n",
    "                              overwrite=True,\n",
    "                              directory=\"keras_tuner\",\n",
    "                              project_name=\"model2_tuner\")\n",
    "tuner.search(X_train, y_train, callbacks=[stop_early])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Best Model Hyperparameters"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The hyperparameter search is complete.\n",
      "validation_split: 0.05\n",
      "lstm_units: 10\n",
      "dense_units: 11\n",
      "dense_activation: relu\n",
      "dropout_rate: 0.15000000000000002\n",
      "learning_rate: 0.01\n",
      "batch_size: 32\n",
      "epochs: 30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the optimal hyperparameters\n",
    "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(f\"\"\"\n",
    "The hyperparameter search is complete.\n",
    "validation_split: {best_hps.get('validation_split')}\n",
    "lstm_units: {best_hps.get('lstm_units')}\n",
    "dense_units: {best_hps.get('dense_units')}\n",
    "dense_activation: {best_hps.get('dense_activation')}\n",
    "dropout_rate: {best_hps.get('dropout_rate')}\n",
    "learning_rate: {best_hps.get('learning_rate')}\n",
    "batch_size: {best_hps.get('batch_size')}\n",
    "epochs: {best_hps.get('epochs')}\n",
    "\"\"\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Hyperparameters Tuning Summary"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results summary\n",
      "Results in keras_tuner\\model2_tuner\n",
      "Showing 10 best trials\n",
      "<keras_tuner.engine.objective.Objective object at 0x0000017EC57F3FA0>\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "lstm_units: 10\n",
      "dense_units: 11\n",
      "dense_activation: relu\n",
      "dropout_rate: 0.15000000000000002\n",
      "learning_rate: 0.01\n",
      "validation_split: 0.05\n",
      "batch_size: 32\n",
      "epochs: 30\n",
      "Score: 0.06801072508096695\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "lstm_units: 10\n",
      "dense_units: 11\n",
      "dense_activation: relu\n",
      "dropout_rate: 0.15000000000000002\n",
      "learning_rate: 0.01\n",
      "validation_split: 0.05\n",
      "batch_size: 32\n",
      "epochs: 30\n",
      "Score: 0.06806003302335739\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "lstm_units: 10\n",
      "dense_units: 11\n",
      "dense_activation: relu\n",
      "dropout_rate: 0.1\n",
      "learning_rate: 0.01\n",
      "validation_split: 0.05\n",
      "batch_size: 32\n",
      "epochs: 30\n",
      "Score: 0.06849660724401474\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "lstm_units: 10\n",
      "dense_units: 11\n",
      "dense_activation: relu\n",
      "dropout_rate: 0.15000000000000002\n",
      "learning_rate: 0.01\n",
      "validation_split: 0.05\n",
      "batch_size: 32\n",
      "epochs: 30\n",
      "Score: 0.06947013735771179\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "lstm_units: 10\n",
      "dense_units: 11\n",
      "dense_activation: relu\n",
      "dropout_rate: 0.1\n",
      "learning_rate: 0.01\n",
      "validation_split: 0.05\n",
      "batch_size: 32\n",
      "epochs: 30\n",
      "Score: 0.06966453045606613\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "lstm_units: 10\n",
      "dense_units: 101\n",
      "dense_activation: relu\n",
      "dropout_rate: 0.15000000000000002\n",
      "learning_rate: 0.01\n",
      "validation_split: 0.05\n",
      "batch_size: 80\n",
      "epochs: 265\n",
      "Score: 0.070155069231987\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "lstm_units: 10\n",
      "dense_units: 11\n",
      "dense_activation: relu\n",
      "dropout_rate: 0.15000000000000002\n",
      "learning_rate: 0.01\n",
      "validation_split: 0.05\n",
      "batch_size: 32\n",
      "epochs: 30\n",
      "Score: 0.07027161866426468\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "lstm_units: 10\n",
      "dense_units: 11\n",
      "dense_activation: relu\n",
      "dropout_rate: 0.15000000000000002\n",
      "learning_rate: 0.01\n",
      "validation_split: 0.05\n",
      "batch_size: 32\n",
      "epochs: 30\n",
      "Score: 0.07070223987102509\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "lstm_units: 10\n",
      "dense_units: 11\n",
      "dense_activation: relu\n",
      "dropout_rate: 0.1\n",
      "learning_rate: 0.01\n",
      "validation_split: 0.05\n",
      "batch_size: 32\n",
      "epochs: 30\n",
      "Score: 0.0707276314496994\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "lstm_units: 10\n",
      "dense_units: 11\n",
      "dense_activation: relu\n",
      "dropout_rate: 0.15000000000000002\n",
      "learning_rate: 0.01\n",
      "validation_split: 0.05\n",
      "batch_size: 32\n",
      "epochs: 30\n",
      "Score: 0.07081308215856552\n"
     ]
    }
   ],
   "source": [
    "tuner.results_summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Results\n",
    "Running the above BayesianOptimization and other hyperparameters tuning jobs and model tests, the best results on the validation set where obtained with a different set hyperparameters than the ones found by the above BayesianOptimization:\n",
    "* validation_split: 0.1\n",
    "* lstm_units: 100\n",
    "* lstm_activation: sigmoid\n",
    "* dense_units: 11\n",
    "* dense_activations: tanh\n",
    "* dropout_rate: 0.1\n",
    "* learning_rate: 0.0001\n",
    "* batch_size: 32\n",
    "* epochs: 200\n",
    "## Model3 Hyper-parameter tuning\n",
    "![Encoder-Decoder LSTM Model](../doc/images/lstm_architecture_3.jpg)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "class Model3(keras_tuner.HyperModel):\n",
    "    def build(self, hp):\n",
    "        model = Sequential()\n",
    "        lstm_units = hp.Int(\"lstm_units\", min_value=10, max_value=300, step=10)\n",
    "        model.add(LSTM(units=lstm_units, activation=\"sigmoid\", input_shape=(7, nb_features)))\n",
    "        model.add(RepeatVector(1))\n",
    "        lstm_units_2 = hp.Int(\"2nd_lstm_units\", min_value=10, max_value=300, step=10)\n",
    "        model.add(LSTM(units=lstm_units_2, activation=\"sigmoid\", return_sequences=True))\n",
    "        dense_units = hp.Int(\"dense_units\", min_value=11, max_value=101, step=2)\n",
    "        dense_activation = hp.Choice(\"dense_activation\", values=[\"relu\", \"tanh\", \"sigmoid\"])\n",
    "        model.add(TimeDistributed(Dense(dense_units, activation=dense_activation)))\n",
    "        hp_dropout = hp.Float(\"dropout_rate\", min_value=0.05, max_value=0.25, step=0.05)\n",
    "        model.add(Dropout(hp_dropout))\n",
    "        model.add(Dense(1, activation=\"linear\"))\n",
    "        hp_learning_rate = hp.Choice(\"learning_rate\", values=[1e-2, 1e-3, 1e-4])\n",
    "        model.compile(loss=\"mse\", optimizer=Adam(learning_rate=hp_learning_rate), metrics=[keras.metrics.RootMeanSquaredError()])\n",
    "        return model\n",
    "\n",
    "    def fit(self, hp, model, *args, **kwargs):\n",
    "        return model.fit(\n",
    "            *args,\n",
    "            validation_split=hp.Choice(\"validation_split\", values=[0.05, 0.1, 0.15, 0.2]),\n",
    "            batch_size=hp.Int(\"batch_size\", min_value=32, max_value=192, step=16),\n",
    "            epochs=hp.Int(\"epochs\", min_value=30, max_value=500, step=5),\n",
    "            shuffle=True,\n",
    "            **kwargs,\n",
    "        )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 400 Complete [00h 00m 08s]\n",
      "val_root_mean_squared_error: 0.09001205861568451\n",
      "\n",
      "Best val_root_mean_squared_error So Far: 0.061044275760650635\n",
      "Total elapsed time: 03h 25m 35s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "stop_early = tensorflow.keras.callbacks.EarlyStopping(monitor=\"val_root_mean_squared_error\", patience=10, verbose=1)\n",
    "tuner = keras_tuner.BayesianOptimization(Model3(),\n",
    "                              objective=keras_tuner.Objective(\"val_root_mean_squared_error\", direction=\"min\"),\n",
    "                              max_trials=400,\n",
    "                              beta=3.2,\n",
    "                              seed=RANDOM_SEED,\n",
    "                              overwrite=True,\n",
    "                              directory=\"keras_tuner\",\n",
    "                              project_name=\"model3_tuner\")\n",
    "tuner.search(X_train, y_train, callbacks=[stop_early])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Best Model Hyperparameters"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The hyperparameter search is complete.\n",
      "validation_split: 0.05\n",
      "lstm_units: 300\n",
      "2nd_lstm_units: 300\n",
      "dense_units: 11\n",
      "dense_activation: sigmoid\n",
      "dropout_rate: 0.05\n",
      "learning_rate: 0.01\n",
      "batch_size: 32\n",
      "epochs: 500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the optimal hyperparameters\n",
    "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(f\"\"\"\n",
    "The hyperparameter search is complete.\n",
    "validation_split: {best_hps.get('validation_split')}\n",
    "lstm_units: {best_hps.get('lstm_units')}\n",
    "2nd_lstm_units: {best_hps.get('2nd_lstm_units')}\n",
    "dense_units: {best_hps.get('dense_units')}\n",
    "dense_activation: {best_hps.get('dense_activation')}\n",
    "dropout_rate: {best_hps.get('dropout_rate')}\n",
    "learning_rate: {best_hps.get('learning_rate')}\n",
    "batch_size: {best_hps.get('batch_size')}\n",
    "epochs: {best_hps.get('epochs')}\n",
    "\"\"\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Hyperparameters Tuning Summary"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results summary\n",
      "Results in keras_tuner\\model3_tuner\n",
      "Showing 10 best trials\n",
      "<keras_tuner.engine.objective.Objective object at 0x0000017EBF298850>\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "lstm_units: 300\n",
      "2nd_lstm_units: 300\n",
      "dense_units: 11\n",
      "dense_activation: sigmoid\n",
      "dropout_rate: 0.05\n",
      "learning_rate: 0.01\n",
      "validation_split: 0.05\n",
      "batch_size: 32\n",
      "epochs: 500\n",
      "Score: 0.061044275760650635\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "lstm_units: 300\n",
      "2nd_lstm_units: 180\n",
      "dense_units: 57\n",
      "dense_activation: sigmoid\n",
      "dropout_rate: 0.25\n",
      "learning_rate: 0.01\n",
      "validation_split: 0.05\n",
      "batch_size: 32\n",
      "epochs: 360\n",
      "Score: 0.07467430830001831\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "lstm_units: 300\n",
      "2nd_lstm_units: 300\n",
      "dense_units: 11\n",
      "dense_activation: sigmoid\n",
      "dropout_rate: 0.25\n",
      "learning_rate: 0.01\n",
      "validation_split: 0.05\n",
      "batch_size: 32\n",
      "epochs: 155\n",
      "Score: 0.07538601011037827\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "lstm_units: 300\n",
      "2nd_lstm_units: 300\n",
      "dense_units: 11\n",
      "dense_activation: sigmoid\n",
      "dropout_rate: 0.05\n",
      "learning_rate: 0.01\n",
      "validation_split: 0.05\n",
      "batch_size: 32\n",
      "epochs: 500\n",
      "Score: 0.0787382572889328\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "lstm_units: 300\n",
      "2nd_lstm_units: 270\n",
      "dense_units: 45\n",
      "dense_activation: sigmoid\n",
      "dropout_rate: 0.2\n",
      "learning_rate: 0.01\n",
      "validation_split: 0.05\n",
      "batch_size: 32\n",
      "epochs: 235\n",
      "Score: 0.08074501901865005\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "lstm_units: 300\n",
      "2nd_lstm_units: 300\n",
      "dense_units: 31\n",
      "dense_activation: sigmoid\n",
      "dropout_rate: 0.2\n",
      "learning_rate: 0.01\n",
      "validation_split: 0.05\n",
      "batch_size: 32\n",
      "epochs: 205\n",
      "Score: 0.08126538246870041\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "lstm_units: 300\n",
      "2nd_lstm_units: 300\n",
      "dense_units: 45\n",
      "dense_activation: sigmoid\n",
      "dropout_rate: 0.15000000000000002\n",
      "learning_rate: 0.01\n",
      "validation_split: 0.05\n",
      "batch_size: 32\n",
      "epochs: 500\n",
      "Score: 0.08250907808542252\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "lstm_units: 300\n",
      "2nd_lstm_units: 230\n",
      "dense_units: 55\n",
      "dense_activation: sigmoid\n",
      "dropout_rate: 0.2\n",
      "learning_rate: 0.01\n",
      "validation_split: 0.05\n",
      "batch_size: 32\n",
      "epochs: 280\n",
      "Score: 0.08296287059783936\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "lstm_units: 300\n",
      "2nd_lstm_units: 300\n",
      "dense_units: 45\n",
      "dense_activation: sigmoid\n",
      "dropout_rate: 0.2\n",
      "learning_rate: 0.01\n",
      "validation_split: 0.05\n",
      "batch_size: 32\n",
      "epochs: 170\n",
      "Score: 0.08399957418441772\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "lstm_units: 10\n",
      "2nd_lstm_units: 10\n",
      "dense_units: 101\n",
      "dense_activation: relu\n",
      "dropout_rate: 0.25\n",
      "learning_rate: 0.01\n",
      "validation_split: 0.1\n",
      "batch_size: 32\n",
      "epochs: 500\n",
      "Score: 0.08407008647918701\n"
     ]
    }
   ],
   "source": [
    "tuner.results_summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Results\n",
    "Running the above BayesianOptimization and other hyperparameters tuning jobs and model tests, the best results on the validation set where obtained with a different set hyperparameters than the ones found by the above BayesianOptimization:\n",
    "* validation_split: 0.1\n",
    "* lstm_units: 300\n",
    "* lstm_activation: sigmoid\n",
    "* 2nd_lstm_units: 140\n",
    "* 2nd_lstm_activation: sigmoid\n",
    "* dense_units: 21\n",
    "* dense_activations: tanh\n",
    "* dropout_rate: 0.2\n",
    "* learning_rate: 0.001\n",
    "* batch_size: 32\n",
    "* epochs: 200"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "deepnote_notebook_id": "5bae3659-fae4-4d12-b1ea-64cab0ec1541",
  "deepnote": {},
  "deepnote_execution_queue": []
 }
}