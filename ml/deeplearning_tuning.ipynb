{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# Deeplearning LSTM Model Hyperpamaters Tuning",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "cell_id": "18337427-36ce-4fe5-8247-a76920367c21",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "cell_id": "00001-06fa5675-9b65-4e8a-8cfa-9cac63e669d9",
    "deepnote_cell_type": "code"
   },
   "source": "import sys\nsys.path.append('..')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "cell_id": "00002-c8998ab4-cdb1-423c-b355-71b104dc2263",
    "deepnote_cell_type": "code"
   },
   "source": "import numpy as np\nimport random\nimport tensorflow\nimport keras_tuner\n\nfrom tensorflow import keras\nfrom keras.models import Sequential\nfrom keras.layers import LSTM, Dense, Dropout, TimeDistributed, RepeatVector\nfrom keras.optimizers import Adam\nfrom lib.read_data import read_and_join_output_file\nfrom lib.deeplearning import get_train_test_datasets,  get_sets_shapes",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "cell_id": "00003-cd907517-9663-4391-ac26-29e95188437a",
    "deepnote_cell_type": "code"
   },
   "source": "RANDOM_SEED = 31\nrandom.seed(RANDOM_SEED)\nnp.random.seed(RANDOM_SEED)\ntensorflow.random.set_seed(RANDOM_SEED)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Preparing the Dataset\nThe dataset is prepared as explained in the /ml/deeplearning.ipynb notebook. Please refer to it for more details. As a summary:\n* The train and test sets are split by Township-Ranges, i.e. some Township-Ranges data are either fully in the train or test set.\n* The target value is the value of that variable for 2021\n* Data are imputed using a custom pipeline\n\nThe resulting train and test sets are of shape [number of Township-Ranges, 7 years (2014-2020), the number of features].\nWe do not create a validation dataset as we use Keras internal cross-validation mechanism to shuffle the data points (i.e., the Township-Ranges) and keep some for the validation at each training epoch.",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "cell_id": "00004-afd3bb8d-a214-4047-a2a6-49c4025f1e5e",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "cell_id": "00005-424c9697-01b5-4f1f-8070-dda712b9fd6b",
    "deepnote_cell_type": "code"
   },
   "source": "test_size=0.15\ntarget_variable=\"GSE_GWE\"\n# Load the data from the ETL output files\nX = read_and_join_output_file()\n# Split the input pandas Dataframe into training and test datasets, applies the impute pipeline\n# transformation and reshapes the datasets to 3D (samples, time, features) numpy arrays\nX_train, X_test, y_train, y_test, _, _ = get_train_test_datasets(X, target_variable=target_variable, test_size=test_size, random_seed=RANDOM_SEED)\nnb_features = X_train.shape[-1]\nget_sets_shapes(X_train, X_test)",
   "outputs": [
    {
     "data": {
      "text/plain": "                  nb_items  nb_timestamps  nb_features\ntraining dataset       406              7           82\ntest dataset            72              7           82",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>nb_items</th>\n      <th>nb_timestamps</th>\n      <th>nb_features</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>training dataset</th>\n      <td>406</td>\n      <td>7</td>\n      <td>82</td>\n    </tr>\n    <tr>\n      <th>test dataset</th>\n      <td>72</td>\n      <td>7</td>\n      <td>82</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Hyperparameters Tuning\nFor each of the 3 LSTM models architectures (from simplest to most complex), we use the Keras BayesianOptimization hyperparameters tuner to estimate the best values for the following hyperparameters:\n* the number of units for each *LSTM* or *Dense* unit\n* the activation function (*sigmoid*, *tanh*, *relu*) used for all layers, except the output layer which is fixed to a *linear* activation function.\n* the learning rate\n* the size of the validation dataset\n* the batch size\n* the number of epochs\n## Simple Model Hyper-parameter Tuning\n![Simple LSTM Model](../doc/images/deeplearning-architecture-1.jpg)",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "cell_id": "00006-2a6dc8a2-5f2f-4fd4-bb28-f2d9600d0e1b",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "cell_id": "00007-3fd801cd-f94d-4ab6-bc90-1b5bb20ece44",
    "deepnote_cell_type": "code"
   },
   "source": "class Model1(keras_tuner.HyperModel):\n    def build(self, hp):\n        model = Sequential()\n        hp_units = hp.Int(\"units\", min_value=10, max_value=300, step=10)\n        hp_activ = hp.Choice(\"activation\", values=[\"tanh\", \"sigmoid\"])\n        model.add(LSTM(units=hp_units, activation=hp_activ, input_shape=(7, nb_features)))\n        model.add(Dense(1, activation=\"linear\"))\n        hp_learning_rate = hp.Choice(\"learning_rate\", values=[1e-2, 1e-3, 1e-4])\n        model.compile(loss=\"mse\", optimizer=Adam(learning_rate=hp_learning_rate), metrics=[keras.metrics.RootMeanSquaredError()])\n        return model\n\n    def fit(self, hp, model, *args, **kwargs):\n        return model.fit(\n            *args,\n            validation_split=hp.Choice(\"validation_split\", values=[0.05, 0.1, 0.15, 0.2]),\n            batch_size=hp.Int(\"batch_size\", min_value=32, max_value=192, step=16),\n            epochs=hp.Int(\"epochs\", min_value=30, max_value=500, step=5),\n            shuffle=True,\n            **kwargs,\n        )",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "cell_id": "00008-9f15ed15-72c9-4e95-9d2c-385dc9f17fe0",
    "deepnote_cell_type": "code"
   },
   "source": "stop_early = tensorflow.keras.callbacks.EarlyStopping(monitor='val_root_mean_squared_error', patience=10, verbose=1)\ntuner = keras_tuner.BayesianOptimization(Model1(),\n                             objective=keras_tuner.Objective(\"val_root_mean_squared_error\", direction=\"min\"),\n                             max_trials=250,\n                             beta=3.2,\n                             seed=RANDOM_SEED,\n                             overwrite=True,\n                             directory=\"keras_tuner\",\n                             project_name=\"model1_tuner\")\ntuner.search(X_train, y_train, callbacks=[stop_early])",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Trial 150 Complete [00h 00m 31s]\nval_root_mean_squared_error: 0.08564456552267075\n\nBest val_root_mean_squared_error So Far: 0.0542435497045517\nTotal elapsed time: 01h 10m 42s\n\nSearch: Running Trial #151\n\nValue             |Best Value So Far |Hyperparameter\n300               |120               |units\ntanh              |sigmoid           |activation\n0.0001            |0.01              |learning_rate\n0.2               |0.05              |validation_split\n80                |32                |batch_size\n440               |500               |epochs\n\nEpoch 1/440\n5/5 [==============================] - 2s 183ms/step - loss: 0.0641 - root_mean_squared_error: 0.2533 - val_loss: 0.0844 - val_root_mean_squared_error: 0.2905\nEpoch 2/440\n5/5 [==============================] - 0s 99ms/step - loss: 0.0405 - root_mean_squared_error: 0.2011 - val_loss: 0.0566 - val_root_mean_squared_error: 0.2380\nEpoch 3/440\n5/5 [==============================] - 0s 95ms/step - loss: 0.0331 - root_mean_squared_error: 0.1818 - val_loss: 0.0454 - val_root_mean_squared_error: 0.2132\nEpoch 4/440\n5/5 [==============================] - 0s 100ms/step - loss: 0.0296 - root_mean_squared_error: 0.1720 - val_loss: 0.0402 - val_root_mean_squared_error: 0.2005\nEpoch 5/440\n5/5 [==============================] - 0s 94ms/step - loss: 0.0264 - root_mean_squared_error: 0.1626 - val_loss: 0.0394 - val_root_mean_squared_error: 0.1986\nEpoch 6/440\n5/5 [==============================] - 0s 50ms/step - loss: 0.0224 - root_mean_squared_error: 0.1498 - val_loss: 0.0402 - val_root_mean_squared_error: 0.2006\nEpoch 7/440\n5/5 [==============================] - 0s 51ms/step - loss: 0.0193 - root_mean_squared_error: 0.1390 - val_loss: 0.0404 - val_root_mean_squared_error: 0.2010\nEpoch 8/440\n5/5 [==============================] - 0s 90ms/step - loss: 0.0171 - root_mean_squared_error: 0.1309 - val_loss: 0.0386 - val_root_mean_squared_error: 0.1966\nEpoch 9/440\n5/5 [==============================] - 0s 97ms/step - loss: 0.0154 - root_mean_squared_error: 0.1239 - val_loss: 0.0355 - val_root_mean_squared_error: 0.1883\nEpoch 10/440\n5/5 [==============================] - 0s 98ms/step - loss: 0.0137 - root_mean_squared_error: 0.1169 - val_loss: 0.0331 - val_root_mean_squared_error: 0.1819\nEpoch 11/440\n5/5 [==============================] - 0s 90ms/step - loss: 0.0123 - root_mean_squared_error: 0.1110 - val_loss: 0.0307 - val_root_mean_squared_error: 0.1754\nEpoch 12/440\n"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 10>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m stop_early \u001b[38;5;241m=\u001b[39m tensorflow\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mEarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_root_mean_squared_error\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      2\u001b[0m tuner \u001b[38;5;241m=\u001b[39m keras_tuner\u001b[38;5;241m.\u001b[39mBayesianOptimization(Model1(),\n\u001b[0;32m      3\u001b[0m                              objective\u001b[38;5;241m=\u001b[39mkeras_tuner\u001b[38;5;241m.\u001b[39mObjective(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_root_mean_squared_error\u001b[39m\u001b[38;5;124m\"\u001b[39m, direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m      4\u001b[0m                              max_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m250\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      8\u001b[0m                              directory\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras_tuner\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      9\u001b[0m                              project_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel1_tuner\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 10\u001b[0m \u001b[43mtuner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mstop_early\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\milestone2\\lib\\site-packages\\keras_tuner\\engine\\base_tuner.py:179\u001b[0m, in \u001b[0;36mBaseTuner.search\u001b[1;34m(self, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[0;32m    176\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_trial_begin(trial)\n\u001b[1;32m--> 179\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_trial(trial, \u001b[38;5;241m*\u001b[39mfit_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_kwargs)\n\u001b[0;32m    180\u001b[0m \u001b[38;5;66;03m# `results` is None indicates user updated oracle in `run_trial()`.\u001b[39;00m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m results \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\milestone2\\lib\\site-packages\\keras_tuner\\engine\\tuner.py:294\u001b[0m, in \u001b[0;36mTuner.run_trial\u001b[1;34m(self, trial, *args, **kwargs)\u001b[0m\n\u001b[0;32m    292\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(model_checkpoint)\n\u001b[0;32m    293\u001b[0m     copied_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m callbacks\n\u001b[1;32m--> 294\u001b[0m     obj_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_and_fit_model(trial, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcopied_kwargs)\n\u001b[0;32m    296\u001b[0m     histories\u001b[38;5;241m.\u001b[39mappend(obj_value)\n\u001b[0;32m    297\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m histories\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\milestone2\\lib\\site-packages\\keras_tuner\\engine\\tuner.py:222\u001b[0m, in \u001b[0;36mTuner._build_and_fit_model\u001b[1;34m(self, trial, *args, **kwargs)\u001b[0m\n\u001b[0;32m    220\u001b[0m hp \u001b[38;5;241m=\u001b[39m trial\u001b[38;5;241m.\u001b[39mhyperparameters\n\u001b[0;32m    221\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_build(hp)\n\u001b[1;32m--> 222\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhypermodel\u001b[38;5;241m.\u001b[39mfit(hp, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tuner_utils\u001b[38;5;241m.\u001b[39mconvert_to_metrics_dict(\n\u001b[0;32m    224\u001b[0m     results, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moracle\u001b[38;5;241m.\u001b[39mobjective, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHyperModel.fit()\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    225\u001b[0m )\n",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36mModel1.fit\u001b[1;34m(self, hp, model, *args, **kwargs)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, hp, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m---> 13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[0;32m     14\u001b[0m         \u001b[38;5;241m*\u001b[39margs,\n\u001b[0;32m     15\u001b[0m         validation_split\u001b[38;5;241m=\u001b[39mhp\u001b[38;5;241m.\u001b[39mChoice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_split\u001b[39m\u001b[38;5;124m\"\u001b[39m, values\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.05\u001b[39m, \u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m0.15\u001b[39m, \u001b[38;5;241m0.2\u001b[39m]),\n\u001b[0;32m     16\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mhp\u001b[38;5;241m.\u001b[39mInt(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m, min_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, max_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m192\u001b[39m, step\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m),\n\u001b[0;32m     17\u001b[0m         epochs\u001b[38;5;241m=\u001b[39mhp\u001b[38;5;241m.\u001b[39mInt(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m\"\u001b[39m, min_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m, max_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m, step\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m),\n\u001b[0;32m     18\u001b[0m         shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     19\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m     20\u001b[0m     )\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\milestone2\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\milestone2\\lib\\site-packages\\keras\\engine\\training.py:1409\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1402\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1403\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   1404\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   1405\u001b[0m     step_num\u001b[38;5;241m=\u001b[39mstep,\n\u001b[0;32m   1406\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   1407\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m   1408\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1409\u001b[0m   tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1410\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1411\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\milestone2\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\milestone2\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\milestone2\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateless_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\milestone2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2453\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2450\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2451\u001b[0m   (graph_function,\n\u001b[0;32m   2452\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2453\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2454\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\milestone2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1860\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1856\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1857\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1858\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1859\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1860\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1861\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1862\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1863\u001b[0m     args,\n\u001b[0;32m   1864\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1865\u001b[0m     executing_eagerly)\n\u001b[0;32m   1866\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\milestone2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:497\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    495\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    496\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 497\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    503\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    504\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    505\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    506\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    509\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    510\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\milestone2\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "### Best Model Hyperparameters",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "cell_id": "00009-0200b6d7-bc19-4da6-afd7-17b040876d98",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "cell_id": "00010-f786804d-293d-4ace-958d-131868f86749",
    "deepnote_cell_type": "code"
   },
   "source": "# Get the optimal hyperparameters\nbest_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\nprint(f\"\"\"\nThe hyperparameter search is complete.\nvalidation_split: {best_hps.get('validation_split')}\nlstm_units: {best_hps.get('units')}\nlstm_activation: {best_hps.get('activation')}\nlearning_rate: {best_hps.get('learning_rate')}\nbatch_size: {best_hps.get('batch_size')}\nepochs: {best_hps.get('epochs')}\n\"\"\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Hyperparameters Tuning Summary",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "cell_id": "00011-13c828c6-c4a2-4911-97de-efe7821bdcb0",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "cell_id": "00012-b2f19576-a584-40fc-8bed-e3c72882dc12",
    "deepnote_cell_type": "code"
   },
   "source": "tuner.results_summary()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Model2 Hyper-parameter tuning\n![LSTM Model With Dense Layer](../doc/images/deeplearning-architecture-2.jpg)",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "cell_id": "00013-27edb75d-525b-4f50-9b49-85ae39a6cea7",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "cell_id": "00014-8a33dfb3-8b44-4ba0-866d-ca348e663e2c",
    "deepnote_cell_type": "code"
   },
   "source": "class Model2(keras_tuner.HyperModel):\n    def build(self, hp):\n        model = Sequential()\n        lstm_units = hp.Int(\"lstm_units\", min_value=10, max_value=300, step=10)\n        lstm_activ = hp.Choice(\"lstm_activation\", values=[\"tanh\", \"sigmoid\"])\n        model.add(LSTM(units=lstm_units, activation=lstm_activ, input_shape=(7, nb_features)))\n        dense_units = hp.Int(\"dense_units\", min_value=11, max_value=101, step=2)\n        dense_activation = hp.Choice(\"dense_activation\", values=[\"relu\", \"tanh\", \"sigmoid\"])\n        model.add(Dense(dense_units, activation=dense_activation))\n        hp_dropout = hp.Float(\"dropout_rate\", min_value=0.05, max_value=0.25, step=0.05)\n        model.add(Dropout(hp_dropout))\n        model.add(Dense(1, activation=\"linear\"))\n        hp_learning_rate = hp.Choice(\"learning_rate\", values=[1e-2, 1e-3, 1e-4])\n        model.compile(loss=\"mse\", optimizer=Adam(learning_rate=hp_learning_rate), metrics=[keras.metrics.RootMeanSquaredError()])\n        return model\n\n    def fit(self, hp, model, *args, **kwargs):\n        return model.fit(\n            *args,\n            validation_split=hp.Choice(\"validation_split\", values=[0.05, 0.1, 0.15, 0.2]),\n            batch_size=hp.Int(\"batch_size\", min_value=32, max_value=192, step=16),\n            epochs=hp.Int(\"epochs\", min_value=30, max_value=500, step=5),\n            shuffle=True,\n            **kwargs,\n        )",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "cell_id": "00015-fec98965-d47c-4f91-8571-5ab7c6659785",
    "deepnote_cell_type": "code"
   },
   "source": "stop_early = tensorflow.keras.callbacks.EarlyStopping(monitor=\"val_root_mean_squared_error\", patience=10, verbose=1)\ntuner = keras_tuner.BayesianOptimization(Model2(),\n                              objective=keras_tuner.Objective(\"val_root_mean_squared_error\", direction=\"min\"),\n                              max_trials=400,\n                              beta=3.2,\n                              seed=RANDOM_SEED,\n                              overwrite=True,\n                              directory=\"keras_tuner\",\n                              project_name=\"model2_tuner\")\ntuner.search(X_train, y_train, callbacks=[stop_early])",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Trial 400 Complete [00h 00m 06s]\nval_root_mean_squared_error: 0.08002614974975586\n\nBest val_root_mean_squared_error So Far: 0.06468775868415833\nTotal elapsed time: 03h 14m 13s\nINFO:tensorflow:Oracle triggered exit\n"
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "### Best Model Hyperparameters",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "cell_id": "00016-f9158a8d-771c-4e14-9c82-6c685c138468",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "cell_id": "00017-fad0f832-adc4-4518-bae4-2d1021228051",
    "deepnote_cell_type": "code"
   },
   "source": "# Get the optimal hyperparameters\nbest_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\nprint(f\"\"\"\nThe hyperparameter search is complete.\nvalidation_split: {best_hps.get('validation_split')}\nlstm_units: {best_hps.get('lstm_units')}\nlstm_activation: {best_hps.get('lstm_activation')}\ndense_units: {best_hps.get('dense_units')}\ndense_activation: {best_hps.get('dense_activation')}\ndropout_rate: {best_hps.get('dropout_rate')}\nlearning_rate: {best_hps.get('learning_rate')}\nbatch_size: {best_hps.get('batch_size')}\nepochs: {best_hps.get('epochs')}\n\"\"\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\nThe hyperparameter search is complete.\nvalidation_split: 0.05\nlstm_units: 10\nlstm_activation: sigmoid\ndense_units: 11\ndense_activation: relu\ndropout_rate: 0.05\nlearning_rate: 0.01\nbatch_size: 32\nepochs: 500\n\n"
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "### Hyperparameters Tuning Summary",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "cell_id": "00018-dc5f7e68-5fcf-4a48-a757-3ca6463f9704",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "cell_id": "00019-5f9ba635-abbf-4461-965a-5611cd2a8622",
    "deepnote_cell_type": "code"
   },
   "source": "tuner.results_summary()",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Results summary\nResults in keras_tuner\\model2_tuner\nShowing 10 best trials\n<keras_tuner.engine.objective.Objective object at 0x00000220D8130B50>\nTrial summary\nHyperparameters:\nlstm_units: 10\nlstm_activation: sigmoid\ndense_units: 11\ndense_activation: relu\ndropout_rate: 0.05\nlearning_rate: 0.01\nvalidation_split: 0.05\nbatch_size: 32\nepochs: 500\nScore: 0.06468775868415833\nTrial summary\nHyperparameters:\nlstm_units: 10\nlstm_activation: sigmoid\ndense_units: 11\ndense_activation: relu\ndropout_rate: 0.1\nlearning_rate: 0.01\nvalidation_split: 0.05\nbatch_size: 32\nepochs: 500\nScore: 0.06481722742319107\nTrial summary\nHyperparameters:\nlstm_units: 10\nlstm_activation: sigmoid\ndense_units: 11\ndense_activation: relu\ndropout_rate: 0.15000000000000002\nlearning_rate: 0.01\nvalidation_split: 0.05\nbatch_size: 32\nepochs: 415\nScore: 0.06549284607172012\nTrial summary\nHyperparameters:\nlstm_units: 10\nlstm_activation: sigmoid\ndense_units: 11\ndense_activation: relu\ndropout_rate: 0.15000000000000002\nlearning_rate: 0.01\nvalidation_split: 0.05\nbatch_size: 32\nepochs: 415\nScore: 0.06606409698724747\nTrial summary\nHyperparameters:\nlstm_units: 10\nlstm_activation: sigmoid\ndense_units: 57\ndense_activation: relu\ndropout_rate: 0.2\nlearning_rate: 0.01\nvalidation_split: 0.05\nbatch_size: 32\nepochs: 145\nScore: 0.06691084057092667\nTrial summary\nHyperparameters:\nlstm_units: 10\nlstm_activation: sigmoid\ndense_units: 11\ndense_activation: relu\ndropout_rate: 0.05\nlearning_rate: 0.01\nvalidation_split: 0.05\nbatch_size: 32\nepochs: 500\nScore: 0.06699465960264206\nTrial summary\nHyperparameters:\nlstm_units: 10\nlstm_activation: sigmoid\ndense_units: 11\ndense_activation: relu\ndropout_rate: 0.15000000000000002\nlearning_rate: 0.01\nvalidation_split: 0.05\nbatch_size: 32\nepochs: 410\nScore: 0.06715192645788193\nTrial summary\nHyperparameters:\nlstm_units: 10\nlstm_activation: sigmoid\ndense_units: 11\ndense_activation: relu\ndropout_rate: 0.05\nlearning_rate: 0.01\nvalidation_split: 0.05\nbatch_size: 32\nepochs: 500\nScore: 0.06768625229597092\nTrial summary\nHyperparameters:\nlstm_units: 10\nlstm_activation: sigmoid\ndense_units: 21\ndense_activation: relu\ndropout_rate: 0.05\nlearning_rate: 0.01\nvalidation_split: 0.05\nbatch_size: 32\nepochs: 360\nScore: 0.06787063926458359\nTrial summary\nHyperparameters:\nlstm_units: 10\nlstm_activation: sigmoid\ndense_units: 11\ndense_activation: relu\ndropout_rate: 0.05\nlearning_rate: 0.01\nvalidation_split: 0.05\nbatch_size: 32\nepochs: 500\nScore: 0.06824542582035065\n"
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Model3 Hyper-parameter tuning\n![Encoder-Decoder LSTM Model](../doc/images/deeplearning-architecture-3.jpg)",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "cell_id": "00020-703c2022-351e-4bed-99c8-a005fe94b7ba",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "cell_id": "00021-32af92af-f851-4678-9030-8809de1772e3",
    "deepnote_cell_type": "code"
   },
   "source": "class Model3(keras_tuner.HyperModel):\n    def build(self, hp):\n        model = Sequential()\n        lstm_units = hp.Int(\"lstm_units\", min_value=10, max_value=300, step=10)\n        lstm_activ = hp.Choice(\"lstm_activation\", values=[\"tanh\", \"sigmoid\"])\n        model.add(LSTM(units=lstm_units, activation=\"sigmoid\", input_shape=(7, nb_features)))\n        model.add(RepeatVector(1))\n        lstm_units_2 = hp.Int(\"2nd_lstm_units\", min_value=10, max_value=300, step=10)\n        lstm_activ_2 = hp.Choice(\"2nd_lstm_activation\", values=[\"tanh\", \"sigmoid\"])\n        model.add(LSTM(units=lstm_units_2, activation=\"sigmoid\", return_sequences=True))\n        dense_units = hp.Int(\"dense_units\", min_value=11, max_value=101, step=2)\n        dense_activation = hp.Choice(\"dense_activation\", values=[\"relu\", \"tanh\", \"sigmoid\"])\n        model.add(TimeDistributed(Dense(dense_units, activation=dense_activation)))\n        hp_dropout = hp.Float(\"dropout_rate\", min_value=0.05, max_value=0.25, step=0.05)\n        model.add(Dropout(hp_dropout))\n        model.add(Dense(1, activation=\"linear\"))\n        hp_learning_rate = hp.Choice(\"learning_rate\", values=[1e-2, 1e-3, 1e-4])\n        model.compile(loss=\"mse\", optimizer=Adam(learning_rate=hp_learning_rate), metrics=[keras.metrics.RootMeanSquaredError()])\n        return model\n\n    def fit(self, hp, model, *args, **kwargs):\n        return model.fit(\n            *args,\n            validation_split=hp.Choice(\"validation_split\", values=[0.05, 0.1, 0.15, 0.2]),\n            batch_size=hp.Int(\"batch_size\", min_value=32, max_value=192, step=16),\n            epochs=hp.Int(\"epochs\", min_value=30, max_value=500, step=5),\n            shuffle=True,\n            **kwargs,\n        )",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "cell_id": "00022-bb6bc21f-3ca6-4747-8dee-46a69cf66d98",
    "deepnote_cell_type": "code"
   },
   "source": "stop_early = tensorflow.keras.callbacks.EarlyStopping(monitor=\"val_root_mean_squared_error\", patience=10, verbose=1)\ntuner = keras_tuner.BayesianOptimization(Model3(),\n                              objective=keras_tuner.Objective(\"val_root_mean_squared_error\", direction=\"min\"),\n                              max_trials=400,\n                              beta=3.2,\n                              seed=RANDOM_SEED,\n                              overwrite=True,\n                              directory=\"keras_tuner\",\n                              project_name=\"model3_tuner\")\ntuner.search(X_train, y_train, callbacks=[stop_early])",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Trial 500 Complete [00h 00m 18s]\nval_root_mean_squared_error: 0.1150362491607666\n\nBest val_root_mean_squared_error So Far: 0.06309118866920471\nTotal elapsed time: 07h 29m 36s\nINFO:tensorflow:Oracle triggered exit\n"
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "### Best Model Hyperparameters",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "cell_id": "00023-79aebc71-ff28-49e0-976e-a004f1c79cd3",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "cell_id": "00024-c3e304c2-21cf-4acb-98de-c692e28c3023",
    "deepnote_cell_type": "code"
   },
   "source": "# Get the optimal hyperparameters\nbest_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\nprint(f\"\"\"\nThe hyperparameter search is complete.\nvalidation_split: {best_hps.get('validation_split')}\nlstm_units: {best_hps.get('lstm_units')}\nlstm_activation: {best_hps.get('lstm_activation')}\n2nd_lstm_units: {best_hps.get('2nd_lstm_units')}\n2nd_lstm_activation: {best_hps.get('2nd_lstm_activation')}\ndense_units: {best_hps.get('dense_units')}\ndense_activation: {best_hps.get('dense_activation')}\ndropout_rate: {best_hps.get('dropout_rate')}\nlearning_rate: {best_hps.get('learning_rate')}\nbatch_size: {best_hps.get('batch_size')}\nepochs: {best_hps.get('epochs')}\n\"\"\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\nThe hyperparameter search is complete.\nvalidation_split: 0.05\nlstm_units: 300\nlstm_activation: sigmoid\n2nd_lstm_units: 300\n2nd_lstm_activation: tanh\ndense_units: 11\ndense_activation: sigmoid\ndropout_rate: 0.05\nlearning_rate: 0.01\nbatch_size: 32\nepochs: 30\n\n"
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "### Hyperparameters Tuning Summary",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "cell_id": "00025-889ac36a-4641-4f9a-846d-8a180bd833e0",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "cell_id": "00026-58fce9d5-882a-4a43-b850-16a4db6de4b5",
    "deepnote_cell_type": "code"
   },
   "source": "tuner.results_summary()",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Results summary\nResults in keras_tuner\\model3_tuner\nShowing 10 best trials\n<keras_tuner.engine.objective.Objective object at 0x000002575F038250>\nTrial summary\nHyperparameters:\nlstm_units: 300\nlstm_activation: sigmoid\n2nd_lstm_units: 300\n2nd_lstm_activation: tanh\ndense_units: 11\ndense_activation: sigmoid\ndropout_rate: 0.05\nlearning_rate: 0.01\nvalidation_split: 0.05\nbatch_size: 32\nepochs: 30\nScore: 0.06309118866920471\nTrial summary\nHyperparameters:\nlstm_units: 300\nlstm_activation: sigmoid\n2nd_lstm_units: 300\n2nd_lstm_activation: tanh\ndense_units: 11\ndense_activation: sigmoid\ndropout_rate: 0.05\nlearning_rate: 0.01\nvalidation_split: 0.05\nbatch_size: 32\nepochs: 30\nScore: 0.06499446928501129\nTrial summary\nHyperparameters:\nlstm_units: 300\nlstm_activation: sigmoid\n2nd_lstm_units: 300\n2nd_lstm_activation: sigmoid\ndense_units: 11\ndense_activation: sigmoid\ndropout_rate: 0.05\nlearning_rate: 0.01\nvalidation_split: 0.05\nbatch_size: 32\nepochs: 30\nScore: 0.06594016402959824\nTrial summary\nHyperparameters:\nlstm_units: 300\nlstm_activation: sigmoid\n2nd_lstm_units: 300\n2nd_lstm_activation: tanh\ndense_units: 11\ndense_activation: sigmoid\ndropout_rate: 0.05\nlearning_rate: 0.01\nvalidation_split: 0.05\nbatch_size: 32\nepochs: 30\nScore: 0.06661056727170944\nTrial summary\nHyperparameters:\nlstm_units: 300\nlstm_activation: sigmoid\n2nd_lstm_units: 300\n2nd_lstm_activation: tanh\ndense_units: 11\ndense_activation: sigmoid\ndropout_rate: 0.05\nlearning_rate: 0.01\nvalidation_split: 0.05\nbatch_size: 32\nepochs: 30\nScore: 0.06901232153177261\nTrial summary\nHyperparameters:\nlstm_units: 300\nlstm_activation: sigmoid\n2nd_lstm_units: 300\n2nd_lstm_activation: tanh\ndense_units: 11\ndense_activation: sigmoid\ndropout_rate: 0.05\nlearning_rate: 0.01\nvalidation_split: 0.05\nbatch_size: 32\nepochs: 30\nScore: 0.06999509781599045\nTrial summary\nHyperparameters:\nlstm_units: 300\nlstm_activation: sigmoid\n2nd_lstm_units: 300\n2nd_lstm_activation: tanh\ndense_units: 11\ndense_activation: sigmoid\ndropout_rate: 0.05\nlearning_rate: 0.01\nvalidation_split: 0.05\nbatch_size: 32\nepochs: 30\nScore: 0.07048741728067398\nTrial summary\nHyperparameters:\nlstm_units: 300\nlstm_activation: tanh\n2nd_lstm_units: 300\n2nd_lstm_activation: tanh\ndense_units: 11\ndense_activation: sigmoid\ndropout_rate: 0.05\nlearning_rate: 0.01\nvalidation_split: 0.05\nbatch_size: 32\nepochs: 30\nScore: 0.07083387672901154\nTrial summary\nHyperparameters:\nlstm_units: 300\nlstm_activation: tanh\n2nd_lstm_units: 300\n2nd_lstm_activation: tanh\ndense_units: 11\ndense_activation: sigmoid\ndropout_rate: 0.05\nlearning_rate: 0.01\nvalidation_split: 0.05\nbatch_size: 32\nepochs: 30\nScore: 0.07127515226602554\nTrial summary\nHyperparameters:\nlstm_units: 300\nlstm_activation: sigmoid\n2nd_lstm_units: 300\n2nd_lstm_activation: sigmoid\ndense_units: 11\ndense_activation: sigmoid\ndropout_rate: 0.05\nlearning_rate: 0.01\nvalidation_split: 0.05\nbatch_size: 32\nepochs: 30\nScore: 0.07147232443094254\n"
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "cell_id": "00027-dbd74463-450d-4fdf-9e21-c3764a299e27",
    "deepnote_cell_type": "code"
   },
   "source": "",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=b042e2da-6536-449d-95b8-d85fa08825de' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
   "metadata": {
    "tags": [],
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown"
   }
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "deepnote_notebook_id": "5bae3659-fae4-4d12-b1ea-64cab0ec1541",
  "deepnote": {},
  "deepnote_execution_queue": []
 }
}