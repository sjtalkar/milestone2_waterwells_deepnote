{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Deeplearning LSTM Model Hyperpamaters Tuning\n",
    "This notebook runs a Bayesian tuning process to find the best hyperparameters for 3 different LSTM models architectures."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "cell_id": "18337427-36ce-4fe5-8247-a76920367c21",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "cell_id": "00001-06fa5675-9b65-4e8a-8cfa-9cac63e669d9",
    "deepnote_cell_type": "code"
   },
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "cell_id": "00002-c8998ab4-cdb1-423c-b355-71b104dc2263",
    "deepnote_cell_type": "code"
   },
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import tensorflow\n",
    "import keras_tuner\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, TimeDistributed, RepeatVector\n",
    "from keras.optimizers import Adam\n",
    "from lib.read_data import read_and_join_output_file\n",
    "from lib.deeplearning import get_train_test_datasets,  get_sets_shapes"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "cell_id": "00003-cd907517-9663-4391-ac26-29e95188437a",
    "deepnote_cell_type": "code"
   },
   "source": [
    "RANDOM_SEED = 31\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tensorflow.random.set_seed(RANDOM_SEED)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preparing the Dataset\n",
    "The dataset is prepared as explained in the /ml/deeplearning_training.ipynb notebook. Please refer to it for more details. As a summary:\n",
    "* The train and test sets are split by Township-Ranges, i.e. some Township-Ranges data are either fully in the train or test set.\n",
    "* The target value is the value of that variable for 2021\n",
    "* Data are imputed using a custom pipeline\n",
    "\n",
    "The resulting train and test sets are of shape [number of Township-Ranges, 7 years (2014-2020), the number of features].\n",
    "We do not create a validation dataset as we use Keras internal cross-validation mechanism to shuffle the data points (i.e., the Township-Ranges) and keep some for the validation at each training epoch."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "cell_id": "00004-afd3bb8d-a214-4047-a2a6-49c4025f1e5e",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "cell_id": "00005-424c9697-01b5-4f1f-8070-dda712b9fd6b",
    "deepnote_cell_type": "code"
   },
   "source": [
    "test_size=0.15\n",
    "target_variable=\"GSE_GWE\"\n",
    "# Load the data from the ETL output files\n",
    "X = read_and_join_output_file()\n",
    "# Split the input pandas Dataframe into training and test datasets, applies the impute pipeline\n",
    "# transformation and reshapes the datasets to 3D (samples, time, features) numpy arrays\n",
    "X_train, X_test, y_train, y_test, _, _ = get_train_test_datasets(X, target_variable=target_variable, test_size=test_size, random_seed=RANDOM_SEED)\n",
    "nb_features = X_train.shape[-1]\n",
    "get_sets_shapes(X_train, X_test)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": "                  nb_items  nb_timestamps  nb_features\ntraining dataset       406              7           82\ntest dataset            72              7           82",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>nb_items</th>\n      <th>nb_timestamps</th>\n      <th>nb_features</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>training dataset</th>\n      <td>406</td>\n      <td>7</td>\n      <td>82</td>\n    </tr>\n    <tr>\n      <th>test dataset</th>\n      <td>72</td>\n      <td>7</td>\n      <td>82</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Hyperparameters Tuning\n",
    "For each of the 3 LSTM models architectures (from simplest to most complex), we use the Keras BayesianOptimization hyperparameters tuner to estimate the best values for the following hyperparameters:\n",
    "* the number of units for each *LSTM* or *Dense* unit\n",
    "* the activation function (*sigmoid*, *tanh*, *relu*) used for all layers, except the output layer which is fixed to a *linear* activation function.\n",
    "* the learning rate\n",
    "* the size of the validation dataset\n",
    "* the batch size\n",
    "* the number of epochs\n",
    "## Simple Model Hyper-parameter Tuning\n",
    "![Simple LSTM Model](../doc/images/lstm_architecture_1.jpg)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "cell_id": "00006-2a6dc8a2-5f2f-4fd4-bb28-f2d9600d0e1b",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "cell_id": "00007-3fd801cd-f94d-4ab6-bc90-1b5bb20ece44",
    "deepnote_cell_type": "code"
   },
   "source": [
    "class Model1(keras_tuner.HyperModel):\n",
    "    def build(self, hp):\n",
    "        model = Sequential()\n",
    "        hp_units = hp.Int(\"units\", min_value=10, max_value=300, step=10)\n",
    "        hp_activ = hp.Choice(\"activation\", values=[\"tanh\", \"sigmoid\"])\n",
    "        model.add(LSTM(units=hp_units, activation=hp_activ, input_shape=(7, nb_features)))\n",
    "        model.add(Dense(1, activation=\"linear\"))\n",
    "        hp_learning_rate = hp.Choice(\"learning_rate\", values=[1e-2, 1e-3, 1e-4])\n",
    "        model.compile(loss=\"mse\", optimizer=Adam(learning_rate=hp_learning_rate), metrics=[keras.metrics.RootMeanSquaredError()])\n",
    "        return model\n",
    "\n",
    "    def fit(self, hp, model, *args, **kwargs):\n",
    "        return model.fit(\n",
    "            *args,\n",
    "            validation_split=hp.Choice(\"validation_split\", values=[0.05, 0.1, 0.15, 0.2]),\n",
    "            batch_size=hp.Int(\"batch_size\", min_value=32, max_value=192, step=16),\n",
    "            epochs=hp.Int(\"epochs\", min_value=30, max_value=500, step=5),\n",
    "            shuffle=True,\n",
    "            **kwargs,\n",
    "        )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    },
    "cell_id": "00008-9f15ed15-72c9-4e95-9d2c-385dc9f17fe0",
    "deepnote_cell_type": "code"
   },
   "source": [
    "stop_early = tensorflow.keras.callbacks.EarlyStopping(monitor='val_root_mean_squared_error', patience=10, verbose=1)\n",
    "tuner = keras_tuner.BayesianOptimization(Model1(),\n",
    "                             objective=keras_tuner.Objective(\"val_root_mean_squared_error\", direction=\"min\"),\n",
    "                             max_trials=250,\n",
    "                             beta=3.2,\n",
    "                             seed=RANDOM_SEED,\n",
    "                             overwrite=True,\n",
    "                             directory=\"keras_tuner\",\n",
    "                             project_name=\"model1_tuner\")\n",
    "tuner.search(X_train, y_train, callbacks=[stop_early])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Best Model Hyperparameters"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "cell_id": "00009-0200b6d7-bc19-4da6-afd7-17b040876d98",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "cell_id": "00010-f786804d-293d-4ace-958d-131868f86749",
    "deepnote_cell_type": "code"
   },
   "source": [
    "# Get the optimal hyperparameters\n",
    "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(f\"\"\"\n",
    "The hyperparameter search is complete.\n",
    "validation_split: {best_hps.get('validation_split')}\n",
    "lstm_units: {best_hps.get('units')}\n",
    "lstm_activation: {best_hps.get('activation')}\n",
    "learning_rate: {best_hps.get('learning_rate')}\n",
    "batch_size: {best_hps.get('batch_size')}\n",
    "epochs: {best_hps.get('epochs')}\n",
    "\"\"\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Hyperparameters Tuning Summary"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "cell_id": "00011-13c828c6-c4a2-4911-97de-efe7821bdcb0",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "cell_id": "00012-b2f19576-a584-40fc-8bed-e3c72882dc12",
    "deepnote_cell_type": "code"
   },
   "source": [
    "tuner.results_summary()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "###  Results\n",
    "Running tha above BayesianOptimization, the best results on the validation set where obtained for the following hyperparameters:\n",
    "* lstm_units: 160\n",
    "* lstm_activation: \"sigmoid\"\n",
    "* learning_rate: 0.001\n",
    "* validation_split: 0.1\n",
    "* batch_size: 128\n",
    "* epochs: 270\n",
    "## Model2 Hyper-parameter tuning\n",
    "![LSTM Model With Dense Layer](../doc/images/lstm_architecture_2.jpg)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "cell_id": "00013-27edb75d-525b-4f50-9b49-85ae39a6cea7",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "cell_id": "00014-8a33dfb3-8b44-4ba0-866d-ca348e663e2c",
    "deepnote_cell_type": "code"
   },
   "source": [
    "class Model2(keras_tuner.HyperModel):\n",
    "    def build(self, hp):\n",
    "        model = Sequential()\n",
    "        lstm_units = hp.Int(\"lstm_units\", min_value=10, max_value=300, step=10)\n",
    "        lstm_activ = hp.Choice(\"lstm_activation\", values=[\"tanh\", \"sigmoid\"])\n",
    "        model.add(LSTM(units=lstm_units, activation=lstm_activ, input_shape=(7, nb_features)))\n",
    "        dense_units = hp.Int(\"dense_units\", min_value=11, max_value=101, step=2)\n",
    "        dense_activation = hp.Choice(\"dense_activation\", values=[\"relu\", \"tanh\", \"sigmoid\"])\n",
    "        model.add(Dense(dense_units, activation=dense_activation))\n",
    "        hp_dropout = hp.Float(\"dropout_rate\", min_value=0.05, max_value=0.25, step=0.05)\n",
    "        model.add(Dropout(hp_dropout))\n",
    "        model.add(Dense(1, activation=\"linear\"))\n",
    "        hp_learning_rate = hp.Choice(\"learning_rate\", values=[1e-2, 1e-3, 1e-4])\n",
    "        model.compile(loss=\"mse\", optimizer=Adam(learning_rate=hp_learning_rate), metrics=[keras.metrics.RootMeanSquaredError()])\n",
    "        return model\n",
    "\n",
    "    def fit(self, hp, model, *args, **kwargs):\n",
    "        return model.fit(\n",
    "            *args,\n",
    "            validation_split=hp.Choice(\"validation_split\", values=[0.05, 0.1, 0.15, 0.2]),\n",
    "            batch_size=hp.Int(\"batch_size\", min_value=32, max_value=192, step=16),\n",
    "            epochs=hp.Int(\"epochs\", min_value=30, max_value=500, step=5),\n",
    "            shuffle=True,\n",
    "            **kwargs,\n",
    "        )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "cell_id": "00015-fec98965-d47c-4f91-8571-5ab7c6659785",
    "deepnote_cell_type": "code"
   },
   "source": [
    "stop_early = tensorflow.keras.callbacks.EarlyStopping(monitor=\"val_root_mean_squared_error\", patience=10, verbose=1)\n",
    "tuner = keras_tuner.BayesianOptimization(Model2(),\n",
    "                              objective=keras_tuner.Objective(\"val_root_mean_squared_error\", direction=\"min\"),\n",
    "                              max_trials=400,\n",
    "                              beta=3.2,\n",
    "                              seed=RANDOM_SEED,\n",
    "                              overwrite=True,\n",
    "                              directory=\"keras_tuner\",\n",
    "                              project_name=\"model2_tuner\")\n",
    "tuner.search(X_train, y_train, callbacks=[stop_early])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Trial 400 Complete [00h 00m 06s]\nval_root_mean_squared_error: 0.08002614974975586\n\nBest val_root_mean_squared_error So Far: 0.06468775868415833\nTotal elapsed time: 03h 14m 13s\nINFO:tensorflow:Oracle triggered exit\n"
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Best Model Hyperparameters"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "cell_id": "00016-f9158a8d-771c-4e14-9c82-6c685c138468",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "cell_id": "00017-fad0f832-adc4-4518-bae4-2d1021228051",
    "deepnote_cell_type": "code"
   },
   "source": [
    "# Get the optimal hyperparameters\n",
    "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(f\"\"\"\n",
    "The hyperparameter search is complete.\n",
    "validation_split: {best_hps.get('validation_split')}\n",
    "lstm_units: {best_hps.get('lstm_units')}\n",
    "lstm_activation: {best_hps.get('lstm_activation')}\n",
    "dense_units: {best_hps.get('dense_units')}\n",
    "dense_activation: {best_hps.get('dense_activation')}\n",
    "dropout_rate: {best_hps.get('dropout_rate')}\n",
    "learning_rate: {best_hps.get('learning_rate')}\n",
    "batch_size: {best_hps.get('batch_size')}\n",
    "epochs: {best_hps.get('epochs')}\n",
    "\"\"\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\nThe hyperparameter search is complete.\nvalidation_split: 0.05\nlstm_units: 10\nlstm_activation: sigmoid\ndense_units: 11\ndense_activation: relu\ndropout_rate: 0.05\nlearning_rate: 0.01\nbatch_size: 32\nepochs: 500\n\n"
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Hyperparameters Tuning Summary"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "cell_id": "00018-dc5f7e68-5fcf-4a48-a757-3ca6463f9704",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "cell_id": "00019-5f9ba635-abbf-4461-965a-5611cd2a8622",
    "deepnote_cell_type": "code"
   },
   "source": [
    "tuner.results_summary()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Results summary\nResults in keras_tuner\\model2_tuner\nShowing 10 best trials\n<keras_tuner.engine.objective.Objective object at 0x00000220D8130B50>\nTrial summary\nHyperparameters:\nlstm_units: 10\nlstm_activation: sigmoid\ndense_units: 11\ndense_activation: relu\ndropout_rate: 0.05\nlearning_rate: 0.01\nvalidation_split: 0.05\nbatch_size: 32\nepochs: 500\nScore: 0.06468775868415833\nTrial summary\nHyperparameters:\nlstm_units: 10\nlstm_activation: sigmoid\ndense_units: 11\ndense_activation: relu\ndropout_rate: 0.1\nlearning_rate: 0.01\nvalidation_split: 0.05\nbatch_size: 32\nepochs: 500\nScore: 0.06481722742319107\nTrial summary\nHyperparameters:\nlstm_units: 10\nlstm_activation: sigmoid\ndense_units: 11\ndense_activation: relu\ndropout_rate: 0.15000000000000002\nlearning_rate: 0.01\nvalidation_split: 0.05\nbatch_size: 32\nepochs: 415\nScore: 0.06549284607172012\nTrial summary\nHyperparameters:\nlstm_units: 10\nlstm_activation: sigmoid\ndense_units: 11\ndense_activation: relu\ndropout_rate: 0.15000000000000002\nlearning_rate: 0.01\nvalidation_split: 0.05\nbatch_size: 32\nepochs: 415\nScore: 0.06606409698724747\nTrial summary\nHyperparameters:\nlstm_units: 10\nlstm_activation: sigmoid\ndense_units: 57\ndense_activation: relu\ndropout_rate: 0.2\nlearning_rate: 0.01\nvalidation_split: 0.05\nbatch_size: 32\nepochs: 145\nScore: 0.06691084057092667\nTrial summary\nHyperparameters:\nlstm_units: 10\nlstm_activation: sigmoid\ndense_units: 11\ndense_activation: relu\ndropout_rate: 0.05\nlearning_rate: 0.01\nvalidation_split: 0.05\nbatch_size: 32\nepochs: 500\nScore: 0.06699465960264206\nTrial summary\nHyperparameters:\nlstm_units: 10\nlstm_activation: sigmoid\ndense_units: 11\ndense_activation: relu\ndropout_rate: 0.15000000000000002\nlearning_rate: 0.01\nvalidation_split: 0.05\nbatch_size: 32\nepochs: 410\nScore: 0.06715192645788193\nTrial summary\nHyperparameters:\nlstm_units: 10\nlstm_activation: sigmoid\ndense_units: 11\ndense_activation: relu\ndropout_rate: 0.05\nlearning_rate: 0.01\nvalidation_split: 0.05\nbatch_size: 32\nepochs: 500\nScore: 0.06768625229597092\nTrial summary\nHyperparameters:\nlstm_units: 10\nlstm_activation: sigmoid\ndense_units: 21\ndense_activation: relu\ndropout_rate: 0.05\nlearning_rate: 0.01\nvalidation_split: 0.05\nbatch_size: 32\nepochs: 360\nScore: 0.06787063926458359\nTrial summary\nHyperparameters:\nlstm_units: 10\nlstm_activation: sigmoid\ndense_units: 11\ndense_activation: relu\ndropout_rate: 0.05\nlearning_rate: 0.01\nvalidation_split: 0.05\nbatch_size: 32\nepochs: 500\nScore: 0.06824542582035065\n"
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Results\n",
    "Running tha above BayesianOptimization, the best results on the validation set where obtained for the following hyperparameters:\n",
    "* validation_split: 0.05\n",
    "* lstm_units: 10\n",
    "* lstm_activation: sigmoid\n",
    "* dense_units: 11\n",
    "* dense_activations: relu\n",
    "* dropout_rate: 0.05\n",
    "* learning_rate: 0.01\n",
    "* batch_size: 32\n",
    "* epochs: 500\n",
    "## Model3 Hyper-parameter tuning\n",
    "![Encoder-Decoder LSTM Model](../doc/images/lstm_architecture_3.jpg)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "cell_id": "00020-703c2022-351e-4bed-99c8-a005fe94b7ba",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "cell_id": "00021-32af92af-f851-4678-9030-8809de1772e3",
    "deepnote_cell_type": "code"
   },
   "source": [
    "class Model3(keras_tuner.HyperModel):\n",
    "    def build(self, hp):\n",
    "        model = Sequential()\n",
    "        lstm_units = hp.Int(\"lstm_units\", min_value=10, max_value=300, step=10)\n",
    "        lstm_activ = hp.Choice(\"lstm_activation\", values=[\"tanh\", \"sigmoid\"])\n",
    "        model.add(LSTM(units=lstm_units, activation=\"sigmoid\", input_shape=(7, nb_features)))\n",
    "        model.add(RepeatVector(1))\n",
    "        lstm_units_2 = hp.Int(\"2nd_lstm_units\", min_value=10, max_value=300, step=10)\n",
    "        lstm_activ_2 = hp.Choice(\"2nd_lstm_activation\", values=[\"tanh\", \"sigmoid\"])\n",
    "        model.add(LSTM(units=lstm_units_2, activation=\"sigmoid\", return_sequences=True))\n",
    "        dense_units = hp.Int(\"dense_units\", min_value=11, max_value=101, step=2)\n",
    "        dense_activation = hp.Choice(\"dense_activation\", values=[\"relu\", \"tanh\", \"sigmoid\"])\n",
    "        model.add(TimeDistributed(Dense(dense_units, activation=dense_activation)))\n",
    "        hp_dropout = hp.Float(\"dropout_rate\", min_value=0.05, max_value=0.25, step=0.05)\n",
    "        model.add(Dropout(hp_dropout))\n",
    "        model.add(Dense(1, activation=\"linear\"))\n",
    "        hp_learning_rate = hp.Choice(\"learning_rate\", values=[1e-2, 1e-3, 1e-4])\n",
    "        model.compile(loss=\"mse\", optimizer=Adam(learning_rate=hp_learning_rate), metrics=[keras.metrics.RootMeanSquaredError()])\n",
    "        return model\n",
    "\n",
    "    def fit(self, hp, model, *args, **kwargs):\n",
    "        return model.fit(\n",
    "            *args,\n",
    "            validation_split=hp.Choice(\"validation_split\", values=[0.05, 0.1, 0.15, 0.2]),\n",
    "            batch_size=hp.Int(\"batch_size\", min_value=32, max_value=192, step=16),\n",
    "            epochs=hp.Int(\"epochs\", min_value=30, max_value=500, step=5),\n",
    "            shuffle=True,\n",
    "            **kwargs,\n",
    "        )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "cell_id": "00022-bb6bc21f-3ca6-4747-8dee-46a69cf66d98",
    "deepnote_cell_type": "code"
   },
   "source": [
    "stop_early = tensorflow.keras.callbacks.EarlyStopping(monitor=\"val_root_mean_squared_error\", patience=10, verbose=1)\n",
    "tuner = keras_tuner.BayesianOptimization(Model3(),\n",
    "                              objective=keras_tuner.Objective(\"val_root_mean_squared_error\", direction=\"min\"),\n",
    "                              max_trials=400,\n",
    "                              beta=3.2,\n",
    "                              seed=RANDOM_SEED,\n",
    "                              overwrite=True,\n",
    "                              directory=\"keras_tuner\",\n",
    "                              project_name=\"model3_tuner\")\n",
    "tuner.search(X_train, y_train, callbacks=[stop_early])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Trial 500 Complete [00h 00m 18s]\nval_root_mean_squared_error: 0.1150362491607666\n\nBest val_root_mean_squared_error So Far: 0.06309118866920471\nTotal elapsed time: 07h 29m 36s\nINFO:tensorflow:Oracle triggered exit\n"
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Best Model Hyperparameters"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "cell_id": "00023-79aebc71-ff28-49e0-976e-a004f1c79cd3",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "cell_id": "00024-c3e304c2-21cf-4acb-98de-c692e28c3023",
    "deepnote_cell_type": "code"
   },
   "source": [
    "# Get the optimal hyperparameters\n",
    "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(f\"\"\"\n",
    "The hyperparameter search is complete.\n",
    "validation_split: {best_hps.get('validation_split')}\n",
    "lstm_units: {best_hps.get('lstm_units')}\n",
    "lstm_activation: {best_hps.get('lstm_activation')}\n",
    "2nd_lstm_units: {best_hps.get('2nd_lstm_units')}\n",
    "2nd_lstm_activation: {best_hps.get('2nd_lstm_activation')}\n",
    "dense_units: {best_hps.get('dense_units')}\n",
    "dense_activation: {best_hps.get('dense_activation')}\n",
    "dropout_rate: {best_hps.get('dropout_rate')}\n",
    "learning_rate: {best_hps.get('learning_rate')}\n",
    "batch_size: {best_hps.get('batch_size')}\n",
    "epochs: {best_hps.get('epochs')}\n",
    "\"\"\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\nThe hyperparameter search is complete.\nvalidation_split: 0.05\nlstm_units: 300\nlstm_activation: sigmoid\n2nd_lstm_units: 300\n2nd_lstm_activation: tanh\ndense_units: 11\ndense_activation: sigmoid\ndropout_rate: 0.05\nlearning_rate: 0.01\nbatch_size: 32\nepochs: 30\n\n"
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Hyperparameters Tuning Summary"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "cell_id": "00025-889ac36a-4641-4f9a-846d-8a180bd833e0",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "cell_id": "00026-58fce9d5-882a-4a43-b850-16a4db6de4b5",
    "deepnote_cell_type": "code"
   },
   "source": [
    "tuner.results_summary()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Results summary\nResults in keras_tuner\\model3_tuner\nShowing 10 best trials\n<keras_tuner.engine.objective.Objective object at 0x000002575F038250>\nTrial summary\nHyperparameters:\nlstm_units: 300\nlstm_activation: sigmoid\n2nd_lstm_units: 300\n2nd_lstm_activation: tanh\ndense_units: 11\ndense_activation: sigmoid\ndropout_rate: 0.05\nlearning_rate: 0.01\nvalidation_split: 0.05\nbatch_size: 32\nepochs: 30\nScore: 0.06309118866920471\nTrial summary\nHyperparameters:\nlstm_units: 300\nlstm_activation: sigmoid\n2nd_lstm_units: 300\n2nd_lstm_activation: tanh\ndense_units: 11\ndense_activation: sigmoid\ndropout_rate: 0.05\nlearning_rate: 0.01\nvalidation_split: 0.05\nbatch_size: 32\nepochs: 30\nScore: 0.06499446928501129\nTrial summary\nHyperparameters:\nlstm_units: 300\nlstm_activation: sigmoid\n2nd_lstm_units: 300\n2nd_lstm_activation: sigmoid\ndense_units: 11\ndense_activation: sigmoid\ndropout_rate: 0.05\nlearning_rate: 0.01\nvalidation_split: 0.05\nbatch_size: 32\nepochs: 30\nScore: 0.06594016402959824\nTrial summary\nHyperparameters:\nlstm_units: 300\nlstm_activation: sigmoid\n2nd_lstm_units: 300\n2nd_lstm_activation: tanh\ndense_units: 11\ndense_activation: sigmoid\ndropout_rate: 0.05\nlearning_rate: 0.01\nvalidation_split: 0.05\nbatch_size: 32\nepochs: 30\nScore: 0.06661056727170944\nTrial summary\nHyperparameters:\nlstm_units: 300\nlstm_activation: sigmoid\n2nd_lstm_units: 300\n2nd_lstm_activation: tanh\ndense_units: 11\ndense_activation: sigmoid\ndropout_rate: 0.05\nlearning_rate: 0.01\nvalidation_split: 0.05\nbatch_size: 32\nepochs: 30\nScore: 0.06901232153177261\nTrial summary\nHyperparameters:\nlstm_units: 300\nlstm_activation: sigmoid\n2nd_lstm_units: 300\n2nd_lstm_activation: tanh\ndense_units: 11\ndense_activation: sigmoid\ndropout_rate: 0.05\nlearning_rate: 0.01\nvalidation_split: 0.05\nbatch_size: 32\nepochs: 30\nScore: 0.06999509781599045\nTrial summary\nHyperparameters:\nlstm_units: 300\nlstm_activation: sigmoid\n2nd_lstm_units: 300\n2nd_lstm_activation: tanh\ndense_units: 11\ndense_activation: sigmoid\ndropout_rate: 0.05\nlearning_rate: 0.01\nvalidation_split: 0.05\nbatch_size: 32\nepochs: 30\nScore: 0.07048741728067398\nTrial summary\nHyperparameters:\nlstm_units: 300\nlstm_activation: tanh\n2nd_lstm_units: 300\n2nd_lstm_activation: tanh\ndense_units: 11\ndense_activation: sigmoid\ndropout_rate: 0.05\nlearning_rate: 0.01\nvalidation_split: 0.05\nbatch_size: 32\nepochs: 30\nScore: 0.07083387672901154\nTrial summary\nHyperparameters:\nlstm_units: 300\nlstm_activation: tanh\n2nd_lstm_units: 300\n2nd_lstm_activation: tanh\ndense_units: 11\ndense_activation: sigmoid\ndropout_rate: 0.05\nlearning_rate: 0.01\nvalidation_split: 0.05\nbatch_size: 32\nepochs: 30\nScore: 0.07127515226602554\nTrial summary\nHyperparameters:\nlstm_units: 300\nlstm_activation: sigmoid\n2nd_lstm_units: 300\n2nd_lstm_activation: sigmoid\ndense_units: 11\ndense_activation: sigmoid\ndropout_rate: 0.05\nlearning_rate: 0.01\nvalidation_split: 0.05\nbatch_size: 32\nepochs: 30\nScore: 0.07147232443094254\n"
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Results\n",
    "Running tha above BayesianOptimization, the best results on the validation set where obtained for the following hyperparameters:\n",
    "* validation_split: 0.05\n",
    "* lstm_units: 300\n",
    "* lstm_activation: sigmoid\n",
    "* 2nd_lstm_units: 300\n",
    "* 2nd_lstm_activation: tanh\n",
    "* dense_units: 11\n",
    "* dense_activations: sigmoid\n",
    "* dropout_rate: 0.05\n",
    "* learning_rate: 0.01\n",
    "* batch_size: 32\n",
    "* epochs: 500"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "deepnote_notebook_id": "5bae3659-fae4-4d12-b1ea-64cab0ec1541",
  "deepnote": {},
  "deepnote_execution_queue": []
 }
}