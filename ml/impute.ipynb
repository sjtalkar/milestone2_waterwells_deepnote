{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Preparing the Data for Machine Learning\n",
    "This notebook is used to prepare the data for machine learning by using Scikit Learn Pipelines to perform data imputation."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# For Deepnote to be able to use the custom libraries in the parent ../lib folder\n",
    "import sys\n",
    "sys.path.append('..')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from lib.read_data import read_and_join_output_file\n",
    "from lib.impute import create_transformation_pipelines, group_train_test_split\n",
    "from lib.viz import draw_missing_data_chart\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import set_config"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "First we load and join all the datasets resulting from the ETL process and initialize some variables.\n",
    "\n",
    "We have two potential targets for supervised and unsupervised learning, which we remove from the list of features:\n",
    "* `GSE_GWE` - The Ground Surface Elevation to Groundwater Water Elevation - Depth to groundwater elevation in feet below ground surface\n",
    "* `SHORTAGE_COUNT` -  The number of reported well shortages"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "X = read_and_join_output_file()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Missing Data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X.sample(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's look at the features with missing data."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "draw_missing_data_chart(X)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data Missing for Specific Years\n",
    "Data were collected from the years 2014 to 2021 but some datasets only have data for specific years, when surveys were done/published. For example:\n",
    "* Soils survey only has data for 2016\n",
    "* Vegetations only has data for 2019\n",
    "* Crops only has data for the the years 2014, 2016 and 2018\n",
    "* Population density is available only for the years 2014 - 2020\n",
    "* The reservoir water `PCT_OF_CAPACITY` is available only for the years 2018 - 2020"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "crops_columns = [col for col in X if col.startswith('CROP_')]\n",
    "crops_df = X[crops_columns].dropna()\n",
    "soils_columns = [col for col in X if col.startswith('SOIL_')]\n",
    "soils_df = X[soils_columns].dropna()\n",
    "print(f\"Years present in the Soils dataset {list(crops_df.index.unique(level='YEAR'))}\")\n",
    "print(f\"Years present in the Crops dataset {list(soils_df.index.unique(level='YEAR'))}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data Missing for Specific Township-Ranges\n",
    "The Well Completion Reports dataset has data for all the years but have missing data for some specific Township-Ranges. Typically, if no wells were drilled in a specific Township-Range during the 2014-2020 period, then there is no data for that Township-Range for any of the following features:\n",
    "* `TOTALDRILLDEPTH_AVG`\n",
    "* `WELLYIELD_AVG`\n",
    "* `STATICWATERLEVEL_AVG`\n",
    "* `TOPOFPERFORATEDINTERVAL_AVG`\n",
    "* `BOTTOMOFPERFORATEDINTERVAL_AVG`\n",
    "* `GROUNDSURFACEELEVATION_AVG`\n",
    "* `TOTALCOMPLETEDDEPTH_AVG`\n",
    "\n",
    "Wells can also be reported with incomplete data, which means that some of the above features data could be missing for some Township-Ranges, even if wells were reported in those Township-Range."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "all_township_ranges = set(X.index.unique(level=\"TOWNSHIP_RANGE\"))\n",
    "wells_columns = [col for col in X if col.endswith('_AVG') or col == \"TOWNSHIP_RANGE\"]\n",
    "wells_df = X[wells_columns].dropna()\n",
    "missing_township_ranges = all_township_ranges - set(wells_df.index.unique(level=\"TOWNSHIP_RANGE\"))\n",
    "print(f\"There are {len(missing_township_ranges)} out of {len(all_township_ranges)} Township-Ranges with missing well completion report data: {missing_township_ranges}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train-Test Split\n",
    "We split the dataset into a training and test set before doing data imputation. As we deal with time series data grouped at the Township-Range level, we can't split the dataset by randomly splitting rows between the train and test sets. We need to keep data of Township-Ranges together. To do that we use our custom train_test_split function which is based on scikit-learn `GroupShuffleSplit` class."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train, X_test = group_train_test_split(X, RANDOM_SEED)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's look at 2 examples of the training and test sets."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train.head(16)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_test.head(16)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Imputation\n",
    "### Imputation Strategies\n",
    "To impute the missing data we will use the following strategies\n",
    "1. We assume little year-to-year variation in Crops, Soils and Vegetation. The missing Crops data will thus be imputed from the previous year (e.g. the 2015 data will be set as the 2014 data). For the Soils and Vegetation where we only have data for 1 year, the missing data will all be imputed from the available year.\n",
    "2. The 2021 population density data will be estimated based on the 2020 population density and the 2019-2020 trend.\n",
    "3. For the pre-2018 missing reservoir water `PCT_OF_CAPACITY` data, as California was affected by sever droughts during those years, we will impute missing data by taking the **minimum** `PCT_OF_CAPACITY` for that Township-Range in the post 2018 data.\n",
    "4. For the well completion reports' features with missing we will use 2 distinct strategies:\n",
    "    * For the  `GROUNDSURFACEELEVATION_AVG` feature we will use the median values over all the years for that Township-Range. For Township-Ranges with no data at all for any of the 2014-2020 years, we will use the median value over all Township-Ranges.\n",
    "    * For the other features they will be set to 0, since these are well measurements and missing data are mainly due to no wells being drilled in that Township-Range and year."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "impute_pipeline, columns = create_transformation_pipelines(X_train)\n",
    "X_train_impute = impute_pipeline.fit_transform(X_train)\n",
    "X_test_impute = impute_pipeline.transform(X_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We combine the imputed training and test datasets into one dataset to visualize the results."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "set_config(display=\"diagram\")\n",
    "display(impute_pipeline)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train_impute_df = pd.DataFrame(X_train_impute, index=X_train.index, columns=columns)\n",
    "X_test_impute_df = pd.DataFrame(X_test_impute, index=X_test.index, columns=columns)\n",
    "X_impute_df = pd.concat([X_train_impute_df, X_test_impute_df], axis=0)\n",
    "X_impute_df.sort_index(level=[\"TOWNSHIP_RANGE\", \"YEAR\"], inplace=True)\n",
    "X_impute_df.head(16)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "draw_missing_data_chart(X_impute_df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}